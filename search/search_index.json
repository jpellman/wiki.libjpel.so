{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The libjpel.so Wiki This is the personal Wiki of John Pellman. He is mostly using this as a palimpsest for snippets of information he gleans (primarily links to good articles) and various notes related to his interests. The main role that he envisions for this wiki is to aid him in remembering what he learns by acting as a canvas to organize his thoughts. This is helpful because elaborative processing increases recall . Aside from technical notes, he hopes that he'll have some time for a little science as well. He also is using this as a location to document his home IT infrastructure, and plan for ways to improve it. This wiki is powered by MkDocs and GitHub Actions .","title":"Home"},{"location":"#the-libjpelso-wiki","text":"This is the personal Wiki of John Pellman. He is mostly using this as a palimpsest for snippets of information he gleans (primarily links to good articles) and various notes related to his interests. The main role that he envisions for this wiki is to aid him in remembering what he learns by acting as a canvas to organize his thoughts. This is helpful because elaborative processing increases recall . Aside from technical notes, he hopes that he'll have some time for a little science as well. He also is using this as a location to document his home IT infrastructure, and plan for ways to improve it. This wiki is powered by MkDocs and GitHub Actions .","title":"The libjpel.so Wiki"},{"location":"Charities/","text":"Charities/organizations / open source projects that I find worth giving to / supporting in some manner. Science Broad Institute Allen Institute Numfocus Center for Open Science Society for the Improvement of Psychological Science The Planetary Society PyBOSSA Zooniverse Octave Government Agencies NASA The NIH (via FNIH) Informative article w/ caveats List of all agencies receiving gifts Technology Software in the Public Interest Software Freedom Conservancy Free Software Foundation Apache Software Foundation ReactOS InfoAge Computer History Museum Living Computers (allows you to log in remotely to several vintage machines) SDF Public Internet Electronic Frontier Foundation OFFIS Dicom / DCMTK Orthanc Associations w/ an Educational Mission: LOPSA Unigroup CSPR Social Issues Mohawk Valley Resource Center for Refugees RIP Medical Debt Debt Collective Kiva WFM Open Society Foundations Education A Better Chance Clinton Central School District Foundation MVCC Foundation Oberlin College Hamilton College World Possible Mali Linux Tele SciShow CrashCourse Khan Academy EdX","title":"Charities"},{"location":"Charities/#science","text":"Broad Institute Allen Institute Numfocus Center for Open Science Society for the Improvement of Psychological Science The Planetary Society PyBOSSA Zooniverse Octave","title":"Science"},{"location":"Charities/#government-agencies","text":"NASA The NIH (via FNIH) Informative article w/ caveats List of all agencies receiving gifts","title":"Government Agencies"},{"location":"Charities/#technology","text":"Software in the Public Interest Software Freedom Conservancy Free Software Foundation Apache Software Foundation ReactOS InfoAge Computer History Museum Living Computers (allows you to log in remotely to several vintage machines) SDF Public Internet Electronic Frontier Foundation OFFIS Dicom / DCMTK Orthanc","title":"Technology"},{"location":"Charities/#associations-w-an-educational-mission","text":"LOPSA Unigroup CSPR","title":"Associations w/ an Educational Mission:"},{"location":"Charities/#social-issues","text":"Mohawk Valley Resource Center for Refugees RIP Medical Debt Debt Collective Kiva WFM Open Society Foundations","title":"Social Issues"},{"location":"Charities/#education","text":"A Better Chance Clinton Central School District Foundation MVCC Foundation Oberlin College Hamilton College World Possible Mali Linux Tele SciShow CrashCourse Khan Academy EdX","title":"Education"},{"location":"Academic-Computing/","text":"Pages that related to academic computing. Major Past Academic Computing Initiatives of Note Andrew Project (also here and here ) Project Athena (also here ) Stanford University Network whose efforts, alongside the Computer Systems Research Group (BSD folks), were successfully transitioned into and continued with Sun Microsystems sudo was made at SUNY Buffalo in the early to mid 80s. The Stanford Integrated Digital Library Project effectively morphed into Google (interestingly enough it looks like Eolas also was planning on submitting a proposal for the same NSF initiative) Kermit (Columbia site here ) Pamacea (also see general Columbia AcIS UnixDev Projects ) ITK / the Insight Software Consortium. YouTube founder Jawed Karim was briefly involved in this. Mach (now part of OS X) Python was created first at CWI and then continued to be developed at CNRI EISPACK was created at Argonne National Laboratory and funded by the NSF and the US Atomic Energy Commission. LINPACK was developed at three universities (UCSD, University of New Mexico, and University of Maryland) and Argonne National Laboratory under NSF grant MCS76-03297. On Technology Transfer In which CCNMTL/CTL briefly talks about Bayh-Dole Consequences of the Bayh-Dole Act On Funding Models Essentially, most academic software is either a) grant-funded in some way or b) created via some sort of consortium (i.e., the community source model). Some software lives under an umbrella organization, such as the Apereo Foundation or Apache Foundation . Open Source / Community Source Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure (Ford Foundation report on sustainability of open source model) Community Source Is Dead How to Achieve Vendor Lock-in with a Legit Open Source License \u2013 Affero GPL Sakai: Building an Open Source Community Cutting Edge: Collaboration gets the most out of software (an article discussing the funding model for and history of the SBGrid Consortium Government Policy Towards Open Source Software","title":"Index"},{"location":"Academic-Computing/#major-past-academic-computing-initiatives-of-note","text":"Andrew Project (also here and here ) Project Athena (also here ) Stanford University Network whose efforts, alongside the Computer Systems Research Group (BSD folks), were successfully transitioned into and continued with Sun Microsystems sudo was made at SUNY Buffalo in the early to mid 80s. The Stanford Integrated Digital Library Project effectively morphed into Google (interestingly enough it looks like Eolas also was planning on submitting a proposal for the same NSF initiative) Kermit (Columbia site here ) Pamacea (also see general Columbia AcIS UnixDev Projects ) ITK / the Insight Software Consortium. YouTube founder Jawed Karim was briefly involved in this. Mach (now part of OS X) Python was created first at CWI and then continued to be developed at CNRI EISPACK was created at Argonne National Laboratory and funded by the NSF and the US Atomic Energy Commission. LINPACK was developed at three universities (UCSD, University of New Mexico, and University of Maryland) and Argonne National Laboratory under NSF grant MCS76-03297.","title":"Major Past Academic Computing Initiatives of Note"},{"location":"Academic-Computing/#on-technology-transfer","text":"In which CCNMTL/CTL briefly talks about Bayh-Dole Consequences of the Bayh-Dole Act","title":"On Technology Transfer"},{"location":"Academic-Computing/#on-funding-models","text":"Essentially, most academic software is either a) grant-funded in some way or b) created via some sort of consortium (i.e., the community source model). Some software lives under an umbrella organization, such as the Apereo Foundation or Apache Foundation .","title":"On Funding Models"},{"location":"Academic-Computing/#open-source-community-source","text":"Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure (Ford Foundation report on sustainability of open source model) Community Source Is Dead How to Achieve Vendor Lock-in with a Legit Open Source License \u2013 Affero GPL Sakai: Building an Open Source Community Cutting Edge: Collaboration gets the most out of software (an article discussing the funding model for and history of the SBGrid Consortium Government Policy Towards Open Source Software","title":"Open Source / Community Source"},{"location":"Academic-Computing/Web-Design-Firms-For-Academics/","text":"List of Web Design Firms that Specialize in Academic Clientele Singlebrook Academic Web Pages (aptly-named) Squishy Media KnowledgeTown The Bureau (UK)","title":"Web Design Firms For Academics"},{"location":"Academic-Computing/Web-Design-Firms-For-Academics/#list-of-web-design-firms-that-specialize-in-academic-clientele","text":"Singlebrook Academic Web Pages (aptly-named) Squishy Media KnowledgeTown The Bureau (UK)","title":"List of Web Design Firms that Specialize in Academic Clientele"},{"location":"Academic-Computing/Library-Computing/","text":"Library Computing Metadata/Search vs Filesystems https://gadgetopia.com/post/8290 https://ldapwiki.com/wiki/Containerization%20vs%20Metadata https://stackoverflow.com/questions/1575155/what-is-a-database-file-system http://dbfs.sourceforge.net/ https://docs.oracle.com/database/121/ADLOB/adlob_fs.htm#ADLOB45943 https://github.com/shirosaidev/diskover Digital Asset Management (DAM) FEDORA Article Persistent Identifiers (Handle/DOI) A Framework for Distributed Digital Object Services (article outlining the Handle system) The Semantic Web https://lemire.me/blog/2016/11/15/on-metadata/ https://5stardata.info/en/ Emulation David Rosenthal's Emulation Paper GPU-Accelerated Atari Emulation The Future of Libraries http://www.columbia.edu/~jrh29/licklider/ http://www.columbia.edu/~hauben/libraries-of-future.doc Miscellaneous Reading http://www.dlib.org/dlib/october00/granger/10granger.html Cites and Insights - Articles by Walt Crawford How Academic Science Gave Its Soul to the Publishing Industry","title":"Index"},{"location":"Academic-Computing/Library-Computing/#library-computing","text":"","title":"Library Computing"},{"location":"Academic-Computing/Library-Computing/#metadatasearch-vs-filesystems","text":"https://gadgetopia.com/post/8290 https://ldapwiki.com/wiki/Containerization%20vs%20Metadata https://stackoverflow.com/questions/1575155/what-is-a-database-file-system http://dbfs.sourceforge.net/ https://docs.oracle.com/database/121/ADLOB/adlob_fs.htm#ADLOB45943 https://github.com/shirosaidev/diskover","title":"Metadata/Search vs Filesystems"},{"location":"Academic-Computing/Library-Computing/#digital-asset-management-dam","text":"FEDORA Article","title":"Digital Asset Management (DAM)"},{"location":"Academic-Computing/Library-Computing/#persistent-identifiers-handledoi","text":"A Framework for Distributed Digital Object Services (article outlining the Handle system)","title":"Persistent Identifiers (Handle/DOI)"},{"location":"Academic-Computing/Library-Computing/#the-semantic-web","text":"https://lemire.me/blog/2016/11/15/on-metadata/ https://5stardata.info/en/","title":"The Semantic Web"},{"location":"Academic-Computing/Library-Computing/#emulation","text":"David Rosenthal's Emulation Paper GPU-Accelerated Atari Emulation","title":"Emulation"},{"location":"Academic-Computing/Library-Computing/#the-future-of-libraries","text":"http://www.columbia.edu/~jrh29/licklider/ http://www.columbia.edu/~hauben/libraries-of-future.doc","title":"The Future of Libraries"},{"location":"Academic-Computing/Library-Computing/#miscellaneous-reading","text":"http://www.dlib.org/dlib/october00/granger/10granger.html Cites and Insights - Articles by Walt Crawford How Academic Science Gave Its Soul to the Publishing Industry","title":"Miscellaneous Reading"},{"location":"Hobbies/","text":"Hobbies A set of links for fun, non-work related meaningful activities: External Links ArchiveTeam","title":"Index"},{"location":"Hobbies/#hobbies","text":"A set of links for fun, non-work related meaningful activities:","title":"Hobbies"},{"location":"Hobbies/#external-links","text":"ArchiveTeam","title":"External Links"},{"location":"Hobbies/Academic-History/","text":"Academic History Resources related to the history of academia. General History The Rise of Universities by Charles Homer Haskins The Economics of Tuition and Fees in American Higher Education Demystifying Endowments Tuition Rising: Why College Costs So Much Defunct/Merged Colleges Kirkland College Limited Engagement Franconia College Belknap College Eisenhower College College History Garden","title":"Academic History"},{"location":"Hobbies/Academic-History/#academic-history","text":"Resources related to the history of academia.","title":"Academic History"},{"location":"Hobbies/Academic-History/#general-history","text":"The Rise of Universities by Charles Homer Haskins The Economics of Tuition and Fees in American Higher Education Demystifying Endowments Tuition Rising: Why College Costs So Much","title":"General History"},{"location":"Hobbies/Academic-History/#defunctmerged-colleges","text":"Kirkland College Limited Engagement Franconia College Belknap College Eisenhower College College History Garden","title":"Defunct/Merged Colleges"},{"location":"Hobbies/Computational-Linguistics/","text":"Computational Linguistics Latent Semantic Analysis http://opensourceconnections.com/blog/2016/03/29/semantic-search-with-latent-semantic-analysis/ https://www.youtube.com/watch?v=BJ0MnawUpaU","title":"Computational Linguistics"},{"location":"Hobbies/Computational-Linguistics/#computational-linguistics","text":"","title":"Computational Linguistics"},{"location":"Hobbies/Computational-Linguistics/#latent-semantic-analysis","text":"http://opensourceconnections.com/blog/2016/03/29/semantic-search-with-latent-semantic-analysis/ https://www.youtube.com/watch?v=BJ0MnawUpaU","title":"Latent Semantic Analysis"},{"location":"Hobbies/Food/","text":"Food A page where I keep my favorite recipes, and more broadly thoughts about food. Recipes Oat Milk 1 cup oats 4 cups water 1/4 tsp guar gum 1/4 tsp Calcium Carbonate (should be around 1250mg Calcium, 500 mg for dietary purposes) 1 tsp sugar 1/4 tsp salt Soak 1 cup oats in disposable water for 15 minutes. Drain out disposable water. Add in 4 cups of water. Blend for 30 seconds. Filter out oats with nut milk bag / cheesecloth. Add salt, guar gum, sugar and calcium carbonate. Blend again. General Tso's Textured Vegetable Protein (TVP) 1/2 cup TVP 2 2/3 Tbsp sugar 1 1/3 Tbsp General Tso's seasoning packet or 1 tsp garlic powder + 1 tsp ginger + 1/2 tsp red pepper 2 tsp methylcellulose HV 1 tsp transglutaminase (possibly overkill- 1/4 tsp might be more accurate, but I had no scale to measure; should be 1/2 tsp according to the conversion here ) 2 Tbsp soy sauce (or perhaps worcestershire sauce) 1/2 cup boiling water Beverages Tea I'm a big fan of the following kinds of tea: Lapsang Souchong Earl Grey Kusmi St. Petersburg Tea-like Yerba Mate Cassina / Yaupon Holly Roasted Chicory / pero Sodas Moxie Pacific (alcohol-free Pastis) Basically anything that tastes vaguely medicinal Alcohol/Spirits Good Riesling White Wine McKenzie Whiskey Laphroaig Whiskey Pretty much anything from my home region or the Finger Lakes. I have no palate so I just choose based off geography and it generally works out well. Pastis Shitty Utica Club Wi\u015bni\u00f3wka Johnny Bootlegger Ingredients https://cooking.stackexchange.com/questions/6619/are-there-any-vegetarian-applications-for-transglutaminase-meat-glue","title":"Food"},{"location":"Hobbies/Food/#food","text":"A page where I keep my favorite recipes, and more broadly thoughts about food.","title":"Food"},{"location":"Hobbies/Food/#recipes","text":"","title":"Recipes"},{"location":"Hobbies/Food/#oat-milk","text":"1 cup oats 4 cups water 1/4 tsp guar gum 1/4 tsp Calcium Carbonate (should be around 1250mg Calcium, 500 mg for dietary purposes) 1 tsp sugar 1/4 tsp salt Soak 1 cup oats in disposable water for 15 minutes. Drain out disposable water. Add in 4 cups of water. Blend for 30 seconds. Filter out oats with nut milk bag / cheesecloth. Add salt, guar gum, sugar and calcium carbonate. Blend again.","title":"Oat Milk"},{"location":"Hobbies/Food/#general-tsos-textured-vegetable-protein-tvp","text":"1/2 cup TVP 2 2/3 Tbsp sugar 1 1/3 Tbsp General Tso's seasoning packet or 1 tsp garlic powder + 1 tsp ginger + 1/2 tsp red pepper 2 tsp methylcellulose HV 1 tsp transglutaminase (possibly overkill- 1/4 tsp might be more accurate, but I had no scale to measure; should be 1/2 tsp according to the conversion here ) 2 Tbsp soy sauce (or perhaps worcestershire sauce) 1/2 cup boiling water","title":"General Tso's Textured Vegetable Protein (TVP)"},{"location":"Hobbies/Food/#beverages","text":"","title":"Beverages"},{"location":"Hobbies/Food/#tea","text":"I'm a big fan of the following kinds of tea: Lapsang Souchong Earl Grey Kusmi St. Petersburg","title":"Tea"},{"location":"Hobbies/Food/#tea-like","text":"Yerba Mate Cassina / Yaupon Holly Roasted Chicory / pero","title":"Tea-like"},{"location":"Hobbies/Food/#sodas","text":"Moxie Pacific (alcohol-free Pastis) Basically anything that tastes vaguely medicinal","title":"Sodas"},{"location":"Hobbies/Food/#alcoholspirits","text":"","title":"Alcohol/Spirits"},{"location":"Hobbies/Food/#good","text":"Riesling White Wine McKenzie Whiskey Laphroaig Whiskey Pretty much anything from my home region or the Finger Lakes. I have no palate so I just choose based off geography and it generally works out well. Pastis","title":"Good"},{"location":"Hobbies/Food/#shitty","text":"Utica Club Wi\u015bni\u00f3wka Johnny Bootlegger","title":"Shitty"},{"location":"Hobbies/Food/#ingredients","text":"https://cooking.stackexchange.com/questions/6619/are-there-any-vegetarian-applications-for-transglutaminase-meat-glue","title":"Ingredients"},{"location":"Hobbies/Georectification/","text":"Georectification Maps I've georectified (or am in the process of georectifying) on NYPL's website. See here as well. In Progress http://maps.nypl.org/warper/maps/12785 http://maps.nypl.org/warper/maps/31351 Done http://maps.nypl.org/warper/maps/29634 - last edit = version 70 http://maps.nypl.org/warper/maps/12709 - last edit = version 108 http://maps.nypl.org/warper/maps/26306 - last edit = version 170 http://maps.nypl.org/warper/maps/31229 - last edit = version 79","title":"Georectification"},{"location":"Hobbies/Georectification/#georectification","text":"Maps I've georectified (or am in the process of georectifying) on NYPL's website. See here as well.","title":"Georectification"},{"location":"Hobbies/Georectification/#in-progress","text":"http://maps.nypl.org/warper/maps/12785 http://maps.nypl.org/warper/maps/31351","title":"In Progress"},{"location":"Hobbies/Georectification/#done","text":"http://maps.nypl.org/warper/maps/29634 - last edit = version 70 http://maps.nypl.org/warper/maps/12709 - last edit = version 108 http://maps.nypl.org/warper/maps/26306 - last edit = version 170 http://maps.nypl.org/warper/maps/31229 - last edit = version 79","title":"Done"},{"location":"Hobbies/Music/","text":"Music In which I characterize my recreational music consumption. No production anymore unfortunately. Consumption Medieval Techno / Modern Medieval Qntal Corvus Corax Blackmore's Night Italodisco Raf Fancy Vaporwave Windows 96 Microtonal Music Sevish Other Music Ondes Martenot Glass Armonica Laurie Anderson","title":"Music"},{"location":"Hobbies/Music/#music","text":"In which I characterize my recreational music consumption. No production anymore unfortunately.","title":"Music"},{"location":"Hobbies/Music/#consumption","text":"","title":"Consumption"},{"location":"Hobbies/Music/#medieval-techno-modern-medieval","text":"Qntal Corvus Corax Blackmore's Night","title":"Medieval Techno / Modern Medieval"},{"location":"Hobbies/Music/#italodisco","text":"Raf Fancy","title":"Italodisco"},{"location":"Hobbies/Music/#vaporwave","text":"Windows 96","title":"Vaporwave"},{"location":"Hobbies/Music/#microtonal-music","text":"Sevish","title":"Microtonal Music"},{"location":"Hobbies/Music/#other-music","text":"Ondes Martenot Glass Armonica Laurie Anderson","title":"Other Music"},{"location":"Hobbies/Reading-List/","text":"Reading List I'm using the tables below to keep track of the books I'm currently reading / have read. In Progress Paperbacks Title Author Start Date Finish Date Amazon Link Sakai: Building an Open Source Community Chuck Severance 10/30/21 here Go In Action Erik St. Martin, William Kennedy, Brian Ketelsen ~ 03/01/22 here Textbooks Title Author Chapter Start Date Finish Date Amazon Link The Design of Everyday Things: Revised and Expanded Edition Don Norman 07/11/20 here Finished Paperbacks Title Author Start Date Finish Date Amazon Link The Human Use Of Human Beings: Cybernetics And Society Norbert Wiener 6/20/17 Some point in 2017(?) here Amusing Ourselves to Death: Public Discourse in the Age of Show Business Neil Postman 6/7/17 6/20/17 (?) here Life's Greatest Secret: The Race to Crack the Genetic Code Matthew Cobb 1/29/17 ~ 3/1/20 here Misquoting Jesus: The Story Behind Who Changed the Bible and Why Bart Ehrman 12/25/19 12/29/19 here The Calculating Stars: A Lady Astronaut Novel Mary Robinette Kowal 12/25/19 ~ 2/6/20 here The Idea of the Brain: The Past and Future of Neuroscience Matthew Cobb ~ 4/20/20 6/3/20 here My Life as a Quant: Reflections on Physics and Finance Emanuel Derman 7/27/20 8/8/20 here Think Least of Death: Spinoza on How to Live and How to Die Steven Nadler 11/26/20 Some point in early 2021(?) here When Genius Failed: The Rise and Fall of Long-Term Capital Management Roger Lowenstein 10/2/21 ~ 10/21/21 here Where Wizards Stay Up Late: The Origins Of The Internet Katie Hafner and Matthew Lyon 4/15/22 5/8/22 here Leviathan Wakes James S. A. Corey 01/15/22 2/26/22 here Caliban's War James S. A. Corey 02/27/22 04/03/22 here Abaddon's Gate James S. A. Corey 04/03/22 05/14/22 here Cibola Burn James S. A. Corey 05/19/22 05/14/22 here Textbooks Title Author Chapter Start Date Finish Date Amazon Link Modern Operating Systems (3rd Edition) Andrew Tanenbaum, Herbert Bos ? ? 04/20/20 ? Computer Networks (5th Edition) Andrew Tanenbaum, David J. Wetherall ? 08/07/21 06/05/22 here Queued This is a list of books (primarily from Humble Bundles) that I want to finish reading at some point. Artificial Intelligence Musts Cybernetics What Computers Still Can't Do The Deep Learning Revolution A Life in Cybernetics Maybes Artificial Intelligence: The Very Idea Artifical Minds Made-Up Minds: A Constructivist Approach to Artificial Intelligence Understanding Intelligence Applied Machine Learning Musts Introduction to Machine Learning with Python Maybes Applied Text Analysis with Python Natural Language Processing with Python Natural Language Processing with PyTorch Deep Learning Cookbook Thoughtful Machine Learning with Python Circuit Design / Electronics Musts Electronics For Dummies, 3rd Edition Maybes Circuitbuilding Do-It-Yourself For Dummies Complete Electronics Self-Teaching Guide with Projects Systems Administration Musts AWS System Administration Kubernetes: Up and Running Maybes Cloud Native Infrastructure DNS and BIND, 5th Edition Programming Musts Making Software: What Really Works, and Why We Believe It Elegant SciPy Testing Python: Applying Unit Testing, TDD, BDD and Acceptance Testing Test-Driven Development with Python, 2nd Edition Algorithms in a Nutshell, 2nd Edition Fluent Python Maybes Cracking Codes with Python Designing Distributed Systems The Art of Readable Code 97 Things Every Programmer Should Know Operating Systems Musts Modern Operating Systems Maybes Linux Programming Interface Mathematics/Statistics Think Bayes Imaging Digital Image Processing, 4th Edition Personal Finance Get a Financial Life: Personal Finance in Your Twenties and Thirties Popular Science This Is Your Brain on Parasites: How Tiny Creatures Manipulate Our Behavior and Shape Society The Big Picture: On the Origins of Life, Meaning, and the Universe Itself","title":"Reading List"},{"location":"Hobbies/Reading-List/#reading-list","text":"I'm using the tables below to keep track of the books I'm currently reading / have read.","title":"Reading List"},{"location":"Hobbies/Reading-List/#in-progress","text":"","title":"In Progress"},{"location":"Hobbies/Reading-List/#paperbacks","text":"Title Author Start Date Finish Date Amazon Link Sakai: Building an Open Source Community Chuck Severance 10/30/21 here Go In Action Erik St. Martin, William Kennedy, Brian Ketelsen ~ 03/01/22 here","title":"Paperbacks"},{"location":"Hobbies/Reading-List/#textbooks","text":"Title Author Chapter Start Date Finish Date Amazon Link The Design of Everyday Things: Revised and Expanded Edition Don Norman 07/11/20 here","title":"Textbooks"},{"location":"Hobbies/Reading-List/#finished","text":"","title":"Finished"},{"location":"Hobbies/Reading-List/#paperbacks_1","text":"Title Author Start Date Finish Date Amazon Link The Human Use Of Human Beings: Cybernetics And Society Norbert Wiener 6/20/17 Some point in 2017(?) here Amusing Ourselves to Death: Public Discourse in the Age of Show Business Neil Postman 6/7/17 6/20/17 (?) here Life's Greatest Secret: The Race to Crack the Genetic Code Matthew Cobb 1/29/17 ~ 3/1/20 here Misquoting Jesus: The Story Behind Who Changed the Bible and Why Bart Ehrman 12/25/19 12/29/19 here The Calculating Stars: A Lady Astronaut Novel Mary Robinette Kowal 12/25/19 ~ 2/6/20 here The Idea of the Brain: The Past and Future of Neuroscience Matthew Cobb ~ 4/20/20 6/3/20 here My Life as a Quant: Reflections on Physics and Finance Emanuel Derman 7/27/20 8/8/20 here Think Least of Death: Spinoza on How to Live and How to Die Steven Nadler 11/26/20 Some point in early 2021(?) here When Genius Failed: The Rise and Fall of Long-Term Capital Management Roger Lowenstein 10/2/21 ~ 10/21/21 here Where Wizards Stay Up Late: The Origins Of The Internet Katie Hafner and Matthew Lyon 4/15/22 5/8/22 here Leviathan Wakes James S. A. Corey 01/15/22 2/26/22 here Caliban's War James S. A. Corey 02/27/22 04/03/22 here Abaddon's Gate James S. A. Corey 04/03/22 05/14/22 here Cibola Burn James S. A. Corey 05/19/22 05/14/22 here","title":"Paperbacks"},{"location":"Hobbies/Reading-List/#textbooks_1","text":"Title Author Chapter Start Date Finish Date Amazon Link Modern Operating Systems (3rd Edition) Andrew Tanenbaum, Herbert Bos ? ? 04/20/20 ? Computer Networks (5th Edition) Andrew Tanenbaum, David J. Wetherall ? 08/07/21 06/05/22 here","title":"Textbooks"},{"location":"Hobbies/Reading-List/#queued","text":"This is a list of books (primarily from Humble Bundles) that I want to finish reading at some point.","title":"Queued"},{"location":"Hobbies/Reading-List/#artificial-intelligence","text":"","title":"Artificial Intelligence"},{"location":"Hobbies/Reading-List/#musts","text":"Cybernetics What Computers Still Can't Do The Deep Learning Revolution A Life in Cybernetics","title":"Musts"},{"location":"Hobbies/Reading-List/#maybes","text":"Artificial Intelligence: The Very Idea Artifical Minds Made-Up Minds: A Constructivist Approach to Artificial Intelligence Understanding Intelligence","title":"Maybes"},{"location":"Hobbies/Reading-List/#applied-machine-learning","text":"","title":"Applied Machine Learning"},{"location":"Hobbies/Reading-List/#musts_1","text":"Introduction to Machine Learning with Python","title":"Musts"},{"location":"Hobbies/Reading-List/#maybes_1","text":"Applied Text Analysis with Python Natural Language Processing with Python Natural Language Processing with PyTorch Deep Learning Cookbook Thoughtful Machine Learning with Python","title":"Maybes"},{"location":"Hobbies/Reading-List/#circuit-design-electronics","text":"","title":"Circuit Design / Electronics"},{"location":"Hobbies/Reading-List/#musts_2","text":"Electronics For Dummies, 3rd Edition","title":"Musts"},{"location":"Hobbies/Reading-List/#maybes_2","text":"Circuitbuilding Do-It-Yourself For Dummies Complete Electronics Self-Teaching Guide with Projects","title":"Maybes"},{"location":"Hobbies/Reading-List/#systems-administration","text":"","title":"Systems Administration"},{"location":"Hobbies/Reading-List/#musts_3","text":"AWS System Administration Kubernetes: Up and Running","title":"Musts"},{"location":"Hobbies/Reading-List/#maybes_3","text":"Cloud Native Infrastructure DNS and BIND, 5th Edition","title":"Maybes"},{"location":"Hobbies/Reading-List/#programming","text":"","title":"Programming"},{"location":"Hobbies/Reading-List/#musts_4","text":"Making Software: What Really Works, and Why We Believe It Elegant SciPy Testing Python: Applying Unit Testing, TDD, BDD and Acceptance Testing Test-Driven Development with Python, 2nd Edition Algorithms in a Nutshell, 2nd Edition Fluent Python","title":"Musts"},{"location":"Hobbies/Reading-List/#maybes_4","text":"Cracking Codes with Python Designing Distributed Systems The Art of Readable Code 97 Things Every Programmer Should Know","title":"Maybes"},{"location":"Hobbies/Reading-List/#operating-systems","text":"","title":"Operating Systems"},{"location":"Hobbies/Reading-List/#musts_5","text":"Modern Operating Systems","title":"Musts"},{"location":"Hobbies/Reading-List/#maybes_5","text":"Linux Programming Interface","title":"Maybes"},{"location":"Hobbies/Reading-List/#mathematicsstatistics","text":"Think Bayes","title":"Mathematics/Statistics"},{"location":"Hobbies/Reading-List/#imaging","text":"Digital Image Processing, 4th Edition","title":"Imaging"},{"location":"Hobbies/Reading-List/#personal-finance","text":"Get a Financial Life: Personal Finance in Your Twenties and Thirties","title":"Personal Finance"},{"location":"Hobbies/Reading-List/#popular-science","text":"This Is Your Brain on Parasites: How Tiny Creatures Manipulate Our Behavior and Shape Society The Big Picture: On the Origins of Life, Meaning, and the Universe Itself","title":"Popular Science"},{"location":"Hobbies/Retroscaping/","text":"Retroscaping In addition to Retrocomputing and VintageScientific-Computing , I have an interest in history more broadly. Specifically, I am interested in historical reconstruction (a la the holodeck in Star Trek: TNG) and the potential of VR to help facilitate accurate depictions of past places and cultures. I propose to use the noun retroscape (\"A landscape or setting filled with things from the past.\" according to Wiktionary ) as a verb to refer to such virtual reconstructions. I believe in using historical data aggregated from multiple sources (archives, historical societies, flea markets, cartographers, etc) to re-animate humanity's collective past. Below I list links related to the fields and tools that are most necessary to facilitate this interest. Geography NYPL Surveyor : A crowdsourced tool that allows you to enrich the metadata of historical photographs by giving them an associated geolocation. Currently only scoped to NYC. NYPL Warper : A crowdsourced georectification tool. I keep track of maps I've rectified at Georectification . USGS Topographic Maps USGS Historical Topographic Maps (also here Photogrammetry Photogrammetry is the science/engineering/art of generating 3d objects by inferring their shape from 2d photos. Photogrammetry Subreddit Introduction to Photogrammetry by T. Schenck Recommendations for intro to photogrammetry Meshroom Photography NYPL Surveyor (see above) 80s.nyc Collection of Stereoscopic views 360 Degree Photography Google Jump used to seem pretty cool, until Google's ADHD kicked in VR180 is Google's latest venture (because I guess the full 360 degrees was too much to ask) I stand by my trusty Ricoh Theta V. Conversion of Illustrations and Grayscale Photographs to Photorealistic 3d Models DeOldify Related Examples from the Wild Motherless Brooklyn's recreation of Penn Station Virtual World Heritage Lab","title":"Retroscaping"},{"location":"Hobbies/Retroscaping/#retroscaping","text":"In addition to Retrocomputing and VintageScientific-Computing , I have an interest in history more broadly. Specifically, I am interested in historical reconstruction (a la the holodeck in Star Trek: TNG) and the potential of VR to help facilitate accurate depictions of past places and cultures. I propose to use the noun retroscape (\"A landscape or setting filled with things from the past.\" according to Wiktionary ) as a verb to refer to such virtual reconstructions. I believe in using historical data aggregated from multiple sources (archives, historical societies, flea markets, cartographers, etc) to re-animate humanity's collective past. Below I list links related to the fields and tools that are most necessary to facilitate this interest.","title":"Retroscaping"},{"location":"Hobbies/Retroscaping/#geography","text":"NYPL Surveyor : A crowdsourced tool that allows you to enrich the metadata of historical photographs by giving them an associated geolocation. Currently only scoped to NYC. NYPL Warper : A crowdsourced georectification tool. I keep track of maps I've rectified at Georectification . USGS Topographic Maps USGS Historical Topographic Maps (also here","title":"Geography"},{"location":"Hobbies/Retroscaping/#photogrammetry","text":"Photogrammetry is the science/engineering/art of generating 3d objects by inferring their shape from 2d photos. Photogrammetry Subreddit Introduction to Photogrammetry by T. Schenck Recommendations for intro to photogrammetry Meshroom","title":"Photogrammetry"},{"location":"Hobbies/Retroscaping/#photography","text":"NYPL Surveyor (see above) 80s.nyc Collection of Stereoscopic views","title":"Photography"},{"location":"Hobbies/Retroscaping/#360-degree-photography","text":"Google Jump used to seem pretty cool, until Google's ADHD kicked in VR180 is Google's latest venture (because I guess the full 360 degrees was too much to ask) I stand by my trusty Ricoh Theta V.","title":"360 Degree Photography"},{"location":"Hobbies/Retroscaping/#conversion-of-illustrations-and-grayscale-photographs-to-photorealistic-3d-models","text":"DeOldify","title":"Conversion of Illustrations and Grayscale Photographs to Photorealistic 3d Models"},{"location":"Hobbies/Retroscaping/#related-examples-from-the-wild","text":"Motherless Brooklyn's recreation of Penn Station Virtual World Heritage Lab","title":"Related Examples from the Wild"},{"location":"Hobbies/Retrocomputing/","text":"Retrocomputing Pages Articles Lore The Chronicle of Michael Spindler at Apple (i.e., corporate politics galore) Links Winworld Macintosh Garden The Unix Tree MULTICS Source Code CTSS Source Code","title":"Index"},{"location":"Hobbies/Retrocomputing/#retrocomputing-pages","text":"","title":"Retrocomputing Pages"},{"location":"Hobbies/Retrocomputing/#articles","text":"","title":"Articles"},{"location":"Hobbies/Retrocomputing/#lore","text":"The Chronicle of Michael Spindler at Apple (i.e., corporate politics galore)","title":"Lore"},{"location":"Hobbies/Retrocomputing/#links","text":"Winworld Macintosh Garden The Unix Tree MULTICS Source Code CTSS Source Code","title":"Links"},{"location":"Hobbies/Retrocomputing/Mac-OSClassic/","text":"Mac OS Classic Implementation Details What language is OS 9 written in? Regarding the ROM... Wikipedia article on Mac OS Classic Inside Macintosh (on archive.org here Macintosh Programmer's Introduction to the Macintosh Family (also here and here and here ) Vintage Apple's Mac Programming Books PowerPC History - in particular, this discusses in some detail the 68k emulator \"Question from a non-programmer\" History Lowend Mac's article on Pink / Taligent On the Sound Design of TalOS (includes a screenshot from a TalOS demo) A Technical History of Apple's Operating Systems Community Apparently, Mac OS 9 is very popular amongst audio engineers that want to create very low-budget setups. An OS 9 odyssey: Why these Mac users won\u2019t abandon 16-year-old software ArsTechnica Forums - Research for the above article Mac OS 9 Lives Projects QEMU wiki page on PPC support Apparent current state of QEMU support for PPC / last git commits Classilla","title":"Mac OSClassic"},{"location":"Hobbies/Retrocomputing/Mac-OSClassic/#mac-os-classic","text":"","title":"Mac OS Classic"},{"location":"Hobbies/Retrocomputing/Mac-OSClassic/#implementation-details","text":"What language is OS 9 written in? Regarding the ROM... Wikipedia article on Mac OS Classic Inside Macintosh (on archive.org here Macintosh Programmer's Introduction to the Macintosh Family (also here and here and here ) Vintage Apple's Mac Programming Books PowerPC History - in particular, this discusses in some detail the 68k emulator \"Question from a non-programmer\"","title":"Implementation Details"},{"location":"Hobbies/Retrocomputing/Mac-OSClassic/#history","text":"Lowend Mac's article on Pink / Taligent On the Sound Design of TalOS (includes a screenshot from a TalOS demo) A Technical History of Apple's Operating Systems","title":"History"},{"location":"Hobbies/Retrocomputing/Mac-OSClassic/#community","text":"Apparently, Mac OS 9 is very popular amongst audio engineers that want to create very low-budget setups. An OS 9 odyssey: Why these Mac users won\u2019t abandon 16-year-old software ArsTechnica Forums - Research for the above article Mac OS 9 Lives","title":"Community"},{"location":"Hobbies/Retrocomputing/Mac-OSClassic/#projects","text":"QEMU wiki page on PPC support Apparent current state of QEMU support for PPC / last git commits Classilla","title":"Projects"},{"location":"Hobbies/Retrocomputing/Vintage-Scientific-Computing/","text":"Vintage Scientific Computing I'm using this page to track pages related to the intersection of two of my interests: scientific computing and retrocomputing. Together, these interests make a pretty niche topic MATLAB Here are some posts on the history of MATLAB from the official MathWorks blog: Wilkinson and Reinsch Handbook on Linear Algebra LINPACK, Linear Equation Package EISPACK, Matrix Eigensystem Routines MATLAB History, PC-MATLAB Version 1.0 Modern MATLAB, part 1 Modern MATLAB, part 2 The MATLAB Technical Computing Environment MultiMATLAB - Interesting for the large number of MATLAB to C compilers it cites. SAS SAS Institute/FDA Intellectual Partnership for Efficient Regulated Research Data Archival and Analyses Selection from \"Strength in Numbers: The Rising of Academic Statistics Departments in the U. S.\" SAS grows from an on-campus project to an independent company (oral history) Anthony Barr's SAS History SAS: A hard-to-define product but simple success SPSS The introduction to SPSS: statistical package for the social sciences (ISBN: 0070465312) has a lot of historical context for the development of SPSS History of SPSS","title":"Vintage Scientific Computing"},{"location":"Hobbies/Retrocomputing/Vintage-Scientific-Computing/#vintage-scientific-computing","text":"I'm using this page to track pages related to the intersection of two of my interests: scientific computing and retrocomputing. Together, these interests make a pretty niche topic","title":"Vintage Scientific Computing"},{"location":"Hobbies/Retrocomputing/Vintage-Scientific-Computing/#matlab","text":"Here are some posts on the history of MATLAB from the official MathWorks blog: Wilkinson and Reinsch Handbook on Linear Algebra LINPACK, Linear Equation Package EISPACK, Matrix Eigensystem Routines MATLAB History, PC-MATLAB Version 1.0 Modern MATLAB, part 1 Modern MATLAB, part 2 The MATLAB Technical Computing Environment MultiMATLAB - Interesting for the large number of MATLAB to C compilers it cites.","title":"MATLAB"},{"location":"Hobbies/Retrocomputing/Vintage-Scientific-Computing/#sas","text":"SAS Institute/FDA Intellectual Partnership for Efficient Regulated Research Data Archival and Analyses Selection from \"Strength in Numbers: The Rising of Academic Statistics Departments in the U. S.\" SAS grows from an on-campus project to an independent company (oral history) Anthony Barr's SAS History SAS: A hard-to-define product but simple success","title":"SAS"},{"location":"Hobbies/Retrocomputing/Vintage-Scientific-Computing/#spss","text":"The introduction to SPSS: statistical package for the social sciences (ISBN: 0070465312) has a lot of historical context for the development of SPSS History of SPSS","title":"SPSS"},{"location":"Home-Lab/Backups/Blu-Ray-Backup/","text":"Notes on Backing Up My Hard Disk to Blu-Ray Commands to Follow To distribute files into more or less evenly sized bins (as you would when having a backup span multiple disks: dirsplit -e4 -s 25025314816b $PATH_TO_DIR_TO_BACK_UP dirsplit generates multiple lists of which files will live on which disk. You can preview how much each disk space will ultimately be consumed with: cut -d= -f2 $PATH_TO_DIRSPLIT_LIST | tr '\\n' '\\0' | du -sch --files0-from=- And you can use the following to generate the actual image files. Note that this will produce a hybrid UDF/ISO-9660 image file, files that are larger than 4 GB will appear to be truncated when read by drives that do not support UDF (see here and here . mkisofs -udf -V $VOLNAME -D -r --joliet-long -graft-points -allow-limited-size -path-list $PATH_TO_DIRSPLIT_LIST -o $ISONAME Once an image is generated, copy it down to Bruno (or equivalent desktop/laptop). Use scp to get a progress bar, and to eliminate some of the rsync overhead (not as necessary when there's only one file and file metadata doesn't matter). scp jpellman@maxwell.so:$ISOPATH . Timing Here are some approximate benchmarks for how long the above steps take: Step Time Range Making an ISO Image 3-5 minutes Downloading an ISO Image 7-10 minutes Burning an ISO Image 60-70 minutes Speeds based off this LG Blu-Ray Writer .","title":"Blu Ray Backup"},{"location":"Home-Lab/Backups/Blu-Ray-Backup/#notes-on-backing-up-my-hard-disk-to-blu-ray","text":"","title":"Notes on Backing Up My Hard Disk to Blu-Ray"},{"location":"Home-Lab/Backups/Blu-Ray-Backup/#commands-to-follow","text":"To distribute files into more or less evenly sized bins (as you would when having a backup span multiple disks: dirsplit -e4 -s 25025314816b $PATH_TO_DIR_TO_BACK_UP dirsplit generates multiple lists of which files will live on which disk. You can preview how much each disk space will ultimately be consumed with: cut -d= -f2 $PATH_TO_DIRSPLIT_LIST | tr '\\n' '\\0' | du -sch --files0-from=- And you can use the following to generate the actual image files. Note that this will produce a hybrid UDF/ISO-9660 image file, files that are larger than 4 GB will appear to be truncated when read by drives that do not support UDF (see here and here . mkisofs -udf -V $VOLNAME -D -r --joliet-long -graft-points -allow-limited-size -path-list $PATH_TO_DIRSPLIT_LIST -o $ISONAME Once an image is generated, copy it down to Bruno (or equivalent desktop/laptop). Use scp to get a progress bar, and to eliminate some of the rsync overhead (not as necessary when there's only one file and file metadata doesn't matter). scp jpellman@maxwell.so:$ISOPATH .","title":"Commands to Follow"},{"location":"Home-Lab/Backups/Blu-Ray-Backup/#timing","text":"Here are some approximate benchmarks for how long the above steps take: Step Time Range Making an ISO Image 3-5 minutes Downloading an ISO Image 7-10 minutes Burning an ISO Image 60-70 minutes Speeds based off this LG Blu-Ray Writer .","title":"Timing"},{"location":"Home-Lab/Backups/General-Backup-Strategy/","text":"This is a brief summary for how I envision backups will work with my home infrastructure. My primary goal with my backup strategy is to minimize the fallout from a natural disaster or burglary, and ensure that I won't be totally screwed if my house burns down or I'm mugged. While neither of these scenarios are super common, living in a large city with a large population of people makes the improbable inevitable... My backup strategy consists of the following for my computer: Generating one encrypted, compressed full ZFS snapshot and sending it to AWS once a year and tiering it to Deep Glacier after 2 days ( AWS-Backup . Sending encrypted, compressed incremental ZFS snapshots to AWS once every month ( AWS-Backup . Retaining 2 years worth of encrypted, compressed snapshots in AWS ( AWS-Backup . Making full backups of my disk to blu ray once every 2-4 years and storing these backups offsite ( Blu-Ray-Backup . Scanning paper documents using my document scanner and saving them to /datastore/documents on Maxwell (which also holds regular Excel/Word/CSV documents). Syncing /datastore/documents (essential documents from the apartment / my life in general) up to Google Drive (it was necessary to purchase 100GB on Google One for this, although the documents themselves are quite small). For my smartphone: Syncing all photos to Google Drive, where they are then copied down to Maxwell using rclone or similar at some regular interval. I may delete these manually sporadically, just to ensure that they're on hand (basically just treating Drive how I treat my current phone's storage) Syncing all my audio diaries to Google Drive, where they are similarly sync'd down to Maxwell, but then deleted afterwards from Drive (no reason for them to linger there). Syncing all my texts to GMail using IMAP with SMS Backup+ (IMAP is necessary because Google broke its API) For my online data: Syncing all my tweets to Maxwell using t and some scripts. Syncing my email to Maxwell using gmvault (with a workaround because, again, Google broke their own functionality)","title":"General Backup Strategy"},{"location":"Home-Lab/Hosts/Johns-MBP/","text":"Johns-MBP Description Johns-MBP (formerly known as Bruno) is a Mid 2012, 13-inch MacBook Pro . I bought it refurbished around 1/1/15 and it is still kicking. Prior to 2021, it had several issues, some of which have been corrected while the others I've come to tolerate. It currently dual boots into both macOS Catalina and macOS High Sierra. Catalina is for everyday use (web browsing, etc) because it continues to receive security updates (as of early 2022; Apple will likely stop providing security updates in November 2022, but who really knows what their timelines are). That said, there are some caveats to how quickly Apple provides patches for Catalina (see here ). The High Sierra boot partition is only used for the following applications: iMovie HD 6; this is being run in an incredibly hacky way in order to rip a bunch of older DV tapes in my mom's house, and newer versions of OS X deprecate at least one Cocoa API call that iMovie HD 6 depends on. This might no longer be necessary, since I think I ripped all the DV tapes at my mom's house in late 2020, but maybe not. 32-bit OS X apps a.k.a. most of my Steam library (see here ). Kind of a reasonable idea, but also a very unnecessary and brusque move on Apple's part (even thought the decision is far in the past at this point). Catalina has the following applications installed: Vagrant VMWare Fusion Desktop Docker Desktop Kodi Malwarebytes Signal Veracrypt The overall partition scheme is such that there are several major APFS volumes: High Sierra boot volume Catalina boot volume Logical volume for Google Drive mirror / sync Logical volume to hold personal multimedia (i.e., home videos, digitized DV tapes, digitzed photos, contemporary photos/videos from phone) Logical volume to hold VMs Separating my data into distinct APFS volumes makes it easier to perform selective backups. What's Broken One of the RAM slots is bad (see here and here ). Essentially Apple used two different types of solder for the RAM banks and one of them is more liable to crap out. As such I only use one RAM slot now, with half of the MacBook's original RAM (4 GB). In theory I could purchase a single 8 GB SODIMM to fill in this one slot, but I've never felt it to be particularly worth it / performance with 4 GB RAM has been good enough for my purposes (90% of the gaming I do involves 2D graphics only). What's Fixed Modern versions of OS X seem to assume that they're running on SSDs ( APFS definitely makes this assumption ) and don't run well at all on mechanical disks. To work around this, I switched out Johns-MBP's spinning disk to an SSD. Of course, this might just be because I've become so used to SSDs that I can't tolerate the slow speeds of spinning disks, although anecdotally some other people blame Apple as well. Touchpad was replaced around 9/17/21 ( part ) Battery has been replaced with an aftermarket battery around 9/17/21 ( part ) Ailing DVD drive that constantly sounded like it was raspily chanting, \"Kill meeee\" has been replaced with a second SSD ( ~ 12/4/21; part ) What I Like Having an actual Ethernet port and not having to deal with the dongle hell bullshit of the newer USB-C and Thunderbolt 3 Macs. Not having to deal with that asinine / comically useless touch bar. Being able to actually fix parts / take this laptop apart. I believe that this is one of the last Mac models that you can potentially self-service or upgrade/tweak. All of the modern ones involve gluing components together in an unnecessarily thin chassis. I'd gladly trade functionality for aesthetics. Specifications Key Value CPU Core i7 # of Cores (logical) 4 CPU Clock Speed (GHz) 2.9 Memory (GB) 4 GPU Intel HD Graphics 4000 1536 MB Disks 250 GB SSD (boot volume) , 1 TB SSD (photos, home movies, etc) OS macOS High Sierra (10.13), macOS Catalina (10.15) Plans The electricity consumption on the laptop isn't that bad and it seems serviceable enough, so I'm aiming to try to get 1-2 more years of use out of it (as of January 2022). When Apple stops providing security updates for Catalina, I'll start doing web browsing in a virtualized sandbox running Neverware to mitigate risk. I'll probably replace this with an ARM-based MacBook. I've toyed around with just using a tablet (e.g., iPad, Surface) in the past, but due to the amount of multimedia work I do as I digitize documents and other media in my childhood home, I think that I'll still need a portable computer with a modest amount of horsepower.","title":"Johns MBP"},{"location":"Home-Lab/Hosts/Johns-MBP/#johns-mbp","text":"","title":"Johns-MBP"},{"location":"Home-Lab/Hosts/Johns-MBP/#description","text":"Johns-MBP (formerly known as Bruno) is a Mid 2012, 13-inch MacBook Pro . I bought it refurbished around 1/1/15 and it is still kicking. Prior to 2021, it had several issues, some of which have been corrected while the others I've come to tolerate. It currently dual boots into both macOS Catalina and macOS High Sierra. Catalina is for everyday use (web browsing, etc) because it continues to receive security updates (as of early 2022; Apple will likely stop providing security updates in November 2022, but who really knows what their timelines are). That said, there are some caveats to how quickly Apple provides patches for Catalina (see here ). The High Sierra boot partition is only used for the following applications: iMovie HD 6; this is being run in an incredibly hacky way in order to rip a bunch of older DV tapes in my mom's house, and newer versions of OS X deprecate at least one Cocoa API call that iMovie HD 6 depends on. This might no longer be necessary, since I think I ripped all the DV tapes at my mom's house in late 2020, but maybe not. 32-bit OS X apps a.k.a. most of my Steam library (see here ). Kind of a reasonable idea, but also a very unnecessary and brusque move on Apple's part (even thought the decision is far in the past at this point). Catalina has the following applications installed: Vagrant VMWare Fusion Desktop Docker Desktop Kodi Malwarebytes Signal Veracrypt The overall partition scheme is such that there are several major APFS volumes: High Sierra boot volume Catalina boot volume Logical volume for Google Drive mirror / sync Logical volume to hold personal multimedia (i.e., home videos, digitized DV tapes, digitzed photos, contemporary photos/videos from phone) Logical volume to hold VMs Separating my data into distinct APFS volumes makes it easier to perform selective backups.","title":"Description"},{"location":"Home-Lab/Hosts/Johns-MBP/#whats-broken","text":"One of the RAM slots is bad (see here and here ). Essentially Apple used two different types of solder for the RAM banks and one of them is more liable to crap out. As such I only use one RAM slot now, with half of the MacBook's original RAM (4 GB). In theory I could purchase a single 8 GB SODIMM to fill in this one slot, but I've never felt it to be particularly worth it / performance with 4 GB RAM has been good enough for my purposes (90% of the gaming I do involves 2D graphics only).","title":"What's Broken"},{"location":"Home-Lab/Hosts/Johns-MBP/#whats-fixed","text":"Modern versions of OS X seem to assume that they're running on SSDs ( APFS definitely makes this assumption ) and don't run well at all on mechanical disks. To work around this, I switched out Johns-MBP's spinning disk to an SSD. Of course, this might just be because I've become so used to SSDs that I can't tolerate the slow speeds of spinning disks, although anecdotally some other people blame Apple as well. Touchpad was replaced around 9/17/21 ( part ) Battery has been replaced with an aftermarket battery around 9/17/21 ( part ) Ailing DVD drive that constantly sounded like it was raspily chanting, \"Kill meeee\" has been replaced with a second SSD ( ~ 12/4/21; part )","title":"What's Fixed"},{"location":"Home-Lab/Hosts/Johns-MBP/#what-i-like","text":"Having an actual Ethernet port and not having to deal with the dongle hell bullshit of the newer USB-C and Thunderbolt 3 Macs. Not having to deal with that asinine / comically useless touch bar. Being able to actually fix parts / take this laptop apart. I believe that this is one of the last Mac models that you can potentially self-service or upgrade/tweak. All of the modern ones involve gluing components together in an unnecessarily thin chassis. I'd gladly trade functionality for aesthetics.","title":"What I Like"},{"location":"Home-Lab/Hosts/Johns-MBP/#specifications","text":"Key Value CPU Core i7 # of Cores (logical) 4 CPU Clock Speed (GHz) 2.9 Memory (GB) 4 GPU Intel HD Graphics 4000 1536 MB Disks 250 GB SSD (boot volume) , 1 TB SSD (photos, home movies, etc) OS macOS High Sierra (10.13), macOS Catalina (10.15)","title":"Specifications"},{"location":"Home-Lab/Hosts/Johns-MBP/#plans","text":"The electricity consumption on the laptop isn't that bad and it seems serviceable enough, so I'm aiming to try to get 1-2 more years of use out of it (as of January 2022). When Apple stops providing security updates for Catalina, I'll start doing web browsing in a virtualized sandbox running Neverware to mitigate risk. I'll probably replace this with an ARM-based MacBook. I've toyed around with just using a tablet (e.g., iPad, Surface) in the past, but due to the amount of multimedia work I do as I digitize documents and other media in my childhood home, I think that I'll still need a portable computer with a modest amount of horsepower.","title":"Plans"},{"location":"Home-Lab/Hosts/hypervisor01/","text":"hypervisor01 Description hypervisor01 (formerly known as Maxwell; I've stopped doing pet names for machines because I find such names obnoxious) is essentially a gaming workstation that I use to try out new things locally within my apartment. It runs Proxmox VE 6.2-4 off of a 240 GB SSD. I first built it around February/March 2017, with a goal of creating a workstation that I could use for both experimentation and running BOINC . Frankly, part of my motivation was to see how close I could get to surpassing (or at least equalling) some of the older scientific lab hardware I had dealt with in the past using only consumer-grade parts. Specifications Key Value CPU Core I7-6700 FC-LGA14C # of Cores (logical) 8 CPU Clock Speed (GHz) 3.40 Memory (GB) 32 Disks 240 GB SSD (Linux volume) OS Proxmox VE 6.2-4 IP Address 192.168.1.10 Ansible Playbook hypervisor.yml Part List Zalman MS800 ATX Mid Tower Gaming Case (Black) Rosewill 3 x 5.25-Inch to 4 x 3.5-Inch Hot-swap SATA III/SAS Hard Disk Drive Cage - Black SanDisk SSD PLUS 240GB Internal SSD - SATA III 6 Gb/s Intel Boxed Core I7-6700 FC-LGA14C 3.40 GHz 8 M Processor Cache 4 LGA 1151 BX80662I76700 x2 Crucial 16GB Single DDR4 2133 MT/s (PC4-17000) DR x8 Unbuffered DIMM 288-Pin Memory - CT16G4DFD8213 MSI Pro Solution Intel Z170A LGA 1151 DDR4 USB 3.1 ATX Motherboard (Z170A SLI) Corsair CX Series 750 Watt 80 Plus Bronze Certified Non-Modular Power Supply (CP-9020015-NA) EVGA GeForce GTX 1050 SC ITX, 2GB Applications KVM Proxmox nginx (reverse proxy for Proxmox) Service Accounts packerserv : Used to create VM templates using Packer's proxmox-iso builder (see here ). Repository for VM templates can be found here . Notes GPU passthrough is enabled. This could not be accomplished previously when this workstation ran CentOS because Red Hat deliberately cripples GPU passthrough for consumer-grade cards in RHEL (see here ). The GPU is assigned to one VM ( compute01 ; no article yet as of writing), which runs Folding@Home and BOINC .","title":"Hypervisor01"},{"location":"Home-Lab/Hosts/hypervisor01/#hypervisor01","text":"","title":"hypervisor01"},{"location":"Home-Lab/Hosts/hypervisor01/#description","text":"hypervisor01 (formerly known as Maxwell; I've stopped doing pet names for machines because I find such names obnoxious) is essentially a gaming workstation that I use to try out new things locally within my apartment. It runs Proxmox VE 6.2-4 off of a 240 GB SSD. I first built it around February/March 2017, with a goal of creating a workstation that I could use for both experimentation and running BOINC . Frankly, part of my motivation was to see how close I could get to surpassing (or at least equalling) some of the older scientific lab hardware I had dealt with in the past using only consumer-grade parts.","title":"Description"},{"location":"Home-Lab/Hosts/hypervisor01/#specifications","text":"Key Value CPU Core I7-6700 FC-LGA14C # of Cores (logical) 8 CPU Clock Speed (GHz) 3.40 Memory (GB) 32 Disks 240 GB SSD (Linux volume) OS Proxmox VE 6.2-4 IP Address 192.168.1.10 Ansible Playbook hypervisor.yml","title":"Specifications"},{"location":"Home-Lab/Hosts/hypervisor01/#part-list","text":"Zalman MS800 ATX Mid Tower Gaming Case (Black) Rosewill 3 x 5.25-Inch to 4 x 3.5-Inch Hot-swap SATA III/SAS Hard Disk Drive Cage - Black SanDisk SSD PLUS 240GB Internal SSD - SATA III 6 Gb/s Intel Boxed Core I7-6700 FC-LGA14C 3.40 GHz 8 M Processor Cache 4 LGA 1151 BX80662I76700 x2 Crucial 16GB Single DDR4 2133 MT/s (PC4-17000) DR x8 Unbuffered DIMM 288-Pin Memory - CT16G4DFD8213 MSI Pro Solution Intel Z170A LGA 1151 DDR4 USB 3.1 ATX Motherboard (Z170A SLI) Corsair CX Series 750 Watt 80 Plus Bronze Certified Non-Modular Power Supply (CP-9020015-NA) EVGA GeForce GTX 1050 SC ITX, 2GB","title":"Part List"},{"location":"Home-Lab/Hosts/hypervisor01/#applications","text":"KVM Proxmox nginx (reverse proxy for Proxmox)","title":"Applications"},{"location":"Home-Lab/Hosts/hypervisor01/#service-accounts","text":"packerserv : Used to create VM templates using Packer's proxmox-iso builder (see here ). Repository for VM templates can be found here .","title":"Service Accounts"},{"location":"Home-Lab/Hosts/hypervisor01/#notes","text":"GPU passthrough is enabled. This could not be accomplished previously when this workstation ran CentOS because Red Hat deliberately cripples GPU passthrough for consumer-grade cards in RHEL (see here ). The GPU is assigned to one VM ( compute01 ; no article yet as of writing), which runs Folding@Home and BOINC .","title":"Notes"},{"location":"Home-Lab/Hosts/storage01/","text":"storage01 Description storage01 is a simple NAS device. It serves files from /srv/nfs and uses lvmcache to speed up load times for commonly accessed files. lvmcache is built upon dm-cache, and it's important to note that the benefits of caching are only reaped for files that are accessed multiple times (see here ). Specifications Key Value CPU Intel(R) Celeron(R) J4005 CPU # of Cores (logical) 2 CPU Clock Speed (GHz) 2.00 Memory (GB) 4 Disks 60 GB SSD (LVM cache), 2 TB HDD OS CentOS 7 IP Address 192.168.1.4 Ansible Playbook storage.yml Kickstart Config centos7-storage01-full.cfg Part List Cooler Master Elite 110 RC-110-KKN2 Midnight Black Steel / Plastic Mini-ITX Tower Computer Case CORSAIR VS Series, VS450, 450 Watt, 80+ White Certified, Non-Modular Power Supply ASRock J4005B-ITX Western Digital 2TB WD Blue PC Hard Drive HDD Corsair Force GT 60 GB Applications NFS Server Plans I want to convert storage01 into a dedicated backup device (rather than a crappy NAS with no RAID) that regularly syncs down all of my Google Drive, emails, and other bits of data that exist non-locally. I also want to install rsnapshot on my laptop to regularly sync my personal photo/home video collection (consisting of digitized VHS tapes, DAT tapes, etc) to this backup device. Why don't I just back these up to the cloud? Well, a) they take up a huge amount of space (the digitized DAT tapes I have are all encoded as FLAC files) and b) I'm reluctant to entrust my home / personal videos to a third-party provider. I think it would make sense to install rsnapshot on the backup device as well and have it regularly take \"snapshots\" of my Google Drive / emails.","title":"Storage01"},{"location":"Home-Lab/Hosts/storage01/#storage01","text":"","title":"storage01"},{"location":"Home-Lab/Hosts/storage01/#description","text":"storage01 is a simple NAS device. It serves files from /srv/nfs and uses lvmcache to speed up load times for commonly accessed files. lvmcache is built upon dm-cache, and it's important to note that the benefits of caching are only reaped for files that are accessed multiple times (see here ).","title":"Description"},{"location":"Home-Lab/Hosts/storage01/#specifications","text":"Key Value CPU Intel(R) Celeron(R) J4005 CPU # of Cores (logical) 2 CPU Clock Speed (GHz) 2.00 Memory (GB) 4 Disks 60 GB SSD (LVM cache), 2 TB HDD OS CentOS 7 IP Address 192.168.1.4 Ansible Playbook storage.yml Kickstart Config centos7-storage01-full.cfg","title":"Specifications"},{"location":"Home-Lab/Hosts/storage01/#part-list","text":"Cooler Master Elite 110 RC-110-KKN2 Midnight Black Steel / Plastic Mini-ITX Tower Computer Case CORSAIR VS Series, VS450, 450 Watt, 80+ White Certified, Non-Modular Power Supply ASRock J4005B-ITX Western Digital 2TB WD Blue PC Hard Drive HDD Corsair Force GT 60 GB","title":"Part List"},{"location":"Home-Lab/Hosts/storage01/#applications","text":"NFS Server","title":"Applications"},{"location":"Home-Lab/Hosts/storage01/#plans","text":"I want to convert storage01 into a dedicated backup device (rather than a crappy NAS with no RAID) that regularly syncs down all of my Google Drive, emails, and other bits of data that exist non-locally. I also want to install rsnapshot on my laptop to regularly sync my personal photo/home video collection (consisting of digitized VHS tapes, DAT tapes, etc) to this backup device. Why don't I just back these up to the cloud? Well, a) they take up a huge amount of space (the digitized DAT tapes I have are all encoded as FLAC files) and b) I'm reluctant to entrust my home / personal videos to a third-party provider. I think it would make sense to install rsnapshot on the backup device as well and have it regularly take \"snapshots\" of my Google Drive / emails.","title":"Plans"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/","text":"Documentation Concerning Migration from Linode to AWS General Notes I killed the Wordpress service because I never used it and MoinMoin was serving me well for what I wanted (a place to throw my ideas) anyways. AMI Used and Dependencies I needed to install I used a fresh CentOS 7 AMI: ami-2051294a. wget was not installed, nor was vim ( yum install wget vim ). Wanted to install htop too, but EPEL repos not available. Followed instructions to get EPEL from here . Step 3 is incorrect- rpm \u2013ivh epel-release-7-5.noarch.rpm should be rpm epel-release-7-5.noarch.rpm -ivh for some befuddling reason lost to the depths of time. The former command will print a help screen; the latter does what it's supposed to do. Apache Migration Notes Ran yum install httpd . Removed default default /etc/httpd. rsync'd /etc/httpd from old VPS. Had to remove PHP config files left from Wordpress ( rm /etc/httpd/conf.modules.d/10-php.conf ; rm /etc/httpd/conf.d/php.conf ). Cloned main branch of this repo into /var/www/html. MoinMoin Migration Notes I re-installed MoinMoin 1.9.8 on the AMI (also wsgi; yum install wsgi ). Removed default /usr/local/share/moin and rsync'd /usr/local/share/moin from old VPS. The AWS AMI has SELinux enabled by default. This caused some issues with MoinMoin being able to open up the data directory. See here for resolution. Started Apache ( service start httpd ). Primitive Backup Migrated primitive backup scripts and last 10 backups to /usr/local/bin/backup. Created a dedicated user with passwordless ssh for so that I can rsync tar'd backups directly to my home computer nightly using a cronjob. Added the following line to root's crontab: 35 20 * * 0,4 /usr/local/bin/backup/bin/fullBackup.sh This backup system is very rudimentary and exists as it does primarily because I don't want to spend more money on this than it is worth. It's mostly so that the Wiki doesn't get totally zapped if something happens to the Amazon instance. I can always start using snapshots if this gets more serious (and/or if I have more disposable income). Misc Updated DNS entry to point to new IP address. Archived home directories to home laptop (these looked like trash, but I decided to keep them at least for a bit as a precaution).","title":"Documentation Concerning Migration from Linode to AWS"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/#documentation-concerning-migration-from-linode-to-aws","text":"","title":"Documentation Concerning Migration from Linode to AWS"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/#general-notes","text":"I killed the Wordpress service because I never used it and MoinMoin was serving me well for what I wanted (a place to throw my ideas) anyways.","title":"General Notes"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/#ami-used-and-dependencies-i-needed-to-install","text":"I used a fresh CentOS 7 AMI: ami-2051294a. wget was not installed, nor was vim ( yum install wget vim ). Wanted to install htop too, but EPEL repos not available. Followed instructions to get EPEL from here . Step 3 is incorrect- rpm \u2013ivh epel-release-7-5.noarch.rpm should be rpm epel-release-7-5.noarch.rpm -ivh for some befuddling reason lost to the depths of time. The former command will print a help screen; the latter does what it's supposed to do.","title":"AMI Used and Dependencies I needed to install"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/#apache-migration-notes","text":"Ran yum install httpd . Removed default default /etc/httpd. rsync'd /etc/httpd from old VPS. Had to remove PHP config files left from Wordpress ( rm /etc/httpd/conf.modules.d/10-php.conf ; rm /etc/httpd/conf.d/php.conf ). Cloned main branch of this repo into /var/www/html.","title":"Apache Migration Notes"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/#moinmoin-migration-notes","text":"I re-installed MoinMoin 1.9.8 on the AMI (also wsgi; yum install wsgi ). Removed default /usr/local/share/moin and rsync'd /usr/local/share/moin from old VPS. The AWS AMI has SELinux enabled by default. This caused some issues with MoinMoin being able to open up the data directory. See here for resolution. Started Apache ( service start httpd ).","title":"MoinMoin Migration Notes"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/#primitive-backup","text":"Migrated primitive backup scripts and last 10 backups to /usr/local/bin/backup. Created a dedicated user with passwordless ssh for so that I can rsync tar'd backups directly to my home computer nightly using a cronjob. Added the following line to root's crontab: 35 20 * * 0,4 /usr/local/bin/backup/bin/fullBackup.sh This backup system is very rudimentary and exists as it does primarily because I don't want to spend more money on this than it is worth. It's mostly so that the Wiki doesn't get totally zapped if something happens to the Amazon instance. I can always start using snapshots if this gets more serious (and/or if I have more disposable income).","title":"Primitive Backup"},{"location":"Home-Lab/Legacy/AWS-Migration-Documentation/#misc","text":"Updated DNS entry to point to new IP address. Archived home directories to home laptop (these looked like trash, but I decided to keep them at least for a bit as a precaution).","title":"Misc"},{"location":"Home-Lab/Legacy/Cluster-Documentation/","text":"Documentation for Home Beowulf Cluster - SDH (The Salon of Digital Humanists) An inventory of all the computers used in my home cluster, including their specs, software installed, the present topology of the cluster, and the users. Section 1: Machine Specifications Hugo (head node) Key Value Brand Apple Macbook Pro (circa 2008) CPU 2.4 Ghz Intel Core 2 Duo (64-bit) Cores 2 RAM 2 GB Graphics Card NVIDIA GeForce 8600M GT Hard Drive 142 GB (according to df) Voltaire Key Value Brand Apple iBook G4 (circa 2006) CPU 1.33 Ghz PowerPC 7447a (32-bit) Cores 1 RAM 512 MB Graphics Card ATI Mobility Radeon 9550 Hard Drive 40GB (36 GB according to df) Montaigne Key Value Brand Custom Made (circa 2002) CPU 900 Mhz AMD Duron (32-bit) Cores 1 RAM 512 MB SDRAM Graphics Card (2) ATI Rage 128 and ATI Radeon 9000 Hard Drive 15.3 GB (14 GB according to df) Cervantes Key Value Brand Macbook Pro (circa 2011) CPU 2.5 Ghz Intel Core 2 Duo (64-bit) Cores 2 RAM 4 GB (hypervisor); 2 GB (VM) Graphics Card NVIDIA GeForce 8600M GT (128 MB video memory for VM) Hard Drive 320 GB (hypervisor); 181 GB (VM) Section 2: Software Installed Hugo (head node) nfs-kernel-server OpenSSH PostgreSQL 8.4 MPICH R 2.10.1 (r-base) dsh slapd, ldap-utils SLURM (not presently configured) Voltaire nfs-common OpenSSH PostgreSQL 8.4 MPICH R 2.10.1 (r-base) dsh slapd, ldap-utils Montaigne nfs-common OpenSSH PostgreSQL 8.4 MPICH R 2.10.1 (r-base) dsh slapd, ldap-utils Cervantes nfs-kernel-server nfs-common OpenSSH PostgreSQL 8.4 SLURM (not presently configured) dpkg-server Section 3: Network Topology Hugo, Voltaire, Montaigne and Cervantes are connected to a 6 port hub, with 4 ports used. 1 port is broken. The last port is an unused uplink port. All nodes can connect to each other. Cervantes can connect outside of the LAN using Wifi, and is thus used as a means of downloading packages from the Canonical repos. Cervantes has a user ( installuser ) whose home directory is mounted as an nfs partition on the other nodes- this user's home directory contains .deb files and a Packages.gz file. Section 4: Users Hugo (head node) jpellman mpiuser (home folder is nfs mount; hugo is nfs server), uid=999 installuser (home folder is nfs mount; fstab modified), uid=998 Voltaire jpellman mpiuser (home folder is nfs mount; fstab modified), uid=999 installuser (home folder is nfs mount; fstab modified), uid=998 Montaigne jpellman mpiuser (home folder is nfs mount; fstab modified), uid=999 installuser (home folder is nfs mount; fstab modified), uid=998 Cervantes jpellman mpiuser (home folder is nfs mount; fstab modified), uid=999 installuser (home folder is nfs mount; cervantes is nfs server), uid=998 TODO Install SLURM. Configure SLURM on all nodes. Complication: Version of SLURM installed on Cervantes and Hugo is 64-bit. Will need to recompile SLURM for 32-bit if possible.","title":"Documentation for Home Beowulf Cluster - SDH (The Salon of Digital Humanists)"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#documentation-for-home-beowulf-cluster-sdh-the-salon-of-digital-humanists","text":"An inventory of all the computers used in my home cluster, including their specs, software installed, the present topology of the cluster, and the users.","title":"Documentation for Home Beowulf Cluster - SDH (The Salon of Digital Humanists)"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#section-1-machine-specifications","text":"","title":"Section 1: Machine Specifications"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#hugo-head-node","text":"Key Value Brand Apple Macbook Pro (circa 2008) CPU 2.4 Ghz Intel Core 2 Duo (64-bit) Cores 2 RAM 2 GB Graphics Card NVIDIA GeForce 8600M GT Hard Drive 142 GB (according to df)","title":"Hugo (head node)"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#voltaire","text":"Key Value Brand Apple iBook G4 (circa 2006) CPU 1.33 Ghz PowerPC 7447a (32-bit) Cores 1 RAM 512 MB Graphics Card ATI Mobility Radeon 9550 Hard Drive 40GB (36 GB according to df)","title":"Voltaire"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#montaigne","text":"Key Value Brand Custom Made (circa 2002) CPU 900 Mhz AMD Duron (32-bit) Cores 1 RAM 512 MB SDRAM Graphics Card (2) ATI Rage 128 and ATI Radeon 9000 Hard Drive 15.3 GB (14 GB according to df)","title":"Montaigne"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#cervantes","text":"Key Value Brand Macbook Pro (circa 2011) CPU 2.5 Ghz Intel Core 2 Duo (64-bit) Cores 2 RAM 4 GB (hypervisor); 2 GB (VM) Graphics Card NVIDIA GeForce 8600M GT (128 MB video memory for VM) Hard Drive 320 GB (hypervisor); 181 GB (VM)","title":"Cervantes"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#section-2-software-installed","text":"","title":"Section 2: Software Installed"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#hugo-head-node_1","text":"nfs-kernel-server OpenSSH PostgreSQL 8.4 MPICH R 2.10.1 (r-base) dsh slapd, ldap-utils SLURM (not presently configured)","title":"Hugo (head node)"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#voltaire_1","text":"nfs-common OpenSSH PostgreSQL 8.4 MPICH R 2.10.1 (r-base) dsh slapd, ldap-utils","title":"Voltaire"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#montaigne_1","text":"nfs-common OpenSSH PostgreSQL 8.4 MPICH R 2.10.1 (r-base) dsh slapd, ldap-utils","title":"Montaigne"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#cervantes_1","text":"nfs-kernel-server nfs-common OpenSSH PostgreSQL 8.4 SLURM (not presently configured) dpkg-server","title":"Cervantes"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#section-3-network-topology","text":"Hugo, Voltaire, Montaigne and Cervantes are connected to a 6 port hub, with 4 ports used. 1 port is broken. The last port is an unused uplink port. All nodes can connect to each other. Cervantes can connect outside of the LAN using Wifi, and is thus used as a means of downloading packages from the Canonical repos. Cervantes has a user ( installuser ) whose home directory is mounted as an nfs partition on the other nodes- this user's home directory contains .deb files and a Packages.gz file.","title":"Section 3: Network Topology"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#section-4-users","text":"","title":"Section 4: Users"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#hugo-head-node_2","text":"jpellman mpiuser (home folder is nfs mount; hugo is nfs server), uid=999 installuser (home folder is nfs mount; fstab modified), uid=998","title":"Hugo (head node)"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#voltaire_2","text":"jpellman mpiuser (home folder is nfs mount; fstab modified), uid=999 installuser (home folder is nfs mount; fstab modified), uid=998","title":"Voltaire"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#montaigne_2","text":"jpellman mpiuser (home folder is nfs mount; fstab modified), uid=999 installuser (home folder is nfs mount; fstab modified), uid=998","title":"Montaigne"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#cervantes_2","text":"jpellman mpiuser (home folder is nfs mount; fstab modified), uid=999 installuser (home folder is nfs mount; cervantes is nfs server), uid=998","title":"Cervantes"},{"location":"Home-Lab/Legacy/Cluster-Documentation/#todo","text":"Install SLURM. Configure SLURM on all nodes. Complication: Version of SLURM installed on Cervantes and Hugo is 64-bit. Will need to recompile SLURM for 32-bit if possible.","title":"TODO"},{"location":"Home-Lab/Legacy/Libjpel-Wiki/","text":"Update: I gave up on using Gollum. The benefits of using it were marginal at best vs just editing Markdown directly, and it didn't play well with my mkdocs setup (though I think this could have been fixed by telling MkDocs to look for Markdown files in a gollum subdirectory within this repository; I have no patience for this kind of tomfoolery / convoluted workarounds though). I recently switched from MoinMoin to Gollum for my personal wiki. I'm using this page to list my justifications, as well as include any notes related to the transition. Reasons to Use Gollum Over MoinMoin I was the only user of my wiki, which kind of goes against the spirit of a proper wiki in the first place. My MoinMoin instance lived on a reserved AWS t3.micro instance, which was costing me more money than I thought was justifiable for what I was getting out of it (at least for this use case). Since I already paid for a year upfront for this, I'm thinking of converting the reserved instance into a BOINC machine . Regardless of any security measures I took (i.e., using AWS security groups, changing the SSH port- this only really improves security marginally at the expense of usability and might not have been super necessary since AWS enforces the use of key pairs, but I did it anyways because I was the only person using the machine and it didn't hurt) I would still get pounded by brute force login attempts on the (public-facing) web app's login page. I wanted to find a way to share my notes while not having to deal with bad actors at all, and having the wiki exist as a static site, with editing done locally on machines with Gollum installed, seemed like the best way to accomplish this. Using a static site should pretty much get rid of the attack space entirely on my end- since I'm planning on serving this with a gh-pages branch, security becomes GitHub's problem and I'm okay with that. TODO Find a way to convert Gollum pages into HTML automatically via a Travis job. I was thinking about using gollum-site , but it seems moribund / hasn't been updated in 6 years and I don't trust it. Most likely I'll use a Python script to convert Gollum-specific syntax into Markdown, and then use Pandoc to render HTML versions of all of the Markdown docs in wiki repository. Find a way to automatically build a tree that people can navigate in the static site version. Update any misc leftover MoinMoin syntax that is lingering about. Remove the MoinMoin category designations at the end of some documents. I'm thinking that I'll drop a Gollum footer file in each of the sub-directories within the wiki repo that contains representation of where you're at in the tree, and that will be rendered instead. Fix sidebar and links on main page.","title":"Libjpel Wiki"},{"location":"Home-Lab/Legacy/Libjpel-Wiki/#reasons-to-use-gollum-over-moinmoin","text":"I was the only user of my wiki, which kind of goes against the spirit of a proper wiki in the first place. My MoinMoin instance lived on a reserved AWS t3.micro instance, which was costing me more money than I thought was justifiable for what I was getting out of it (at least for this use case). Since I already paid for a year upfront for this, I'm thinking of converting the reserved instance into a BOINC machine . Regardless of any security measures I took (i.e., using AWS security groups, changing the SSH port- this only really improves security marginally at the expense of usability and might not have been super necessary since AWS enforces the use of key pairs, but I did it anyways because I was the only person using the machine and it didn't hurt) I would still get pounded by brute force login attempts on the (public-facing) web app's login page. I wanted to find a way to share my notes while not having to deal with bad actors at all, and having the wiki exist as a static site, with editing done locally on machines with Gollum installed, seemed like the best way to accomplish this. Using a static site should pretty much get rid of the attack space entirely on my end- since I'm planning on serving this with a gh-pages branch, security becomes GitHub's problem and I'm okay with that.","title":"Reasons to Use Gollum Over MoinMoin"},{"location":"Home-Lab/Legacy/Libjpel-Wiki/#todo","text":"Find a way to convert Gollum pages into HTML automatically via a Travis job. I was thinking about using gollum-site , but it seems moribund / hasn't been updated in 6 years and I don't trust it. Most likely I'll use a Python script to convert Gollum-specific syntax into Markdown, and then use Pandoc to render HTML versions of all of the Markdown docs in wiki repository. Find a way to automatically build a tree that people can navigate in the static site version. Update any misc leftover MoinMoin syntax that is lingering about. Remove the MoinMoin category designations at the end of some documents. I'm thinking that I'll drop a Gollum footer file in each of the sub-directories within the wiki repo that contains representation of where you're at in the tree, and that will be rendered instead. Fix sidebar and links on main page.","title":"TODO"},{"location":"Home-Lab/Legacy/Backups/AWS-Backup/","text":"Notes on Backups of Maxwell's ZFS Pool to AWS (DEPRECATED) NOTE: I no longer run ZFS at home except for ad-hoc experiments. This is because I found ZFS to consistently be overkill for my personal needs. In general, I've been re-assessing how I go about home-labbing so that long-running infrastructure / devices are much less complex. I realized that the overhead / complexity of running ZFS didn't fit any of the use cases I have for storage in my personal life and actually was problematic (in the sense that if my loved ones needed to access my data in an emergency, no one would know what to do with a multi-disk ZFS dataset). Furthermore, I found the egress charges from AWS S3 to be not worth it ( ~ $100 worth of egress charges vs a comparable amount to just buy an external hard disk and leave it at my mom's house upstate). I'm leaving this page up for now just to document that I had this set up at one point, but it no longer reflects my contemporary configuration. An anacron script runs on Maxwell once a month (i.e., it lives in /etc/cron.monthly ). It runs a series of commands based off the StackOverflow post here to generate encrypted, compressed snapshots that live in an AWS bucket in US-East 2 (Ohio). The script only does incremental backups, and keeps track off the last incremental snapshot that was made using a text file. It also generates a script that allows me to make a manual backup to an external hard disk within my apartment. This is in case I accidentally tank my ZFS pool and need to restore from scratch, but don't want to have to wait 3-5 hours for Glacier to retrieve files. The commands used (taken from the StackOverflow post above) are as follows: Sending a full backup: zfs send -R <pool name>@<snapshot name> | gzip | gpg --no-use-agent --no-tty --passphrase-file ./passphrase -c - | aws s3 cp - s3://<bucketname>/<filename>.zfs.gz.gpg Sending an incremental backup: zfs send -R -I <pool name>@<snapshot to do incremental backup from> <pool name>@<snapshot name> | gzip | gpg --no-use-agent --no-tty --passphrase-file ./passphrase -c - | aws s3 cp - s3://<bucketname>/<filename>.zfs.gz.gpg Restoring a backup: aws s3 cp s3://<bucketname>/<filename>.zfs.gz.gpg - | gpg --no-use-agent --passphrase-file ./passphrase -d - | gunzip | sudo zfs receive <new dataset name> The specific variant of the restore command that I use looks something like this: aws s3 cp s3://jpellman-maxwell-backup-us-east-2/full-test.gz.gpg - | gpg --passphrase-file /root/.backup_pass --no-tty --batch -d - | gunzip | zfs receive pool0/test2 The initial encrypted, compressed snapshot (full backup) was created manually. The aws cli copy command includes the following flag to account for very large snapshots: --expected-size (string) This argument specifies the expected size of a stream in terms of bytes. Note that this argument is needed only when a stream is being uploaded to s3 and the size is larger than 50GB. Failure to include this argument under these conditions may result in a failed upload due to too many parts in upload. The string that's fed into this param is ESTIMATED_UPLOAD from the following: LASTSNAP_SIZE=$(zfs list -t snapshot -o name,refer -Hp | grep ${POOL} | grep ${LASTSNAP} | awk '{print $2}' | paste -sd+ - | bc) CURSNAP_SIZE=$(zfs list -t snapshot -o name,refer -Hp | grep ${POOL} | grep ${CURSNAP} | awk '{print $2}' | paste -sd+ - | bc) ESTIMATED_UPLOAD=$(( ${CURSNAP_SIZE} - ${LASTSNAP_SIZE} ))","title":"AWS Backup"},{"location":"Home-Lab/Legacy/Backups/AWS-Backup/#notes-on-backups-of-maxwells-zfs-pool-to-aws-deprecated","text":"NOTE: I no longer run ZFS at home except for ad-hoc experiments. This is because I found ZFS to consistently be overkill for my personal needs. In general, I've been re-assessing how I go about home-labbing so that long-running infrastructure / devices are much less complex. I realized that the overhead / complexity of running ZFS didn't fit any of the use cases I have for storage in my personal life and actually was problematic (in the sense that if my loved ones needed to access my data in an emergency, no one would know what to do with a multi-disk ZFS dataset). Furthermore, I found the egress charges from AWS S3 to be not worth it ( ~ $100 worth of egress charges vs a comparable amount to just buy an external hard disk and leave it at my mom's house upstate). I'm leaving this page up for now just to document that I had this set up at one point, but it no longer reflects my contemporary configuration. An anacron script runs on Maxwell once a month (i.e., it lives in /etc/cron.monthly ). It runs a series of commands based off the StackOverflow post here to generate encrypted, compressed snapshots that live in an AWS bucket in US-East 2 (Ohio). The script only does incremental backups, and keeps track off the last incremental snapshot that was made using a text file. It also generates a script that allows me to make a manual backup to an external hard disk within my apartment. This is in case I accidentally tank my ZFS pool and need to restore from scratch, but don't want to have to wait 3-5 hours for Glacier to retrieve files. The commands used (taken from the StackOverflow post above) are as follows: Sending a full backup: zfs send -R <pool name>@<snapshot name> | gzip | gpg --no-use-agent --no-tty --passphrase-file ./passphrase -c - | aws s3 cp - s3://<bucketname>/<filename>.zfs.gz.gpg Sending an incremental backup: zfs send -R -I <pool name>@<snapshot to do incremental backup from> <pool name>@<snapshot name> | gzip | gpg --no-use-agent --no-tty --passphrase-file ./passphrase -c - | aws s3 cp - s3://<bucketname>/<filename>.zfs.gz.gpg Restoring a backup: aws s3 cp s3://<bucketname>/<filename>.zfs.gz.gpg - | gpg --no-use-agent --passphrase-file ./passphrase -d - | gunzip | sudo zfs receive <new dataset name> The specific variant of the restore command that I use looks something like this: aws s3 cp s3://jpellman-maxwell-backup-us-east-2/full-test.gz.gpg - | gpg --passphrase-file /root/.backup_pass --no-tty --batch -d - | gunzip | zfs receive pool0/test2 The initial encrypted, compressed snapshot (full backup) was created manually. The aws cli copy command includes the following flag to account for very large snapshots: --expected-size (string) This argument specifies the expected size of a stream in terms of bytes. Note that this argument is needed only when a stream is being uploaded to s3 and the size is larger than 50GB. Failure to include this argument under these conditions may result in a failed upload due to too many parts in upload. The string that's fed into this param is ESTIMATED_UPLOAD from the following: LASTSNAP_SIZE=$(zfs list -t snapshot -o name,refer -Hp | grep ${POOL} | grep ${LASTSNAP} | awk '{print $2}' | paste -sd+ - | bc) CURSNAP_SIZE=$(zfs list -t snapshot -o name,refer -Hp | grep ${POOL} | grep ${CURSNAP} | awk '{print $2}' | paste -sd+ - | bc) ESTIMATED_UPLOAD=$(( ${CURSNAP_SIZE} - ${LASTSNAP_SIZE} ))","title":"Notes on Backups of Maxwell's ZFS Pool to AWS (DEPRECATED)"},{"location":"Home-Lab/Network/ap01/","text":"ap01 Description ap01 (shorthand for \"Access Point 1\") is an ASUS N300 RT-N12_D1 with stock firmware installed. It's currently set up to function as an access point only (this functionality is built into the stock firmware). Routing functionality is provided by gateway01 . Specifications Key Value CPU Broadcom BCM5357 (ARM?) # of Cores (logical) 1 CPU Clock Speed (MHz) 300 Memory (MB) 32 Disks 8 MB Flash OS Stock Firmware (Linux version 2.6.22.19) IP Address 192.168.1.2 Supported WiFi Bands 2.4 Ghz Configuration Backup I have not yet backed up the configuration for this. After I create a dedicated backup machine I'll stash a backup there. Plans I need to get a wall mount and put this on top of it to see if that provides better coverage. I've been looking at this for both this and ap02 . Maybe I'll install dd-wrt on it? This is possible according to here . Not sure if it's particularly worthwhile though.","title":"Ap01"},{"location":"Home-Lab/Network/ap01/#ap01","text":"","title":"ap01"},{"location":"Home-Lab/Network/ap01/#description","text":"ap01 (shorthand for \"Access Point 1\") is an ASUS N300 RT-N12_D1 with stock firmware installed. It's currently set up to function as an access point only (this functionality is built into the stock firmware). Routing functionality is provided by gateway01 .","title":"Description"},{"location":"Home-Lab/Network/ap01/#specifications","text":"Key Value CPU Broadcom BCM5357 (ARM?) # of Cores (logical) 1 CPU Clock Speed (MHz) 300 Memory (MB) 32 Disks 8 MB Flash OS Stock Firmware (Linux version 2.6.22.19) IP Address 192.168.1.2 Supported WiFi Bands 2.4 Ghz","title":"Specifications"},{"location":"Home-Lab/Network/ap01/#configuration-backup","text":"I have not yet backed up the configuration for this. After I create a dedicated backup machine I'll stash a backup there.","title":"Configuration Backup"},{"location":"Home-Lab/Network/ap01/#plans","text":"I need to get a wall mount and put this on top of it to see if that provides better coverage. I've been looking at this for both this and ap02 . Maybe I'll install dd-wrt on it? This is possible according to here . Not sure if it's particularly worthwhile though.","title":"Plans"},{"location":"Home-Lab/Network/ap02/","text":"ap02 Description ap02 (shorthand for \"Access Point 2\") is a WNR3500Lv2 with dd-wrt on it. It's currently set up to function as a wireless repeater because my main WAP ( ap01 ) does not always provide reliable coverage in my bedroom. Instructions for this particular setup can be found here . Specifications Key Value CPU MIPS32 74k # of Cores (logical) 1 CPU Clock Speed (MHz) 480 Memory (MB) 128 Disks 128 MB NAND Flash OS dd-wrt Kong Build (2015) IP Address 192.168.1.3 Supported WiFi Bands 2.4 Ghz Configuration Backup I have not yet backed up the configuration for this. After I create a dedicated backup machine I'll stash a backup there. Plans I need to get a wall mount and put this on top of it to see if that provides better coverage in my bedroom. I've been looking at this for both this and ap01 . This build of dd-wrt is ancient and Kong is no longer making builds so I'm never going to hook this up directly to a modem again. It will live on only behind a firewall appliance (i.e., gateway01 ).","title":"Ap02"},{"location":"Home-Lab/Network/ap02/#ap02","text":"","title":"ap02"},{"location":"Home-Lab/Network/ap02/#description","text":"ap02 (shorthand for \"Access Point 2\") is a WNR3500Lv2 with dd-wrt on it. It's currently set up to function as a wireless repeater because my main WAP ( ap01 ) does not always provide reliable coverage in my bedroom. Instructions for this particular setup can be found here .","title":"Description"},{"location":"Home-Lab/Network/ap02/#specifications","text":"Key Value CPU MIPS32 74k # of Cores (logical) 1 CPU Clock Speed (MHz) 480 Memory (MB) 128 Disks 128 MB NAND Flash OS dd-wrt Kong Build (2015) IP Address 192.168.1.3 Supported WiFi Bands 2.4 Ghz","title":"Specifications"},{"location":"Home-Lab/Network/ap02/#configuration-backup","text":"I have not yet backed up the configuration for this. After I create a dedicated backup machine I'll stash a backup there.","title":"Configuration Backup"},{"location":"Home-Lab/Network/ap02/#plans","text":"I need to get a wall mount and put this on top of it to see if that provides better coverage in my bedroom. I've been looking at this for both this and ap01 . This build of dd-wrt is ancient and Kong is no longer making builds so I'm never going to hook this up directly to a modem again. It will live on only behind a firewall appliance (i.e., gateway01 ).","title":"Plans"},{"location":"Home-Lab/Network/gateway01/","text":"gateway01 Description gateway01 is a firewall appliance / router running pfSense on top of a Protectli Vault FW2 . A hardware overview can be found here on the Protectli website. Part of my motivation for putting this appliance together is that I noticed that ap01 was running on an ancient version of the Linux kernel that hadn't been updated since 2008, and ap02's firmware couldn't be updated past Kong's 2015 build of dd-wrt. I wanted the main gateway to my LAN to be a piece of hardware that was a) readily updatable, b) versatile (in terms of being able to switch out firmware) and c) secure. The Protectli satisfied these criteria by using the most mainstream CPU architecture in the consumer market (x86_64; although ARM was also appealing) and basically being a full-fledged PC (that is nonetheless not as powerful as most modern desktops). Right now it runs pfSense, but if I ever want to switch to another OS (i.e., if pfSense gets abandoned and doesn't receive security updates or similar) I should be able to do so quite readily (worst case scenario I could just use plain Linux with iptables/firewalld; see here ). Specifications Key Value CPU Celeron J1800 # of Cores (logical) 2 CPU Clock Speed (GHz) 2.41 Memory (GB) 4 (DDR3 SODIMM) Disks 32 GB mSATA OS pfSense IP Address 192.168.1.1 Configuration Backup I have not yet backed up the configuration for this. After I create a dedicated backup machine I'll stash a backup there.","title":"Gateway01"},{"location":"Home-Lab/Network/gateway01/#gateway01","text":"","title":"gateway01"},{"location":"Home-Lab/Network/gateway01/#description","text":"gateway01 is a firewall appliance / router running pfSense on top of a Protectli Vault FW2 . A hardware overview can be found here on the Protectli website. Part of my motivation for putting this appliance together is that I noticed that ap01 was running on an ancient version of the Linux kernel that hadn't been updated since 2008, and ap02's firmware couldn't be updated past Kong's 2015 build of dd-wrt. I wanted the main gateway to my LAN to be a piece of hardware that was a) readily updatable, b) versatile (in terms of being able to switch out firmware) and c) secure. The Protectli satisfied these criteria by using the most mainstream CPU architecture in the consumer market (x86_64; although ARM was also appealing) and basically being a full-fledged PC (that is nonetheless not as powerful as most modern desktops). Right now it runs pfSense, but if I ever want to switch to another OS (i.e., if pfSense gets abandoned and doesn't receive security updates or similar) I should be able to do so quite readily (worst case scenario I could just use plain Linux with iptables/firewalld; see here ).","title":"Description"},{"location":"Home-Lab/Network/gateway01/#specifications","text":"Key Value CPU Celeron J1800 # of Cores (logical) 2 CPU Clock Speed (GHz) 2.41 Memory (GB) 4 (DDR3 SODIMM) Disks 32 GB mSATA OS pfSense IP Address 192.168.1.1","title":"Specifications"},{"location":"Home-Lab/Network/gateway01/#configuration-backup","text":"I have not yet backed up the configuration for this. After I create a dedicated backup machine I'll stash a backup there.","title":"Configuration Backup"},{"location":"Home-Lab/Network/Legacy/Arris-Surfboard/","text":"Arris Surfboard Description This is a decidedly unsexy DOCSIS 3.0 Arris Surfboard SB6141 . I no longer use it. It sits in my closet waiting for the day that it becomes relevant again. This day is unlikely to ever come.","title":"Arris Surfboard"},{"location":"Home-Lab/Network/Legacy/Arris-Surfboard/#arris-surfboard","text":"","title":"Arris Surfboard"},{"location":"Home-Lab/Network/Legacy/Arris-Surfboard/#description","text":"This is a decidedly unsexy DOCSIS 3.0 Arris Surfboard SB6141 . I no longer use it. It sits in my closet waiting for the day that it becomes relevant again. This day is unlikely to ever come.","title":"Description"},{"location":"Musings/Meditations-On-Theranos/","text":"The COVID-19 situation, as well as my own interest in neuroimaging / healthcare imaging technologies, has caused me to think about Theranos a lot lately. The connection between the present context of my life and Theranos is admittedly fuzzy, but I think that I associate the two primarily due to Theranos' status as a health tech company. I find myself to be largely supportive of Theranos' stated goal of \"democratizing medical testing\" with their Edison machines. The single largest contributor to the current COVID-19 crisis in the United States has proven to be a lack of diagnostic tests, which I think is ultimately attributable to: Political incompetence on the part of the Trump administration. The increasing bureaucratization of hospitals (best exemplified by technologies such as Epic , which are less tailored to the needs of doctors and patients and more tailored to billing departments). This bureaucratization, and the rise in a management class in hospitals has led to effects similar to those in academia (as described in Benjamin Ginsberg's The Fall of the Faculty ; a book I have not read, but which I imagine should be taken with a grain of salt). Namely, there is a large body of people that actively impede progress in responding rapidly to patient and public needs. Many of these people likely have the bullshit jobs bemoaned by David Graeber. The tendency for Americans to not seek out medical care early enough due to our Frankenstein of a healthcare system. Ultimately, I don't have any confidence in the ability of large institutions of any stripe (private or public) to deal with the most challenging problems of our time. In the case of COVID-19, I believe that this crisis could have been averted if testing kits were widely available and readily accessible in pharmacies. If any given U.S. citizen was able to just walk into a pharmacy and pick up a testing kit, without the overhead of having to visit a doctor or hospital (and incur some undoubtedly excessive co-pay), this crisis would not be where it is today. Essentially, if medical diagnosis were democratized, barriers were lowered, and testing kit production was adequate (which is admittedly a big if) I don't think I'd be holed up in my apartment currently. I am not bullish on America's ability to fix its healthcare system, reduce excessive bureaucracy in hospitals or elect knowledgable leaders at any level of government. Currently, I think that the best we can do is continue pushing for change in these arenas (and, perhaps more importantly, in education), while also acknowledging that change is unlikely to happen anytime soon. A better investment of time would seem to be to work on improving medical instrumentation, and ultimately make diagnostic tools like MRI scanners and blood tests as safe, reliable and available as over-the-counter drugs, blood pressure tests and thermometers. Essentially, I'm more optimistic about our ability to shift diagnostic services traditionally provided by hospitals down to pharmacies / a self-service model, although I think that this will still require an extremely substantial amount of work. In the end, I think that this would make your average pharmacy OTC/supplements area much much more useful than it currently is (I've always found it amusing that pharmacies have rows upon rows with myriad variants of the same 3 drugs: ibuprofen, acetaminophen, and aspirin). From my perspective, Theranos' concept was desirable, but its implementation was impossible. The tragedy of Theranos stems from the fact that the science wasn't there- Elizabeth Holmes was warned by several field experts at Stanford before she dropped out to found Theranos. A drop of blood does not have adequate statistical power to detect anomalies compared to the 3-4 vials typically needed. In Spinozist terms , Theranos was prone to imagination over reason, and derived knowledge from random experience rather than from adequate knowledge. In particular, Theranos was inspired by the zeitgeist of the early 21st century Silicon Valley world, without understanding the basis for the Bay Area's techno-optimism. An endemic part of the Bay Area's tech industry culture seems to be an overreaching application of technological concepts to societal issues to an almost tragic extent. I won't enumerate the full multitude of sins in this meditation, but I will focus on the sin that I believe Theranos was most guilty of: implicitly applying Moore's law to situations where its assumptions did not hold. Moore's law (really an observation rather than a law) can only narrowly be applied to transistor density on integrated circuits. It predicts an exponential growth in the number of transistors that can be fit on a circuit as years elapse. Basically, all of Silicon Valley's computer hardware and software-centric tech companies are heavily dependent upon the increased performance that can be reaped as a result of Moore's law, which has enabled the average computer to perform tasks that would have been impractical in the past. Yesterday's supercomputer has been miniaturized to today's desktop PC, and there are doubtless microwaves and disk controllers with more computing power than top of the line machines in bygone eras. This miniaturization, however, does not apply equally well to medical or scientific instrumentation, however. As obvious as this should be, Moore's law does not apply to all of nature, and was only scoped to a very specific scenario. This fact seems to have been lost on Theranos, who thought that they could condense a full-fledged diagnostic lab into a mini-ITX case (they at least seem to be in good company ). In summary, I regard Theranos' chief technical and psychological flaw to have been a fixation on making physical objects smaller, derived from misapplying a cognitive schema that belonged to another context (i.e., something like functional fixedness ). If the tech industry is truly hoping to make a difference in democratizing healthcare, I believe that it needs to focus on applying what it knows best- getting peak performance out of modern hardware. Instead of Theranos, I think that there needs to be software out there that can perform fNIRS/EEG/MRI preprocessing or FWI quickly and accurately enough that it can be of use to everyday folk. This software does not exist yet. I think that there needs to be widely run software and hardware architectures that can simulate protein folding, similar to D.E. Shaw Research's Anton , foldingathome , and Rosetta@home . These are the areas where the tech industry can make an impact. These are areas where programmers are in their element. This is the context where a programmer cannot violate the laws of physics or overestimate our ability to make observations of the physical world.","title":"Meditations On Theranos"},{"location":"Musings/Musings/","text":"Various Musings \"Release early, release often.\" - Principles of ' DevOps ' / Continuous Integration in accord with the Bermuda Principles . Could such practices be applied to scientific data? Is not programming code data in a sense? DataOps - ' DevOps for Data Science' a.k.a. data warehousing Jupyter notebooks do for science what configuration management tools like Puppet/Chef do for systems administration. Thought: A machine-learning based scheduler for scientific workflow management. Train on a few runs of a pipeline to determine memory, CPU needs (maybe use priors supplied by researcher or some heuristic). Allow researchers to mark whether or not a pipeline has failed and have the classifier learn from this. If the pipeline is marked as failed, allow the researcher to manually inspect the outputs of each step to determine which node is the culprit. Use this data to predict future failures. If a future failure seems plausible, notify the researcher, pause the pipeline, or some combination of these two. If a failure happens often enough, request manual intervention/maintenance. There should be a facilitation method of connecting philanthropists/foundations to worthy projects based on the insights in Jon Kleinberg et al's social networks book. Load balancers such as nginx/HAProxy are analogous to grid schedulers. The chief difference lies in the constituency that is requesting resources- scientists on an internal network vs the general public.","title":"Musings"},{"location":"Musings/Musings/#various-musings","text":"\"Release early, release often.\" - Principles of ' DevOps ' / Continuous Integration in accord with the Bermuda Principles . Could such practices be applied to scientific data? Is not programming code data in a sense? DataOps - ' DevOps for Data Science' a.k.a. data warehousing Jupyter notebooks do for science what configuration management tools like Puppet/Chef do for systems administration. Thought: A machine-learning based scheduler for scientific workflow management. Train on a few runs of a pipeline to determine memory, CPU needs (maybe use priors supplied by researcher or some heuristic). Allow researchers to mark whether or not a pipeline has failed and have the classifier learn from this. If the pipeline is marked as failed, allow the researcher to manually inspect the outputs of each step to determine which node is the culprit. Use this data to predict future failures. If a future failure seems plausible, notify the researcher, pause the pipeline, or some combination of these two. If a failure happens often enough, request manual intervention/maintenance. There should be a facilitation method of connecting philanthropists/foundations to worthy projects based on the insights in Jon Kleinberg et al's social networks book. Load balancers such as nginx/HAProxy are analogous to grid schedulers. The chief difference lies in the constituency that is requesting resources- scientists on an internal network vs the general public.","title":"Various Musings"},{"location":"Science/CWL-Make/","text":"Thinking about Scientific Workflows w/ Make Analogy make System Workflow System Makefile CWL / WDL file describing workflow make Arvados, Taverna, etc make install Command to execute workflow w/ workflow manager configure Tools like C-PAC that offer customizable pipelines.","title":"CWL Make"},{"location":"Science/CWL-Make/#thinking-about-scientific-workflows-w-make-analogy","text":"make System Workflow System Makefile CWL / WDL file describing workflow make Arvados, Taverna, etc make install Command to execute workflow w/ workflow manager configure Tools like C-PAC that offer customizable pipelines.","title":"Thinking about Scientific Workflows w/ Make Analogy"},{"location":"Science/Citation-Graphs/","text":"See Metascience .","title":"Citation Graphs"},{"location":"Science/Metascience/","text":"Metascience Tools Semantic Scholar Meta Neuroscience Neuroquery Neurosynth Conferences / Presentations Metascience 2019 Citation Graphing An ongoing list of software / techniques for visualizing citations. https://physics.stackexchange.com/questions/5569/is-there-a-nice-tool-to-plot-graphs-of-paper-citations Citespace Mark Longair's Thesis Visualization Script Eigenfactor Projects Open Academic Graph Neurotree and the broader Academic Tree Citation Gecko Clubs of Science (dead) - see here and here Citation Parsing/Munging Anystyle BibTeX Ruby Citation.js biblib BibtexParser Pybtex Pybliographer BabyBib How I scraped data from Google Scholar","title":"Metascience"},{"location":"Science/Metascience/#metascience","text":"","title":"Metascience"},{"location":"Science/Metascience/#tools","text":"Semantic Scholar Meta","title":"Tools"},{"location":"Science/Metascience/#neuroscience","text":"Neuroquery Neurosynth","title":"Neuroscience"},{"location":"Science/Metascience/#conferences-presentations","text":"Metascience 2019","title":"Conferences / Presentations"},{"location":"Science/Metascience/#citation-graphing","text":"An ongoing list of software / techniques for visualizing citations. https://physics.stackexchange.com/questions/5569/is-there-a-nice-tool-to-plot-graphs-of-paper-citations Citespace Mark Longair's Thesis Visualization Script Eigenfactor Projects Open Academic Graph Neurotree and the broader Academic Tree Citation Gecko Clubs of Science (dead) - see here and here","title":"Citation Graphing"},{"location":"Science/Metascience/#citation-parsingmunging","text":"Anystyle BibTeX Ruby Citation.js biblib BibtexParser Pybtex Pybliographer BabyBib How I scraped data from Google Scholar","title":"Citation Parsing/Munging"},{"location":"Science/Open-Science-Critiques/","text":"Critiques of Open Science Metadata \u201cAt the heart of science is an essential balance between two seemingly contradictory attitudes \u2013 an openness to new ideas, no matter how bizarre or counterintuitive, and the most ruthlessly skeptical scrutiny of all ideas, old and new. This is how deep truths are winnowed from deep nonsense.\u201d \u2013Carl Sagan Open science is poised to become one of the most innovative paradigm shifts in science, facilitated in great part by the capacity to share directly via the internet, increases in the ability to store large datasets, and a recognition from scientists that reproducibility and the future re-interpretation of data in light of methodological advances is necessary for the scientific enterprise. Despite this, no human institution is entirely perfect, and I would like to examine some of the caveats of open science in a series of blog posts here. I approach open science with skepticism since, as pointed out by the above Carl Sagan quotation, science necessarily progresses through critical thinking. Nevertheless, do not take this skepticism as cynicism, and my critiques as indictments- my intention is to point out major shortcomings in open science so that they may be addressed by the open science community. Just as iron must be forged by a hammer, so must creativity be refined through blunt inquiry. The first topic I would like to approach is that of metadata. As researchers gather data, there is a huge bevy of supplemental data that is not always recorded. When you are in a room with the object of your study, you are exposed to more input, more sensations than recorded data. Data is an abstraction, and real life experience while collecting data can provide much information that aids in its interpretation. In short, the researchers collecting data will always be in a privileged position when it comes to being able to understand what signal they captured, since they captured other signal that they did not package. This can pose a problem for those who try to interpret that data later. Let us look to the New York Times (as well as Simply Statistics , where I first encountered this article ) to make this more tangible. In this article, NYU students set up an experiment with sensors next to stairs to track when students used the aforementioned stairs the most. They gathered data, but they were not performing the experiment in a hands-on way. In the absence of crucial metadata from the security guards at NYU (i.e., that the elevator was broken), they came to erroneous conclusions. Data released to websites such as the Open Science Framework or Dataverse is no different; without any context, it is possible that a great many individuals will reuse the data in a way that makes no sense because they are missing crucial information. Another example can be found in neuroimaging. In MRI studies, subjects oftentimes will fall asleep while in the scanner. This is a serious confound that can lead to spurious results in data: an analyst must ask whether or not activity is task-related or due to the subject nodding off because they found the video they were viewing particularly boring and were tired. The research tech performing the scan might be able to inform the analyst\u2019s conclusions by letting them know that, yes indeed, that subject was not conscious. One last observation: the philosopher of science John A. Schuster in Chapter 10 of one of his introductory textbooks to philosophy and history of science notes that there is a gap between theory and data, and that there must be a subjective point at which a researcher decides that a gap is too big or too small and accepts or rejects a theory accordingly. He then notes an interesting point about how theory informs the apparatus of data collection itself*. In this way, there is an element of circularity in data collection. If the assumptions behind data collection are incorrect, then the data itself might be totally useless. Imagine the biologist who spends years collecting data with a specialized microscope, only to realize that the conceptual underpinnings of the microscope itself were false and her data is now uninterpretable. Alternatively, consider the scientist who shaped a data gathering program with his fundamentally flawed theory in mind from the beginning (in a way, this resembles the logical fallacy of begging the question). Simply opening up data to the outside world does not address these issues. These issues might not be capable of ever being fully addressed. Nevertheless, I would suggest some partial solutions at least. For one, researchers could record metadata in a manner similar to commenting code in software development and then upload it in addition to their datasets. With respect to the other observation regarding the theory-loading of data gathering / experimental apparatuses, researchers could lay out their assumptions for data gathering or provide references detailing the assumptions of the apparatus that they are using. That way, if other scientists disagree with those assumptions they will know to use different data or gather their own and release it accordingly. Knowing a great many researchers and how strained they are for time and the need to publish, I see these as ideals that likely will never be fulfilled unless we are in a Star Trek-like utopian future. But it doesn\u2019t hurt to dream. Problem 3 of Roger Peng\u2019s Simply Stats post above is conceptually similar to this theory-loading. Response by Bill Broderick I see your point about how metadata is crucial for the analysis of scientific data, but I don\u2019t see how it\u2019s more of a problem for Open Science than for regular/closed Science. Good practice would have people recording all relevant data with their data regardless, since it is necessary for any analysis that makes sense and if the lab wants to return to the data later, to use it for another project or to try and replicate their results, they\u2019ll still need that metadata. So metadata is a necessity, regardless of whether you\u2019re planning to share your data or not. I think this is actually an upside for Open Science. Problems like metadata, proper annotation and care of data, etc. are no more a problem for Open Science than regular Science, but making your data or methods open adds another layer of incentive. These, and version-control and commenting of code, are practices that people should be doing anyway; making their stuff open will hopefully encourage scientists to use them, since they know their colleagues, collaborators, and editors will have access to everything. This is very similar to the argument for why Open Science will help reduce fraud: if your data / methods are out in the open, then dishonesty will be discouraged, because it\u2019s easier to detect. Open Science may also help with the metadata issue because it will encourage scientists to develop consistent metadata, which will improve the quality of meta-analyses and probably regular analyses as well. Poldrack has Cognitive Atlas, which is an effort towards encouraging just that. Response to Bill Broderick The main point that I was trying to make was that openness is necessary, but not sufficient for scientific practice to truly improve. In my experience, the publish or perish culture has made investigators much more apathetic and sloppy about good metadata keeping and good coding practices. The prevailing attitude among postdocs in my current lab is one of, \u201cWhy write good code when the publication record is all that ultimately matters?\u201d Ideally, this would not be the case and I do believe that open science could improve this, but only in the presence of sufficient funding and cultural incentives. Relatedly, a recent column in the Baltimore Sun makes similar points in much greater detail: http://www.baltimoresun.com/news/opinion/oped/bs-ed-science-crisis-20150725-story.html","title":"Open Science Critiques"},{"location":"Science/Open-Science-Critiques/#critiques-of-open-science","text":"","title":"Critiques of Open Science"},{"location":"Science/Open-Science-Critiques/#metadata","text":"\u201cAt the heart of science is an essential balance between two seemingly contradictory attitudes \u2013 an openness to new ideas, no matter how bizarre or counterintuitive, and the most ruthlessly skeptical scrutiny of all ideas, old and new. This is how deep truths are winnowed from deep nonsense.\u201d \u2013Carl Sagan Open science is poised to become one of the most innovative paradigm shifts in science, facilitated in great part by the capacity to share directly via the internet, increases in the ability to store large datasets, and a recognition from scientists that reproducibility and the future re-interpretation of data in light of methodological advances is necessary for the scientific enterprise. Despite this, no human institution is entirely perfect, and I would like to examine some of the caveats of open science in a series of blog posts here. I approach open science with skepticism since, as pointed out by the above Carl Sagan quotation, science necessarily progresses through critical thinking. Nevertheless, do not take this skepticism as cynicism, and my critiques as indictments- my intention is to point out major shortcomings in open science so that they may be addressed by the open science community. Just as iron must be forged by a hammer, so must creativity be refined through blunt inquiry. The first topic I would like to approach is that of metadata. As researchers gather data, there is a huge bevy of supplemental data that is not always recorded. When you are in a room with the object of your study, you are exposed to more input, more sensations than recorded data. Data is an abstraction, and real life experience while collecting data can provide much information that aids in its interpretation. In short, the researchers collecting data will always be in a privileged position when it comes to being able to understand what signal they captured, since they captured other signal that they did not package. This can pose a problem for those who try to interpret that data later. Let us look to the New York Times (as well as Simply Statistics , where I first encountered this article ) to make this more tangible. In this article, NYU students set up an experiment with sensors next to stairs to track when students used the aforementioned stairs the most. They gathered data, but they were not performing the experiment in a hands-on way. In the absence of crucial metadata from the security guards at NYU (i.e., that the elevator was broken), they came to erroneous conclusions. Data released to websites such as the Open Science Framework or Dataverse is no different; without any context, it is possible that a great many individuals will reuse the data in a way that makes no sense because they are missing crucial information. Another example can be found in neuroimaging. In MRI studies, subjects oftentimes will fall asleep while in the scanner. This is a serious confound that can lead to spurious results in data: an analyst must ask whether or not activity is task-related or due to the subject nodding off because they found the video they were viewing particularly boring and were tired. The research tech performing the scan might be able to inform the analyst\u2019s conclusions by letting them know that, yes indeed, that subject was not conscious. One last observation: the philosopher of science John A. Schuster in Chapter 10 of one of his introductory textbooks to philosophy and history of science notes that there is a gap between theory and data, and that there must be a subjective point at which a researcher decides that a gap is too big or too small and accepts or rejects a theory accordingly. He then notes an interesting point about how theory informs the apparatus of data collection itself*. In this way, there is an element of circularity in data collection. If the assumptions behind data collection are incorrect, then the data itself might be totally useless. Imagine the biologist who spends years collecting data with a specialized microscope, only to realize that the conceptual underpinnings of the microscope itself were false and her data is now uninterpretable. Alternatively, consider the scientist who shaped a data gathering program with his fundamentally flawed theory in mind from the beginning (in a way, this resembles the logical fallacy of begging the question). Simply opening up data to the outside world does not address these issues. These issues might not be capable of ever being fully addressed. Nevertheless, I would suggest some partial solutions at least. For one, researchers could record metadata in a manner similar to commenting code in software development and then upload it in addition to their datasets. With respect to the other observation regarding the theory-loading of data gathering / experimental apparatuses, researchers could lay out their assumptions for data gathering or provide references detailing the assumptions of the apparatus that they are using. That way, if other scientists disagree with those assumptions they will know to use different data or gather their own and release it accordingly. Knowing a great many researchers and how strained they are for time and the need to publish, I see these as ideals that likely will never be fulfilled unless we are in a Star Trek-like utopian future. But it doesn\u2019t hurt to dream. Problem 3 of Roger Peng\u2019s Simply Stats post above is conceptually similar to this theory-loading.","title":"Metadata"},{"location":"Science/Open-Science-Critiques/#response-by-bill-broderick","text":"I see your point about how metadata is crucial for the analysis of scientific data, but I don\u2019t see how it\u2019s more of a problem for Open Science than for regular/closed Science. Good practice would have people recording all relevant data with their data regardless, since it is necessary for any analysis that makes sense and if the lab wants to return to the data later, to use it for another project or to try and replicate their results, they\u2019ll still need that metadata. So metadata is a necessity, regardless of whether you\u2019re planning to share your data or not. I think this is actually an upside for Open Science. Problems like metadata, proper annotation and care of data, etc. are no more a problem for Open Science than regular Science, but making your data or methods open adds another layer of incentive. These, and version-control and commenting of code, are practices that people should be doing anyway; making their stuff open will hopefully encourage scientists to use them, since they know their colleagues, collaborators, and editors will have access to everything. This is very similar to the argument for why Open Science will help reduce fraud: if your data / methods are out in the open, then dishonesty will be discouraged, because it\u2019s easier to detect. Open Science may also help with the metadata issue because it will encourage scientists to develop consistent metadata, which will improve the quality of meta-analyses and probably regular analyses as well. Poldrack has Cognitive Atlas, which is an effort towards encouraging just that.","title":"Response by Bill Broderick"},{"location":"Science/Open-Science-Critiques/#response-to-bill-broderick","text":"The main point that I was trying to make was that openness is necessary, but not sufficient for scientific practice to truly improve. In my experience, the publish or perish culture has made investigators much more apathetic and sloppy about good metadata keeping and good coding practices. The prevailing attitude among postdocs in my current lab is one of, \u201cWhy write good code when the publication record is all that ultimately matters?\u201d Ideally, this would not be the case and I do believe that open science could improve this, but only in the presence of sufficient funding and cultural incentives. Relatedly, a recent column in the Baltimore Sun makes similar points in much greater detail: http://www.baltimoresun.com/news/opinion/oped/bs-ed-science-crisis-20150725-story.html","title":"Response to Bill Broderick"},{"location":"Science/Open-Science/","text":"Pages related to open science. Issues in Open Research Data Open Research Funders Group MIT Summarizes Open Access Requirements for various funding agencies/foundations Bermuda Principles (Wikipedia Article) Bermuda Principles (HGP Page) OpenHatch 'Open Science Projects and Organizations' Page (dead) - I mirrored the source though. Add to Wikipedia? Metascience Symposium 2019","title":"Open Science"},{"location":"Science/Project-Management/","text":"I'm putting various links to project management resources here to synthesize how they may be tied to science. Management for Scientists by Addgene Project Management by Nick Jenkinson Project Management for Scientists Social Loafing Mythical Man Month Amdahl's law Gustafson's law W Edwards Deming Herbert Simon Examples of Project Management failures in science/basic research: Charles Babbage's Difference Engine Hooke Other Pitfalls: Taylorism - Don't want to squash creativity. Micromanagement example: guy who almost derailed LIGO Advice: How to start a lab by Tim Behrens","title":"Project Management"},{"location":"Science/Roles-In-Science/","text":"Roles in Science This is a rough list of the many roles I find people performing in scientific labs, many of which are informal due to the rigid, publish-or-perish mentality of academia. In an ideal world, I would wish for these roles to be formally recognized so that individuals who want to pursue these roles may be seen as professionals in their own right (rather than an afterthought in the scientific process). Perhaps, in an ideal world, research would be more of a \"team sport\", with researchers wearing fewer hats and spreading themselves less thin. Analysts and Archivists Statisticians Data Mungers Data Managers Fundraisers Grant Writers Literature Reviewers Educators TAs, teaching-oriented professors Science popularizers (like Green brothers, Neil deGrasse Tyson, Carl Sagan, etc) Research Technologists Research Software Engineers Research Systems Engineers Research Technical Support Quality Control Peer Reviewers Second statistician Code reviewer QA person (if DevOps principles not as much in play) Managers/Synthesizers/Creatives(?) People who are able to take various aspects of theories and the current literature, combine this knowledge in novel ways, and determine a good direction to go in.","title":"Roles In Science"},{"location":"Science/Roles-In-Science/#roles-in-science","text":"This is a rough list of the many roles I find people performing in scientific labs, many of which are informal due to the rigid, publish-or-perish mentality of academia. In an ideal world, I would wish for these roles to be formally recognized so that individuals who want to pursue these roles may be seen as professionals in their own right (rather than an afterthought in the scientific process). Perhaps, in an ideal world, research would be more of a \"team sport\", with researchers wearing fewer hats and spreading themselves less thin.","title":"Roles in Science"},{"location":"Science/Roles-In-Science/#analysts-and-archivists","text":"Statisticians Data Mungers Data Managers","title":"Analysts and Archivists"},{"location":"Science/Roles-In-Science/#fundraisers","text":"Grant Writers Literature Reviewers","title":"Fundraisers"},{"location":"Science/Roles-In-Science/#educators","text":"TAs, teaching-oriented professors Science popularizers (like Green brothers, Neil deGrasse Tyson, Carl Sagan, etc)","title":"Educators"},{"location":"Science/Roles-In-Science/#research-technologists","text":"Research Software Engineers Research Systems Engineers Research Technical Support","title":"Research Technologists"},{"location":"Science/Roles-In-Science/#quality-control","text":"Peer Reviewers Second statistician Code reviewer QA person (if DevOps principles not as much in play)","title":"Quality Control"},{"location":"Science/Roles-In-Science/#managerssynthesizerscreatives","text":"People who are able to take various aspects of theories and the current literature, combine this knowledge in novel ways, and determine a good direction to go in.","title":"Managers/Synthesizers/Creatives(?)"},{"location":"Science/Scientific-Computing-Articles/","text":"Articles Discussing Scientific Computing Computational science: ...Error - ...why scientific programming does not compute Discussion on Hacker News The Low Quality of Scientific Code Response : Why bad scientific code beats code following \"best practices\" HPC is dying, and MPI is killing it Genomics is not Special Indexing polar data with Tika","title":"Scientific Computing Articles"},{"location":"Science/Scientific-Computing-Articles/#articles-discussing-scientific-computing","text":"Computational science: ...Error - ...why scientific programming does not compute Discussion on Hacker News The Low Quality of Scientific Code Response : Why bad scientific code beats code following \"best practices\" HPC is dying, and MPI is killing it Genomics is not Special Indexing polar data with Tika","title":"Articles Discussing Scientific Computing"},{"location":"Science/Scientific-File-Formats/","text":"Scientific File Formats I'm using this page to organize discoveries / thoughts about scientific file formats / compression algorithms. HDF5 http://cyrille.rossant.net/moving-away-hdf5/ NIfTI https://www.nitrc.org/docman/view.php/26/204/TheNIfTI1Format2004.pdf https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Formats https://brainder.org/2012/09/23/the-nifti-file-format/ https://brainder.org/2015/04/03/the-nifti-2-file-format/ https://nifti.nimh.nih.gov/nifti-1/documentation/hbm_nifti_2004.pdf https://nifti.nimh.nih.gov/nifti-2 MRC Chiefly used by Relion. http://www.ccpem.ac.uk/mrc_format/mrc2014.php Compression FPZip : \"Think of fpzip as the floating-point analogue to PNG image compression and zfp as advanced JPEG for floating-point arrays.\" mtscomp : If fpzip/zfp are PNG/JPEG analogues, mtscomp seems to be more of an MP3 / audio compression analogue.","title":"Scientific File Formats"},{"location":"Science/Scientific-File-Formats/#scientific-file-formats","text":"I'm using this page to organize discoveries / thoughts about scientific file formats / compression algorithms.","title":"Scientific File Formats"},{"location":"Science/Scientific-File-Formats/#hdf5","text":"http://cyrille.rossant.net/moving-away-hdf5/","title":"HDF5"},{"location":"Science/Scientific-File-Formats/#nifti","text":"https://www.nitrc.org/docman/view.php/26/204/TheNIfTI1Format2004.pdf https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/Formats https://brainder.org/2012/09/23/the-nifti-file-format/ https://brainder.org/2015/04/03/the-nifti-2-file-format/ https://nifti.nimh.nih.gov/nifti-1/documentation/hbm_nifti_2004.pdf https://nifti.nimh.nih.gov/nifti-2","title":"NIfTI"},{"location":"Science/Scientific-File-Formats/#mrc","text":"Chiefly used by Relion. http://www.ccpem.ac.uk/mrc_format/mrc2014.php","title":"MRC"},{"location":"Science/Scientific-File-Formats/#compression","text":"FPZip : \"Think of fpzip as the floating-point analogue to PNG image compression and zfp as advanced JPEG for floating-point arrays.\" mtscomp : If fpzip/zfp are PNG/JPEG analogues, mtscomp seems to be more of an MP3 / audio compression analogue.","title":"Compression"},{"location":"Science/Scientific-Programming/","text":"Scientific Programming Conventional development techniques seem to be ill-suited for scientific programming, which is more focused on rapid prototyping than on producing stable, enterprise code that's supposed to run for long stretches of time. I'm going to spitball some ideas for how scientific programming methodologies might work here. General Algorithm Prototype in Jupyter notebook / an IDE / JupyterLab . Convert notebook to plain, no-frills script. Create an interface to the script with something like argparse. This allows you to execute your script's logic via bash, which is a common environment that can access scripts written in many different languages. Wrap that interface in CWL so that it can work with a workflow scheduler / editor. Create a rudimentary GUI if your code requires human interaction with something like Gooey .","title":"Scientific Programming"},{"location":"Science/Scientific-Programming/#scientific-programming","text":"Conventional development techniques seem to be ill-suited for scientific programming, which is more focused on rapid prototyping than on producing stable, enterprise code that's supposed to run for long stretches of time. I'm going to spitball some ideas for how scientific programming methodologies might work here.","title":"Scientific Programming"},{"location":"Science/Scientific-Programming/#general-algorithm","text":"Prototype in Jupyter notebook / an IDE / JupyterLab . Convert notebook to plain, no-frills script. Create an interface to the script with something like argparse. This allows you to execute your script's logic via bash, which is a common environment that can access scripts written in many different languages. Wrap that interface in CWL so that it can work with a workflow scheduler / editor. Create a rudimentary GUI if your code requires human interaction with something like Gooey .","title":"General Algorithm"},{"location":"Science/Scientific-Workflow-Management/","text":"Scientific Workflow Management Articles A Survey of Data-Intensive Scientific Workflow Management Parallelization in Scientific Workflow Management Systems Standards CWL Packages Taverna Arvados Kepler Pegasus KNIME Swift Galaxy LONI","title":"Scientific Workflow Management"},{"location":"Science/Scientific-Workflow-Management/#scientific-workflow-management","text":"","title":"Scientific Workflow Management"},{"location":"Science/Scientific-Workflow-Management/#articles","text":"A Survey of Data-Intensive Scientific Workflow Management Parallelization in Scientific Workflow Management Systems","title":"Articles"},{"location":"Science/Scientific-Workflow-Management/#standards","text":"CWL","title":"Standards"},{"location":"Science/Scientific-Workflow-Management/#packages","text":"Taverna Arvados Kepler Pegasus KNIME Swift Galaxy LONI","title":"Packages"},{"location":"Science/Scientist-Computer-Symbiosis-Obstacles/","text":"Top Challenges in Facilitating Man-Computer Symbiosis in 2019 In 1960, J.C.R. Licklider identified the following as the greatest obstacles in facilitating human / computer interaction (see here ): A need for voice recognition. A need for GUIs (fulfilled at PARC) and tablet interfaces (fulfilled more recently with the smartphone revolution). A need for handwriting recognition- this need has become substantially less relevant, as most schoolchildren are taught to type in contemporary times (certainly not the case in my parents' generation), alternative Trie-based touch typing mechanisms are ubiquitous, and cursive is a dying art. Indelible memory. Some sort of workflow system (there was a passage where he anticipated Unix pipes). More elements I've forgotten. Essentially, Licklider thought that man-computer symbiosis would look something like this (or this full length version ). Today, my completely ancedotal observation is that for human/computer symbiosis to be optimal, scientific researchers need: Consistent and intuitive packaging and reproducible environments. As an addendum, the need for consistent packaging has been a perennial issue in scientific computing- that's why Seth Vidal and Michael Stenner wrote yum and why SBGrid was created. Scientific literature condensed into machine-parseable data that can be readily extracted (i.e., non-voice / handwriting related NLP). Methods and results sections need to be easy to aggregate and evaluate at a glance- the excessive verbosity of most publications is a hindrance to scientific progress. Metrics similar to those aggregated in The secret lives of experiments should be automatically generated constantly, without need for painstaking human intervention. Related to the above, there should be open datasets that are readily discoverable. Some sort of intuitive workflow system. Hardware-based accelerators (although this may be more of an optimization; see ASICs ). Of the above, I think that I personally can only contribute to the top 3, and that I should focus especially on packaging and environments / DevOps / SciOps in the immediate future. Spack , Nix and Ansible (or other config management systems) seem to be the best tools for this. I lean towards Spack and Ansible, since they seem to have the lowest barrier to entry conceptually / are more simple and elegant to maintain. The usage of DSLs should probably be discouraged absent any other additional advantages (I'm looking at you, Puppet, Cfengine, and nix). Using NLP to enhance scholarship is a problem that ideally academic libraries should tackle via enhancements to their institutional repositories (or at least subscription services perhaps could work on this). It is possible (even likely) that scientists may delegate this responsibility to themselves, however (as occurred with arXiv). I see this as being of interest to me, but it should not occupy as much time as packaging. I think that dataset discovery is also within the scope of libraries, although also it could be within the province of a data engineer. I'd be interested in continuing to work on this to some degree due to my past involvement with BIDS , although I think at most I would just do proof-of-concept stuff on my own time (e.g., embedding metadata within GZIP file headers). A workflow system is definitely something a data engineer would work on, although I think that there's still some overlap with systems administration for this where I could become involved. Setting up a workflow manager would fall within the same realm as setting up a job scheduler such as SLURM. Working on CWL wrappers could be fun, and is arguably more in line with my primary goal of packaging/making reproducible environments. I don't foresee working on hardware acceleration in any capacity beyond remaining aware of trends in industry and having enough knowledge to make informed choices / decipher a chip maker's propaganda accurately enough. Designing chips in FPGAs could be fun, although my call is that it's better to leave that sort of stuff to the professionals / electrical engineers / chemical engineers. Furthermore, I don't even know the field well enough to really know where I could best contribute- the only unimplemented logic I'm aware of that could potentially useful would be for some sort of Unum processing. Again, there are already professionals working on this at the various chip companies. I address the above problems in more depth in the following pages: Packaging Metascience TODO - probable sub-page of Open-Science . Relevant articles: here TODO - CWL-Make might be adapted for this. ASICs","title":"Scientist Computer Symbiosis Obstacles"},{"location":"Science/Scientist-Computer-Symbiosis-Obstacles/#top-challenges-in-facilitating-man-computer-symbiosis-in-2019","text":"In 1960, J.C.R. Licklider identified the following as the greatest obstacles in facilitating human / computer interaction (see here ): A need for voice recognition. A need for GUIs (fulfilled at PARC) and tablet interfaces (fulfilled more recently with the smartphone revolution). A need for handwriting recognition- this need has become substantially less relevant, as most schoolchildren are taught to type in contemporary times (certainly not the case in my parents' generation), alternative Trie-based touch typing mechanisms are ubiquitous, and cursive is a dying art. Indelible memory. Some sort of workflow system (there was a passage where he anticipated Unix pipes). More elements I've forgotten. Essentially, Licklider thought that man-computer symbiosis would look something like this (or this full length version ). Today, my completely ancedotal observation is that for human/computer symbiosis to be optimal, scientific researchers need: Consistent and intuitive packaging and reproducible environments. As an addendum, the need for consistent packaging has been a perennial issue in scientific computing- that's why Seth Vidal and Michael Stenner wrote yum and why SBGrid was created. Scientific literature condensed into machine-parseable data that can be readily extracted (i.e., non-voice / handwriting related NLP). Methods and results sections need to be easy to aggregate and evaluate at a glance- the excessive verbosity of most publications is a hindrance to scientific progress. Metrics similar to those aggregated in The secret lives of experiments should be automatically generated constantly, without need for painstaking human intervention. Related to the above, there should be open datasets that are readily discoverable. Some sort of intuitive workflow system. Hardware-based accelerators (although this may be more of an optimization; see ASICs ). Of the above, I think that I personally can only contribute to the top 3, and that I should focus especially on packaging and environments / DevOps / SciOps in the immediate future. Spack , Nix and Ansible (or other config management systems) seem to be the best tools for this. I lean towards Spack and Ansible, since they seem to have the lowest barrier to entry conceptually / are more simple and elegant to maintain. The usage of DSLs should probably be discouraged absent any other additional advantages (I'm looking at you, Puppet, Cfengine, and nix). Using NLP to enhance scholarship is a problem that ideally academic libraries should tackle via enhancements to their institutional repositories (or at least subscription services perhaps could work on this). It is possible (even likely) that scientists may delegate this responsibility to themselves, however (as occurred with arXiv). I see this as being of interest to me, but it should not occupy as much time as packaging. I think that dataset discovery is also within the scope of libraries, although also it could be within the province of a data engineer. I'd be interested in continuing to work on this to some degree due to my past involvement with BIDS , although I think at most I would just do proof-of-concept stuff on my own time (e.g., embedding metadata within GZIP file headers). A workflow system is definitely something a data engineer would work on, although I think that there's still some overlap with systems administration for this where I could become involved. Setting up a workflow manager would fall within the same realm as setting up a job scheduler such as SLURM. Working on CWL wrappers could be fun, and is arguably more in line with my primary goal of packaging/making reproducible environments. I don't foresee working on hardware acceleration in any capacity beyond remaining aware of trends in industry and having enough knowledge to make informed choices / decipher a chip maker's propaganda accurately enough. Designing chips in FPGAs could be fun, although my call is that it's better to leave that sort of stuff to the professionals / electrical engineers / chemical engineers. Furthermore, I don't even know the field well enough to really know where I could best contribute- the only unimplemented logic I'm aware of that could potentially useful would be for some sort of Unum processing. Again, there are already professionals working on this at the various chip companies. I address the above problems in more depth in the following pages: Packaging Metascience TODO - probable sub-page of Open-Science . Relevant articles: here TODO - CWL-Make might be adapted for this. ASICs","title":"Top Challenges in Facilitating Man-Computer Symbiosis in 2019"},{"location":"Science/Cognitive-Science/","text":"Notes on cognitive science. Articles: Could a Neuroscientist Understand a Microprocessor? How Academia and Publishing are Destroying Scientific Innovation: A Conversation with Sydney Brenner","title":"Index"},{"location":"Science/Cognitive-Science/#articles","text":"Could a Neuroscientist Understand a Microprocessor? How Academia and Publishing are Destroying Scientific Innovation: A Conversation with Sydney Brenner","title":"Articles:"},{"location":"Science/Cognitive-Science/Decision-Making/","text":"Decision Making Expected Utility Prospect Theory Dual Process Theories References / External Links Lectures: Robert Shiller (Just Prospect Theory)_ Robert Shiller : Behavioral Finance_ UC Berkeley Course on Neuroeconomics_ Wikipedia: Prospect Theory_ Risk Aversion_ Loss Aversion_ LessWrong_: Prospect Theory: A Framework for Understanding Cognitive Biases_ Scholarpedia: Neuroeconomics_","title":"Decision Making"},{"location":"Science/Cognitive-Science/Decision-Making/#decision-making","text":"","title":"Decision Making"},{"location":"Science/Cognitive-Science/Decision-Making/#expected-utility","text":"","title":"Expected Utility"},{"location":"Science/Cognitive-Science/Decision-Making/#prospect-theory","text":"","title":"Prospect Theory"},{"location":"Science/Cognitive-Science/Decision-Making/#dual-process-theories","text":"","title":"Dual Process Theories"},{"location":"Science/Cognitive-Science/Decision-Making/#references-external-links","text":"Lectures: Robert Shiller (Just Prospect Theory)_ Robert Shiller : Behavioral Finance_ UC Berkeley Course on Neuroeconomics_ Wikipedia: Prospect Theory_ Risk Aversion_ Loss Aversion_ LessWrong_: Prospect Theory: A Framework for Understanding Cognitive Biases_ Scholarpedia: Neuroeconomics_","title":"References / External Links"},{"location":"Science/Cognitive-Science/Expertise/","text":"Likely dated video on expertise from Bell Labs: https://www.youtube.com/watch?v=uMN3hVe2rm8","title":"Expertise"},{"location":"Science/Cognitive-Science/Learning/","text":"Learning This page contains notes from the MOOC Learning How to Learn on Coursera. I expect there to be much overlap in scope and content between this and Memory . Week 1 Diffuse vs Focused Thinking Focused thought is analogous to conscious, task-related thought. Could also be analogous to Csikszentmihalyi's flow or a telic/goal-oriented state. Diffuse thought is analogous to the default mode network . Focused thought constricts our neural activity to very small, specific networks in the brain. Diffuse thought allows activity to branch more readily from one network to a possibly unrelated one, such as synesthesia. To learn a new skill or discipline, you need to balance diffuse and focused thought (similar to balancing exercise and resting). Focused learning like a workout. Procrastination Thoughts about a task can activate the insular cortex, leading to feelings of pain. Do cope with this, people may switch to a more pleasurable activity. However, if you just start the undesirable task the feelings of pain quickly taper off. This is reminiscent of some aspects of avoidant behavior that might be treated by CBT. Recommendation of time-boxing technique (Pomodoro). Practice Math might be more difficult to master because it involves more abstraction. You want to strengthen neural connections through repetition- what fires together... Importance of depth Memory Working memory based in prefrontal cortex. Working memory doesn't hold 7 chunks, it holds 4 chunks. No magic number 7. Working memory is like a CPU cache- very small, only holds a little when closer to the core. LTM like a warehouse, distributed over an area. LTM is a NoSQL database? Repetition of facts / procedures when learning is like a manual replication procedure sort of? Spaced repetition is recommended. Cramming is bad. Sleep Wakefulness state generates toxic byproducts in brain. Brain cells (neurons? glia?) shrink during sleep, allowing toxins to be washed away. Sleep as self-cleaning (Xie et al, 2013). Coffee/caffeine then is not inherently bad in the short-term, but can prevent a necessary cleaning. Sleep prunes unwanted info, strengthens wanted info (sort of like deduplication). Sleep is diffuse mode / dmn on steroids; allows connections to be made that might not otherwise be made under the watchful eye of the prefrontal cortex. Sejnowski Interview Mental multi-tasking as context switching with a single core CPU (since most people aren't actually able to attend to multiple things at once). An enriching environment (undergrad, companies with good culture- i.e., Bell Labs) leads to stronger connections in the hippocampus; impoverished environments (cubical farms, isolation) less so. In the absence of an enriching environment, exercise can increase neurons (i.e., increase BDNF) Week 2 Chunking Chunk: A set of relationships tying declarative facts together (like a graph). A network of neurons that are strongly interconnected through repeated activation. The cohesiveness of a chunk makes the individual facts it contains easier to remember. Helps create a \"big picture\" rather than getting tunnel vision about individual facts. A chunk is analogous to a simplified mental map (see Mappers vs Packers ; the folks on that Wiki aren't rigorous personality psychologists though so I take it with a grain of salt). Stress diminishes our ability to form chunks via our working memory. Chunking is a bit like compression. Chunks as schemata? How to chunk Get a sense of a pattern for what you want to learn (i.e., listen or watch someone playing to learn a song; mentorship / learning from someone else who has more mastery). Learn specific mini-chunks (i.e., musical phrases for a song). Synthesize these mini-chunks into a larger chunk. Goal is to make a chunk second nature / not hard to activate (i.e., language). Learning at the start involves a heavy cognitive load, so it makes sense to use example / toy problems and try to imitate them. Like working up gradually from 1 mile runs to 5k to 15k. Worked out solutions are a resource- you still need to use your own judgement to construct chunks. A step-by-step overview of a general chunking process: Focus on the information that you would like to chunk/synthesize (i.e., turn off your phone, minimize interrupts). Comprehend concept (alternate between focused and diffuse networks if you do not have a good gist of concept). Direct experience is often necessary for this (since being able to do something purely in theory doesn't really confer mastery). Practice. Gain context for the chunk (i.e., where it should be connected, and where it should not be). Chunking is bottom-up (via practice), and top-down (taking currently existent networks and connecting them to new discrete chunks). Context as a bridge between practice and higher-level schema. Context can be gained by skimming chapter section titles and figures first. Illusions of Competence Re-reading a chapter in a textbook can be ineffective at crystallizing a memory. More effective is to read a chapter once, look away, and see how much you can recall about its contents (an active approach). Retrieval of knowledge itself can enhance connections. Making a semantic map for studying / organizing before you actually have a mental map in your head is not as effective. Re-readings are effective when spaced. Common illusions of competence: Reading a worked out problem and feeling that you totally understand it conceptually, but you haven't put any mental effort into it yourself / haven't actively learned it so you don't actually know it. Exposure / superficial understanding alone is not enough. Highlighting and underlining is not useful when overdone. Be minimal and look for main ideas beforehand. Margin notes, however are good if used to make connections. Mind-mapping is not as effective as simple recall in learning. It's more important to have the nodes of knowledge set in memory rather than the connections- the connections will result later as long as the knowledge is there. Thinking material is internalized just because it's available (via Google, textbook). The best way to know if you're actually learning is to test yourself. Recall can be seen as a mini-test. Any mistakes you make in your self-test can be used to re-educate yourself (for systems administration, this is what labs / testing environments are for). Switch up your environment when learning (environment-specific learning). Sejnowski Lectures Alpha in monkey troops has highest levels of serotonin. Serotonin associated with lower risk-taking (?). Inmates in prisons have lowest levels of serotonon. Acetycholine involved in focus (i.e., Alpha GPC or eggs). Dopamine can be implicated in long-term rewards as well The Value of a Library of Chunks Successful CEOS (Bill Gates, etc) set aside week long reading periods... therefore you should too . Gradually build up chunks (e.g., chessmasters have library of patterns that they acquire as they gain expertise). Design Patterns in programming, related fields. Transfer: Concepts and problem-solving methods from one field can help you / be re-appropriated for use in another field. Manners of tackling problems: Sequentially (focused mode) or holistically (diffuse mode). The diffuse mode can make connections between tightly-coupled chunks that were created in the focused mode. Diffuse mode insights should be verified with focused mode (since intuition is not always correct; we are in fact very irrational animals). Law of serendipity: Once one concept is added to mental library, more concepts will come more easily. Overlearning Overlearning is generally good if you can establish some level of automaticity for a skill / knowledge. Continuing to practice after you've mastered as much as you can in a study session can be a waste of time. Like learning how to do carpentry with only a hammer (i.e., you get a very constricted view of knowledge). It's important to walk away. Re-learning something you've already learned can make you feel better at something than you are, since you've actually only mastered the easy stuff and not moved on to harder topics (i.e., there's always something more difficult to learn / fields are nuanced). Deliberate practice: Deliberately focus on more difficult topics rather than what you've already mastered. Einstellung: Your initial idea might prevent a better idea from being found (i.e., functional fixedness). If you reinforce your initial idea, you need to unlearn it (i.e., if you are taught that ego depletion or any of the numerous non-replicable effects in psychology are true/valid, it may be harder to pivot your paradigm). Interleaving: Switching between using different tools/operators in different situations. Try using different concepts, procedures, etc. Skip between different chapters. Just knowing how to use a tool isn't enough, you need to know when. Developing expertise in multiple fields means you can have cross-pollination of ideas. But your expertise may be uneven compared to a specialist. One discipline specialists might be too entrenched in their field and not be able to come up with novel approaches. Week 3 Procrastination As we repeat processes, they become automatic on some level (habits). There is a loop of stimulus, response and reward (conditioning) that reinforces this automaticity. Most habits allow us to conserve cognitive resources, by taking common tasks and having them automated at a low-level of the brain. Bad habits (like watching too much Andromeda or Gordon Ramsay's Kitchen Nightmares or some other mix of trashy television), however, can also be automated, which is not such a good thing. These bad habits can be likened to addictions. Techniques Focus on process rather than product. It's more important to kick off the steps to get to the answer (Pareto principle) than the end result (which if you contemplate beforehand can cause you internal pain). Automaticity is more process-friendly than product-friendly. What Marcus Aurelius said Replacing bad habits with good ones (CBT) Determine stimulus (environment, time) that causes bad habit to kick in. Establish a new habit for studying, possibly with a different stimulus (i.e., the learning room rather than the common room in the apartment). Find a new reward for the good habit. Replace your belief system (stoicism looks good). Write a weekly todo list and then write daily lists each day in the evening. Tip: Mix mentally intense tasks with autopilot physical tasks. Tip 2: Plan a quitting time. Leisure is important; people with hobbies outperform people whose work life is their entire life. More Memory Multimodal learning is good. The instructors champion co-opting visuospatial systems (since they are more evolutionary well-honed) in particular (citing this )- related to method of Loci. A discussion of HM. A discussion of mnemonics- including making up phrases / acronyms and the method of Loci. Mnemonics don't incur a penalty to get to semi-skilled expertise (?) Spacing works because it invokes reconsolidating memories / updating them from an inactive state, thus making them stronger (i.e., some amount of elaboration must occur when the memories are being re-consolidated; I don't think this is the traditional view of this theory however ) Week 4 Periods where there is difficulty in understanding new knowledge / skills might be reflective of a period of mental re-structuring. Creating metaphors / relations to concepts can help learning (also can help in creating new knowledge, e.g., fluidic view of electricity in 18th century). Metaphors can help you get out of functional fixedness (local minima) Personifying a concept and pretending to be it could be helpful in some cases. Natural intelligence is important, but even just mediocre intelligence with lots of practice can do great things. \u201cPerseverance is a virtue of the less brilliant.\u201d --Ramon y Cajal Imposter syndrome is not as uncommon as you'd think (reassuring). What you learn from one source is only one part of the big picture (i.e., it's a good idea to seek out many different texts / learning materials / lectures and synthesize them yourself). The right hemisphere is biased towards big picture thinking (though left/right dichotomy is not strict). Right hemisphere = \"devil's advocate\" that questions status quo, left hemisphere is more rigid. Social study sessions can help you realize knowledge gaps (as long as everyone's prepared and on-topic). A checklist for tests Tests as a way of learning (periodically quizzing yourself- 1 hr of testing more useful than 1 hr of studying in some ways). Start with hard problems on a test, switch to easy problems after a minute or two (if you're not done). Hard problems will still be primed when you return to them later, and diffuse thinking may lead you to conclusions more easily (hard start - jump to easy).","title":"Learning"},{"location":"Science/Cognitive-Science/Learning/#learning","text":"This page contains notes from the MOOC Learning How to Learn on Coursera. I expect there to be much overlap in scope and content between this and Memory .","title":"Learning"},{"location":"Science/Cognitive-Science/Learning/#week-1","text":"","title":"Week 1"},{"location":"Science/Cognitive-Science/Learning/#diffuse-vs-focused-thinking","text":"Focused thought is analogous to conscious, task-related thought. Could also be analogous to Csikszentmihalyi's flow or a telic/goal-oriented state. Diffuse thought is analogous to the default mode network . Focused thought constricts our neural activity to very small, specific networks in the brain. Diffuse thought allows activity to branch more readily from one network to a possibly unrelated one, such as synesthesia. To learn a new skill or discipline, you need to balance diffuse and focused thought (similar to balancing exercise and resting). Focused learning like a workout.","title":"Diffuse vs Focused Thinking"},{"location":"Science/Cognitive-Science/Learning/#procrastination","text":"Thoughts about a task can activate the insular cortex, leading to feelings of pain. Do cope with this, people may switch to a more pleasurable activity. However, if you just start the undesirable task the feelings of pain quickly taper off. This is reminiscent of some aspects of avoidant behavior that might be treated by CBT. Recommendation of time-boxing technique (Pomodoro).","title":"Procrastination"},{"location":"Science/Cognitive-Science/Learning/#practice","text":"Math might be more difficult to master because it involves more abstraction. You want to strengthen neural connections through repetition- what fires together... Importance of depth","title":"Practice"},{"location":"Science/Cognitive-Science/Learning/#memory","text":"Working memory based in prefrontal cortex. Working memory doesn't hold 7 chunks, it holds 4 chunks. No magic number 7. Working memory is like a CPU cache- very small, only holds a little when closer to the core. LTM like a warehouse, distributed over an area. LTM is a NoSQL database? Repetition of facts / procedures when learning is like a manual replication procedure sort of? Spaced repetition is recommended. Cramming is bad.","title":"Memory"},{"location":"Science/Cognitive-Science/Learning/#sleep","text":"Wakefulness state generates toxic byproducts in brain. Brain cells (neurons? glia?) shrink during sleep, allowing toxins to be washed away. Sleep as self-cleaning (Xie et al, 2013). Coffee/caffeine then is not inherently bad in the short-term, but can prevent a necessary cleaning. Sleep prunes unwanted info, strengthens wanted info (sort of like deduplication). Sleep is diffuse mode / dmn on steroids; allows connections to be made that might not otherwise be made under the watchful eye of the prefrontal cortex.","title":"Sleep"},{"location":"Science/Cognitive-Science/Learning/#sejnowski-interview","text":"Mental multi-tasking as context switching with a single core CPU (since most people aren't actually able to attend to multiple things at once). An enriching environment (undergrad, companies with good culture- i.e., Bell Labs) leads to stronger connections in the hippocampus; impoverished environments (cubical farms, isolation) less so. In the absence of an enriching environment, exercise can increase neurons (i.e., increase BDNF)","title":"Sejnowski Interview"},{"location":"Science/Cognitive-Science/Learning/#week-2","text":"","title":"Week 2"},{"location":"Science/Cognitive-Science/Learning/#chunking","text":"Chunk: A set of relationships tying declarative facts together (like a graph). A network of neurons that are strongly interconnected through repeated activation. The cohesiveness of a chunk makes the individual facts it contains easier to remember. Helps create a \"big picture\" rather than getting tunnel vision about individual facts. A chunk is analogous to a simplified mental map (see Mappers vs Packers ; the folks on that Wiki aren't rigorous personality psychologists though so I take it with a grain of salt). Stress diminishes our ability to form chunks via our working memory. Chunking is a bit like compression. Chunks as schemata?","title":"Chunking"},{"location":"Science/Cognitive-Science/Learning/#how-to-chunk","text":"Get a sense of a pattern for what you want to learn (i.e., listen or watch someone playing to learn a song; mentorship / learning from someone else who has more mastery). Learn specific mini-chunks (i.e., musical phrases for a song). Synthesize these mini-chunks into a larger chunk. Goal is to make a chunk second nature / not hard to activate (i.e., language). Learning at the start involves a heavy cognitive load, so it makes sense to use example / toy problems and try to imitate them. Like working up gradually from 1 mile runs to 5k to 15k. Worked out solutions are a resource- you still need to use your own judgement to construct chunks. A step-by-step overview of a general chunking process: Focus on the information that you would like to chunk/synthesize (i.e., turn off your phone, minimize interrupts). Comprehend concept (alternate between focused and diffuse networks if you do not have a good gist of concept). Direct experience is often necessary for this (since being able to do something purely in theory doesn't really confer mastery). Practice. Gain context for the chunk (i.e., where it should be connected, and where it should not be). Chunking is bottom-up (via practice), and top-down (taking currently existent networks and connecting them to new discrete chunks). Context as a bridge between practice and higher-level schema. Context can be gained by skimming chapter section titles and figures first.","title":"How to chunk"},{"location":"Science/Cognitive-Science/Learning/#illusions-of-competence","text":"Re-reading a chapter in a textbook can be ineffective at crystallizing a memory. More effective is to read a chapter once, look away, and see how much you can recall about its contents (an active approach). Retrieval of knowledge itself can enhance connections. Making a semantic map for studying / organizing before you actually have a mental map in your head is not as effective. Re-readings are effective when spaced. Common illusions of competence: Reading a worked out problem and feeling that you totally understand it conceptually, but you haven't put any mental effort into it yourself / haven't actively learned it so you don't actually know it. Exposure / superficial understanding alone is not enough. Highlighting and underlining is not useful when overdone. Be minimal and look for main ideas beforehand. Margin notes, however are good if used to make connections. Mind-mapping is not as effective as simple recall in learning. It's more important to have the nodes of knowledge set in memory rather than the connections- the connections will result later as long as the knowledge is there. Thinking material is internalized just because it's available (via Google, textbook). The best way to know if you're actually learning is to test yourself. Recall can be seen as a mini-test. Any mistakes you make in your self-test can be used to re-educate yourself (for systems administration, this is what labs / testing environments are for). Switch up your environment when learning (environment-specific learning).","title":"Illusions of Competence"},{"location":"Science/Cognitive-Science/Learning/#sejnowski-lectures","text":"Alpha in monkey troops has highest levels of serotonin. Serotonin associated with lower risk-taking (?). Inmates in prisons have lowest levels of serotonon. Acetycholine involved in focus (i.e., Alpha GPC or eggs). Dopamine can be implicated in long-term rewards as well","title":"Sejnowski Lectures"},{"location":"Science/Cognitive-Science/Learning/#the-value-of-a-library-of-chunks","text":"Successful CEOS (Bill Gates, etc) set aside week long reading periods... therefore you should too . Gradually build up chunks (e.g., chessmasters have library of patterns that they acquire as they gain expertise). Design Patterns in programming, related fields. Transfer: Concepts and problem-solving methods from one field can help you / be re-appropriated for use in another field. Manners of tackling problems: Sequentially (focused mode) or holistically (diffuse mode). The diffuse mode can make connections between tightly-coupled chunks that were created in the focused mode. Diffuse mode insights should be verified with focused mode (since intuition is not always correct; we are in fact very irrational animals). Law of serendipity: Once one concept is added to mental library, more concepts will come more easily.","title":"The Value of a Library of Chunks"},{"location":"Science/Cognitive-Science/Learning/#overlearning","text":"Overlearning is generally good if you can establish some level of automaticity for a skill / knowledge. Continuing to practice after you've mastered as much as you can in a study session can be a waste of time. Like learning how to do carpentry with only a hammer (i.e., you get a very constricted view of knowledge). It's important to walk away. Re-learning something you've already learned can make you feel better at something than you are, since you've actually only mastered the easy stuff and not moved on to harder topics (i.e., there's always something more difficult to learn / fields are nuanced). Deliberate practice: Deliberately focus on more difficult topics rather than what you've already mastered. Einstellung: Your initial idea might prevent a better idea from being found (i.e., functional fixedness). If you reinforce your initial idea, you need to unlearn it (i.e., if you are taught that ego depletion or any of the numerous non-replicable effects in psychology are true/valid, it may be harder to pivot your paradigm). Interleaving: Switching between using different tools/operators in different situations. Try using different concepts, procedures, etc. Skip between different chapters. Just knowing how to use a tool isn't enough, you need to know when. Developing expertise in multiple fields means you can have cross-pollination of ideas. But your expertise may be uneven compared to a specialist. One discipline specialists might be too entrenched in their field and not be able to come up with novel approaches.","title":"Overlearning"},{"location":"Science/Cognitive-Science/Learning/#week-3","text":"","title":"Week 3"},{"location":"Science/Cognitive-Science/Learning/#procrastination_1","text":"As we repeat processes, they become automatic on some level (habits). There is a loop of stimulus, response and reward (conditioning) that reinforces this automaticity. Most habits allow us to conserve cognitive resources, by taking common tasks and having them automated at a low-level of the brain. Bad habits (like watching too much Andromeda or Gordon Ramsay's Kitchen Nightmares or some other mix of trashy television), however, can also be automated, which is not such a good thing. These bad habits can be likened to addictions. Techniques Focus on process rather than product. It's more important to kick off the steps to get to the answer (Pareto principle) than the end result (which if you contemplate beforehand can cause you internal pain). Automaticity is more process-friendly than product-friendly. What Marcus Aurelius said Replacing bad habits with good ones (CBT) Determine stimulus (environment, time) that causes bad habit to kick in. Establish a new habit for studying, possibly with a different stimulus (i.e., the learning room rather than the common room in the apartment). Find a new reward for the good habit. Replace your belief system (stoicism looks good). Write a weekly todo list and then write daily lists each day in the evening. Tip: Mix mentally intense tasks with autopilot physical tasks. Tip 2: Plan a quitting time. Leisure is important; people with hobbies outperform people whose work life is their entire life.","title":"Procrastination"},{"location":"Science/Cognitive-Science/Learning/#more-memory","text":"Multimodal learning is good. The instructors champion co-opting visuospatial systems (since they are more evolutionary well-honed) in particular (citing this )- related to method of Loci. A discussion of HM. A discussion of mnemonics- including making up phrases / acronyms and the method of Loci. Mnemonics don't incur a penalty to get to semi-skilled expertise (?) Spacing works because it invokes reconsolidating memories / updating them from an inactive state, thus making them stronger (i.e., some amount of elaboration must occur when the memories are being re-consolidated; I don't think this is the traditional view of this theory however )","title":"More Memory"},{"location":"Science/Cognitive-Science/Learning/#week-4","text":"Periods where there is difficulty in understanding new knowledge / skills might be reflective of a period of mental re-structuring. Creating metaphors / relations to concepts can help learning (also can help in creating new knowledge, e.g., fluidic view of electricity in 18th century). Metaphors can help you get out of functional fixedness (local minima) Personifying a concept and pretending to be it could be helpful in some cases. Natural intelligence is important, but even just mediocre intelligence with lots of practice can do great things. \u201cPerseverance is a virtue of the less brilliant.\u201d --Ramon y Cajal Imposter syndrome is not as uncommon as you'd think (reassuring). What you learn from one source is only one part of the big picture (i.e., it's a good idea to seek out many different texts / learning materials / lectures and synthesize them yourself). The right hemisphere is biased towards big picture thinking (though left/right dichotomy is not strict). Right hemisphere = \"devil's advocate\" that questions status quo, left hemisphere is more rigid. Social study sessions can help you realize knowledge gaps (as long as everyone's prepared and on-topic). A checklist for tests Tests as a way of learning (periodically quizzing yourself- 1 hr of testing more useful than 1 hr of studying in some ways). Start with hard problems on a test, switch to easy problems after a minute or two (if you're not done). Hard problems will still be primed when you return to them later, and diffuse thinking may lead you to conclusions more easily (hard start - jump to easy).","title":"Week 4"},{"location":"Science/Cognitive-Science/Meditations-on-Neuroscience-Paradigms/","text":"I've been reading Matthew Cobb's The Idea of the Brain: The Past and Future of Neuroscience lately, and one of the points that it has reinforced is that an overrarching paradigm to guide neuroscience research has really not emerged, and that neuroscience has, despite some fairly substantial achievements, made surprisingly little progress in explaining the brain. I'm using this page to track some thoughts I have on this point. Unorganized bullet points / Notes The brain is clearly not a digital, electronic computer. Folks who see the brain as such a computer are confusing the direction of causality (brains inspired computers, not vice-versa) and mistaking the map for the territory. Aside from the fact that John von Neumann himself cautioned against taking the brain/computer analogy too far (as mind uploading advocates and Singulitarians often do), there are also obvious architectural differences. For instance, microprocessors, unlike brains, deliberately seek to curb electrical current using [latches/flip-flops](https://en.wikipedia.org/wiki/Flip-flop_(electronics) in order to avoid glitches. Electronic computers deliberately seek to prevent analog signals from transmitting fully, and deliberately quantize them, whereas there is no such deliberate / systematic quantization of analog signals to digital signals in the brain (cell membrane pumps and channels are not the same, and the action potential is not strictly binary in the same sense as a computer). That said, there's no reason to not use machine learning models / deep learning models as toy scenarios that could provide some insight into biology. This is, in fact, what many theoretical neuroscientists strive to do. A fixation on comparing conventional, electronic digital computers to the brain may not be ultimately as fruitful as anticipated. It may make sense to look to unconventional computing , such as gooware computers for fresh insights. Chemical computing is particularly interesting as an analogy for brain function due to the brain's heavy reliance on chemical messengers. The emphasis on network structure in connectomics is necessary, but not sufficient for an understanding of a brain. I think that we've been regarding the brain just as we regard a Rubin vase , with humanity collectively only seeing a wine glass (connectome) and not seeing the faces (chemical currents). To use another analogy, if a connectome / wiring diagram is like a map, the chemical concentrations throughout the brain at a given point in time are like weather patterns, and regions that are generally more steeped in one neurotransmitter over another (i.e., the nucleus accumbens w/ dopamine) are said to have a climate that is more suited to a given neurotransmitter (similar to how California is wine country, while upstate New York is generally snowy). As per the link here , there are two groups that most scientific computing tools fall into: 1) 3d simulations that often require multicomputing / MPI , and 2) Big Data applications that are more suited for multithreaded environments / nodes that don't require an interconnect or inter-node communication. Most contemporary neuroinformatics tools seem to fall into group 2 (big data), and are mostly focused on image processing and statistical tests. However, a full treatment of the brain will require software in group 1 (3d simulations) to model the flow/currents of neurotransmitters in the chemical soup that flows around a connectome. I would be surprised if I was the only person in the world who has had all of the above thoughts. For another geosciences analogy, I recall that Dan Margulies of Max Planck (now at CNRS apparently) at one point pitched the idea of thinking of neural networks in terms of tectonic plates. I think that this is another good way of viewing how a connectome can dynamically change (although on a much faster time scale).","title":"Meditations on Neuroscience Paradigms"},{"location":"Science/Cognitive-Science/Meditations-on-Neuroscience-Paradigms/#unorganized-bullet-points-notes","text":"The brain is clearly not a digital, electronic computer. Folks who see the brain as such a computer are confusing the direction of causality (brains inspired computers, not vice-versa) and mistaking the map for the territory. Aside from the fact that John von Neumann himself cautioned against taking the brain/computer analogy too far (as mind uploading advocates and Singulitarians often do), there are also obvious architectural differences. For instance, microprocessors, unlike brains, deliberately seek to curb electrical current using [latches/flip-flops](https://en.wikipedia.org/wiki/Flip-flop_(electronics) in order to avoid glitches. Electronic computers deliberately seek to prevent analog signals from transmitting fully, and deliberately quantize them, whereas there is no such deliberate / systematic quantization of analog signals to digital signals in the brain (cell membrane pumps and channels are not the same, and the action potential is not strictly binary in the same sense as a computer). That said, there's no reason to not use machine learning models / deep learning models as toy scenarios that could provide some insight into biology. This is, in fact, what many theoretical neuroscientists strive to do. A fixation on comparing conventional, electronic digital computers to the brain may not be ultimately as fruitful as anticipated. It may make sense to look to unconventional computing , such as gooware computers for fresh insights. Chemical computing is particularly interesting as an analogy for brain function due to the brain's heavy reliance on chemical messengers. The emphasis on network structure in connectomics is necessary, but not sufficient for an understanding of a brain. I think that we've been regarding the brain just as we regard a Rubin vase , with humanity collectively only seeing a wine glass (connectome) and not seeing the faces (chemical currents). To use another analogy, if a connectome / wiring diagram is like a map, the chemical concentrations throughout the brain at a given point in time are like weather patterns, and regions that are generally more steeped in one neurotransmitter over another (i.e., the nucleus accumbens w/ dopamine) are said to have a climate that is more suited to a given neurotransmitter (similar to how California is wine country, while upstate New York is generally snowy). As per the link here , there are two groups that most scientific computing tools fall into: 1) 3d simulations that often require multicomputing / MPI , and 2) Big Data applications that are more suited for multithreaded environments / nodes that don't require an interconnect or inter-node communication. Most contemporary neuroinformatics tools seem to fall into group 2 (big data), and are mostly focused on image processing and statistical tests. However, a full treatment of the brain will require software in group 1 (3d simulations) to model the flow/currents of neurotransmitters in the chemical soup that flows around a connectome. I would be surprised if I was the only person in the world who has had all of the above thoughts. For another geosciences analogy, I recall that Dan Margulies of Max Planck (now at CNRS apparently) at one point pitched the idea of thinking of neural networks in terms of tectonic plates. I think that this is another good way of viewing how a connectome can dynamically change (although on a much faster time scale).","title":"Unorganized bullet points / Notes"},{"location":"Science/Cognitive-Science/Memory/","text":"Memory Memory is an inherently distributed process. The role of the hippocampus, which traditionally was viewed as a centralized store, is more that of an integrator/synthesizer that brings together disparate fragments of experience. Memories are reconstructions rather than recordings. This means that false memories can be formed in the presence of suggestion. Memories can be strongly encoded in one modality (auditory, tactile, etc) but more weakly encoded in others. Our ability to recall events is heavily dependent upon contextual cues. The following notes may be inconsistent with what is stated in the literature, as they are an interpretation / recollection. Implicit vs Explicit Memories Implicit : Subconsciously-recalled memories that can still have an effect on how we behave. These are memories in their 'raw' state, before being integrated by the hippocampus. They are at least partially traceable to cortical regions. Implicit memories can be seen in word completion studies of amnesiacs. Procedural : A sub-class of implicit memory. Memories for how to perform a task. Procedural memories are quite often encoded at the level of the basal ganglia / cerebellum. Explicit : Consciously-recalled memories. Our experience of explicit memories is a result of the hippocampus synthesizing together our implicit memories. A similar taxonomy of memory types includes declarative (explicit) and non-declarative (implicit) memory. Declarative memory is further broken down into semantic (factual) or episodic (event-based) memory. Nondeclarative memory encompasses procedural, priming, conditioned, and nonassociative memories. In the programming/computer science world, the functional programming paradigm is analogous to declarative memory, while the procedural paradigm is analogous to procedural memory. There seems to have been another case of interdisciplinary conspiracy at some point. Priming Perceiving something in the environment can cause low-level activation of an implicit memory. This can increase the probability that that memory will later be integrated by the hippocampus into conscious experience. It can also influence reasoning/problem-solving on a subconscious level. Trying to recall something without any cues (free recall) is much much harder than cued recall. Simply recognizing something is the least cognitively expensive form of recall (and ergo the easiest/fastest). Interference Effects Proactive Interference: When old memories prevent the retrieval of new memories (i.e., persistence). Retroactive Interference: When new memories prevent the retrieval of old memories (i.e., overwriting). Example: Learning key-value pairings with one set of keys/values, and then learning another set with the same keys but different values afterwards. When tested for the recall of the initial key-value pairs, participants have harder time recalling values when presented with a key. Fan Effect: The more complex a semantic network / network of relations between concepts, the more time it will take to identify a correct fact. This is because more complex networks take more time to traverse/search. This can partially explain interference effects (via Anderson). Interference effects occur when chunks of information have no obvious relationships to each other. Interference does not occur when redundant information is learned. Working vs Short-term vs Long-term Memory Hebbian theory \"What fires together, wires together.\" Cortical Reinstatement The general idea behind cortical reinstatement is that the act of remembering an event directly engages the original modalities that encoded that event's memory. By this explanation of memory, during the act of remembering, the hippocampus serves as a nexus that integrates disparate circuits from multiple sensory centers. In this way, every time we remember something we are, to a degree, literally reliving it. By this framework, an encoded memory might be more like a set of aggregated C pointers, symlinks, metadata or a book index. A memory would be a structured set of references to circuits in other modalities where the memory is encoded. \"The cortical reinstatement hypothesis of memory retrieval posits that content-specific cortical activity at encoding is reinstated at retrieval. Evidence for cortical reinstatement was found in higher-order sensory regions, reflecting reactivation of complex object-based information.\" -- From Bosch et al (2014) Depth of Processing Think of memory as a graph, with encoded aspects of the environment as nodes. The higher the centrality (number of connections to other nodes) that a given encoding has, the more likely it is to be recalled. Since there are more edges to that encoding, the likelihood of reaching that encoding is increased. In practical terms, the more that you can tie a fact to your other memories / experience, the more likely you are to remember that fact. State-Dependent Processing We are more likely to remember something if in the original context that the memory was encoded. Context can include inner emotional states (mood congruence) and physical locations as well as drug-induced states of mind. For example, studying on the subway will increase your odds of remembering what you studied on the subway. A memory that was encoded while we were angry will be better recalled when we are angry again. Also, a memory that was formed while intoxicated will be better recalled in subsequent intoxicated states. In practical terms, the more you vary your learning context (i.e., switch locations), the more likely you are to recall information in new contexts, since that information will not be tied to a particular place/time/state. Ebbinghaus Curve Primacy and Recency Effects When items are presented in a sequence you are most likely to remember the first and last items on the list. Recency effects are slightly stronger than primacy effects. Spacing Effect Chunking Consolidation One of the purported functions of sleep is to aid in paring down memories and consolidating our knowledge. Studying before sleep might be a good idea. Furthermore, learning seems to happen better at night?(Hockey, Davies, Grey, 1972)? Because of a high arousal state? I'm a bit skeptical on this point. References Cortical Reinstatement http://www.ncbi.nlm.nih.gov/pubmed/23921785 Other links: https://www.youtube.com/watch?v=fcQXOLJu9NM","title":"Memory"},{"location":"Science/Cognitive-Science/Memory/#memory","text":"Memory is an inherently distributed process. The role of the hippocampus, which traditionally was viewed as a centralized store, is more that of an integrator/synthesizer that brings together disparate fragments of experience. Memories are reconstructions rather than recordings. This means that false memories can be formed in the presence of suggestion. Memories can be strongly encoded in one modality (auditory, tactile, etc) but more weakly encoded in others. Our ability to recall events is heavily dependent upon contextual cues. The following notes may be inconsistent with what is stated in the literature, as they are an interpretation / recollection.","title":"Memory"},{"location":"Science/Cognitive-Science/Memory/#implicit-vs-explicit-memories","text":"Implicit : Subconsciously-recalled memories that can still have an effect on how we behave. These are memories in their 'raw' state, before being integrated by the hippocampus. They are at least partially traceable to cortical regions. Implicit memories can be seen in word completion studies of amnesiacs. Procedural : A sub-class of implicit memory. Memories for how to perform a task. Procedural memories are quite often encoded at the level of the basal ganglia / cerebellum. Explicit : Consciously-recalled memories. Our experience of explicit memories is a result of the hippocampus synthesizing together our implicit memories. A similar taxonomy of memory types includes declarative (explicit) and non-declarative (implicit) memory. Declarative memory is further broken down into semantic (factual) or episodic (event-based) memory. Nondeclarative memory encompasses procedural, priming, conditioned, and nonassociative memories. In the programming/computer science world, the functional programming paradigm is analogous to declarative memory, while the procedural paradigm is analogous to procedural memory. There seems to have been another case of interdisciplinary conspiracy at some point.","title":"Implicit vs Explicit Memories"},{"location":"Science/Cognitive-Science/Memory/#priming","text":"Perceiving something in the environment can cause low-level activation of an implicit memory. This can increase the probability that that memory will later be integrated by the hippocampus into conscious experience. It can also influence reasoning/problem-solving on a subconscious level. Trying to recall something without any cues (free recall) is much much harder than cued recall. Simply recognizing something is the least cognitively expensive form of recall (and ergo the easiest/fastest).","title":"Priming"},{"location":"Science/Cognitive-Science/Memory/#interference-effects","text":"Proactive Interference: When old memories prevent the retrieval of new memories (i.e., persistence). Retroactive Interference: When new memories prevent the retrieval of old memories (i.e., overwriting). Example: Learning key-value pairings with one set of keys/values, and then learning another set with the same keys but different values afterwards. When tested for the recall of the initial key-value pairs, participants have harder time recalling values when presented with a key. Fan Effect: The more complex a semantic network / network of relations between concepts, the more time it will take to identify a correct fact. This is because more complex networks take more time to traverse/search. This can partially explain interference effects (via Anderson). Interference effects occur when chunks of information have no obvious relationships to each other. Interference does not occur when redundant information is learned.","title":"Interference Effects"},{"location":"Science/Cognitive-Science/Memory/#working-vs-short-term-vs-long-term-memory","text":"","title":"Working vs Short-term vs Long-term Memory"},{"location":"Science/Cognitive-Science/Memory/#hebbian-theory","text":"\"What fires together, wires together.\"","title":"Hebbian theory"},{"location":"Science/Cognitive-Science/Memory/#cortical-reinstatement","text":"The general idea behind cortical reinstatement is that the act of remembering an event directly engages the original modalities that encoded that event's memory. By this explanation of memory, during the act of remembering, the hippocampus serves as a nexus that integrates disparate circuits from multiple sensory centers. In this way, every time we remember something we are, to a degree, literally reliving it. By this framework, an encoded memory might be more like a set of aggregated C pointers, symlinks, metadata or a book index. A memory would be a structured set of references to circuits in other modalities where the memory is encoded. \"The cortical reinstatement hypothesis of memory retrieval posits that content-specific cortical activity at encoding is reinstated at retrieval. Evidence for cortical reinstatement was found in higher-order sensory regions, reflecting reactivation of complex object-based information.\" -- From Bosch et al (2014)","title":"Cortical Reinstatement"},{"location":"Science/Cognitive-Science/Memory/#depth-of-processing","text":"Think of memory as a graph, with encoded aspects of the environment as nodes. The higher the centrality (number of connections to other nodes) that a given encoding has, the more likely it is to be recalled. Since there are more edges to that encoding, the likelihood of reaching that encoding is increased. In practical terms, the more that you can tie a fact to your other memories / experience, the more likely you are to remember that fact.","title":"Depth of Processing"},{"location":"Science/Cognitive-Science/Memory/#state-dependent-processing","text":"We are more likely to remember something if in the original context that the memory was encoded. Context can include inner emotional states (mood congruence) and physical locations as well as drug-induced states of mind. For example, studying on the subway will increase your odds of remembering what you studied on the subway. A memory that was encoded while we were angry will be better recalled when we are angry again. Also, a memory that was formed while intoxicated will be better recalled in subsequent intoxicated states. In practical terms, the more you vary your learning context (i.e., switch locations), the more likely you are to recall information in new contexts, since that information will not be tied to a particular place/time/state.","title":"State-Dependent Processing"},{"location":"Science/Cognitive-Science/Memory/#ebbinghaus-curve","text":"","title":"Ebbinghaus Curve"},{"location":"Science/Cognitive-Science/Memory/#primacy-and-recency-effects","text":"When items are presented in a sequence you are most likely to remember the first and last items on the list. Recency effects are slightly stronger than primacy effects.","title":"Primacy and Recency Effects"},{"location":"Science/Cognitive-Science/Memory/#spacing-effect","text":"","title":"Spacing Effect"},{"location":"Science/Cognitive-Science/Memory/#chunking","text":"","title":"Chunking"},{"location":"Science/Cognitive-Science/Memory/#consolidation","text":"One of the purported functions of sleep is to aid in paring down memories and consolidating our knowledge. Studying before sleep might be a good idea. Furthermore, learning seems to happen better at night?(Hockey, Davies, Grey, 1972)? Because of a high arousal state? I'm a bit skeptical on this point.","title":"Consolidation"},{"location":"Science/Cognitive-Science/Memory/#references","text":"","title":"References"},{"location":"Science/Cognitive-Science/Memory/#cortical-reinstatement_1","text":"http://www.ncbi.nlm.nih.gov/pubmed/23921785","title":"Cortical Reinstatement"},{"location":"Science/Cognitive-Science/Memory/#other-links","text":"https://www.youtube.com/watch?v=fcQXOLJu9NM","title":"Other links:"},{"location":"Science/Cognitive-Science/Neuroimaging/","text":"Summaries / Reviews from the Neuroimaging Literature General Connectome Networks: From Cells to Systems - offers a taxonomy for the scales of imaging. At a glance (I haven't really read it yet as of 1/7/19), it looks like you could combine different factors at different levels of analysis as part of a useful nested hierarchical schema (e.g., microscale neurovascular coupling and microscale action potentials are components of macroscale fMRI and EEG readings). Functional Brain Imaging: A Comprehensive Survey A Comparative Analysis of Registration Tools: Traditional vs Deep Learning Approach on High Resolution Tissue Cleared Data Performance of Image Registration Tools on High-Resolution 3D Brain Images fMRI Image Quality On the Definition of Signal-To-Noise Ratio and Contrast-To-Noise Ratio for fMRI Data Size and shape matter: The impact of voxel geometry on the identification of small nuclei Task-related Resting State Chao-Gan Yan's rsfMRI Course Open/Shared Datasets Summary Table from Poldrack and Gorgolewski 2014 Cimbi EAS Harvard Aging Brain Study CamCAN UK Biobank MR Physics http://www.grahamwideman.com/gw/brain/orientation/orientterms.htm General Courses Chris Rorden's Course at GA Tech Overview of Functional Magnetic Resonance Imaging Stats on MR Usage https://jamanetwork.com/journals/jama/fullarticle/2674671 https://jamanetwork.com/journals/jama/fullarticle/2720430 https://www.england.nhs.uk/statistics/wp-content/uploads/sites/2/2016/08/Provisional-Monthly-Diagnostic-Imaging-Dataset-Statistics-2017-05-18.pdf File Formats Just as an errant thought, the file format used to store most MRI images (NifTI) is very similar to DV, in the sense that each frame is its own distinct bitmap(?) image. Unlike DV, no compression is applied to individual frames in NifTI (as far as I know). Compression is usually accomplished by gzipping an entire raw NifTI file, but I doubt that this provides an optimal compression ratio- in MPEG compression schemes the previous frame informs what the next frame will look like ( motion compensation ). It seems like something similar could be used for 3d volumetric scans. Why not encode each slice with a lossless video compression algo? This should probably go somewhere else, but for now: Heterogeneous acceleration of volumetric JPEG 2000 using OpenCL EEG Spatial and temporal resolutions of EEG: Is it really black and white? A scalp current density view MEG fNIRS ISOI https://www.photometrics.com/wp-content/uploads/2019/10/Intrinsic-Signal-Optical-Imaging-AppNote.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5466092/ https://doi.org/10.1007/s00701-019-04132-8 https://doi.org/10.1016/j.jneumeth.2004.02.025 https://blog.arduino.cc/2015/11/16/a-low-cost-approach-to-intrinsic-optical-signal/ In Vivo Optical Imaging of Brain Function, 2nd edition","title":"Neuroimaging"},{"location":"Science/Cognitive-Science/Neuroimaging/#summaries-reviews-from-the-neuroimaging-literature","text":"","title":"Summaries / Reviews from the Neuroimaging Literature"},{"location":"Science/Cognitive-Science/Neuroimaging/#general","text":"Connectome Networks: From Cells to Systems - offers a taxonomy for the scales of imaging. At a glance (I haven't really read it yet as of 1/7/19), it looks like you could combine different factors at different levels of analysis as part of a useful nested hierarchical schema (e.g., microscale neurovascular coupling and microscale action potentials are components of macroscale fMRI and EEG readings). Functional Brain Imaging: A Comprehensive Survey A Comparative Analysis of Registration Tools: Traditional vs Deep Learning Approach on High Resolution Tissue Cleared Data Performance of Image Registration Tools on High-Resolution 3D Brain Images","title":"General"},{"location":"Science/Cognitive-Science/Neuroimaging/#fmri","text":"","title":"fMRI"},{"location":"Science/Cognitive-Science/Neuroimaging/#image-quality","text":"On the Definition of Signal-To-Noise Ratio and Contrast-To-Noise Ratio for fMRI Data Size and shape matter: The impact of voxel geometry on the identification of small nuclei","title":"Image Quality"},{"location":"Science/Cognitive-Science/Neuroimaging/#task-related","text":"","title":"Task-related"},{"location":"Science/Cognitive-Science/Neuroimaging/#resting-state","text":"Chao-Gan Yan's rsfMRI Course","title":"Resting State"},{"location":"Science/Cognitive-Science/Neuroimaging/#openshared-datasets","text":"Summary Table from Poldrack and Gorgolewski 2014 Cimbi EAS Harvard Aging Brain Study CamCAN UK Biobank","title":"Open/Shared Datasets"},{"location":"Science/Cognitive-Science/Neuroimaging/#mr-physics","text":"http://www.grahamwideman.com/gw/brain/orientation/orientterms.htm","title":"MR Physics"},{"location":"Science/Cognitive-Science/Neuroimaging/#general-courses","text":"Chris Rorden's Course at GA Tech Overview of Functional Magnetic Resonance Imaging","title":"General Courses"},{"location":"Science/Cognitive-Science/Neuroimaging/#stats-on-mr-usage","text":"https://jamanetwork.com/journals/jama/fullarticle/2674671 https://jamanetwork.com/journals/jama/fullarticle/2720430 https://www.england.nhs.uk/statistics/wp-content/uploads/sites/2/2016/08/Provisional-Monthly-Diagnostic-Imaging-Dataset-Statistics-2017-05-18.pdf","title":"Stats on MR Usage"},{"location":"Science/Cognitive-Science/Neuroimaging/#file-formats","text":"Just as an errant thought, the file format used to store most MRI images (NifTI) is very similar to DV, in the sense that each frame is its own distinct bitmap(?) image. Unlike DV, no compression is applied to individual frames in NifTI (as far as I know). Compression is usually accomplished by gzipping an entire raw NifTI file, but I doubt that this provides an optimal compression ratio- in MPEG compression schemes the previous frame informs what the next frame will look like ( motion compensation ). It seems like something similar could be used for 3d volumetric scans. Why not encode each slice with a lossless video compression algo? This should probably go somewhere else, but for now: Heterogeneous acceleration of volumetric JPEG 2000 using OpenCL","title":"File Formats"},{"location":"Science/Cognitive-Science/Neuroimaging/#eeg","text":"Spatial and temporal resolutions of EEG: Is it really black and white? A scalp current density view","title":"EEG"},{"location":"Science/Cognitive-Science/Neuroimaging/#meg","text":"","title":"MEG"},{"location":"Science/Cognitive-Science/Neuroimaging/#fnirs","text":"","title":"fNIRS"},{"location":"Science/Cognitive-Science/Neuroimaging/#isoi","text":"https://www.photometrics.com/wp-content/uploads/2019/10/Intrinsic-Signal-Optical-Imaging-AppNote.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5466092/ https://doi.org/10.1007/s00701-019-04132-8 https://doi.org/10.1016/j.jneumeth.2004.02.025 https://blog.arduino.cc/2015/11/16/a-low-cost-approach-to-intrinsic-optical-signal/ In Vivo Optical Imaging of Brain Function, 2nd edition","title":"ISOI"},{"location":"Science/Cognitive-Science/Problem-Solving/","text":"Problem Solving Difference-Reduction Means-End Analysis An errant thought- might an application that is part of a stack might be considered an operator/tool in a way (i.e., your goal state is to have a web server, and you can use Apache as a means to attain that goal state)? Problem Space and Trees Functional Fixedness Set Effect Incubation Effects Problems of Insight Breadth-First vs Depth-First Techniques for organizing problems Breadth-first as making a spec / writing unit tests. The Myst User's Manual This isn't really based off any empirical literature, but in a more applied sense I find the following quotation from the Myst user's manual to be sound advice for most any problem where you don't know what to do: Don\u2019t thrash! If you\u2019re not sure what to do next, clicking everywhere won\u2019t help. Think about what you know already, ask yourself what you need to know, collect your thoughts, and piece them together. Think of related items or places you\u2019ve seen, think of information you\u2019ve been given, pay close attention to everything you see, and don\u2019t forget anything. But most importantly - think of what you would do if you were really there. Remember, there is always [Google] if you need it... Working through the Stack There are varying levels of analysis when you encounter a problem (i.e., OSI levels, dependencies). When troubleshooting it makes sense to treat each of these levels separately, first determining at what level the problem is occurring. So for instance, in a setup like the following: haproxy nginx Apache Application server If a 503 server error is being propagated, you would first want to isolate which layer was causing the 503. You could accomplish this by perusing the logs at each level and comparing them. Operators Increasing log verbosity- this should be a first step in troubleshooting/debugging. References John R. Anderson - Cognitive Psychology and its implications http://cognitivepsychology.wikidot.com/cognition:problem-solving","title":"Problem Solving"},{"location":"Science/Cognitive-Science/Problem-Solving/#problem-solving","text":"","title":"Problem Solving"},{"location":"Science/Cognitive-Science/Problem-Solving/#difference-reduction","text":"","title":"Difference-Reduction"},{"location":"Science/Cognitive-Science/Problem-Solving/#means-end-analysis","text":"An errant thought- might an application that is part of a stack might be considered an operator/tool in a way (i.e., your goal state is to have a web server, and you can use Apache as a means to attain that goal state)?","title":"Means-End Analysis"},{"location":"Science/Cognitive-Science/Problem-Solving/#problem-space-and-trees","text":"","title":"Problem Space and Trees"},{"location":"Science/Cognitive-Science/Problem-Solving/#functional-fixedness","text":"","title":"Functional Fixedness"},{"location":"Science/Cognitive-Science/Problem-Solving/#set-effect","text":"","title":"Set Effect"},{"location":"Science/Cognitive-Science/Problem-Solving/#incubation-effects","text":"","title":"Incubation Effects"},{"location":"Science/Cognitive-Science/Problem-Solving/#problems-of-insight","text":"","title":"Problems of Insight"},{"location":"Science/Cognitive-Science/Problem-Solving/#breadth-first-vs-depth-first-techniques-for-organizing-problems","text":"Breadth-first as making a spec / writing unit tests.","title":"Breadth-First vs Depth-First Techniques for organizing problems"},{"location":"Science/Cognitive-Science/Problem-Solving/#the-myst-users-manual","text":"This isn't really based off any empirical literature, but in a more applied sense I find the following quotation from the Myst user's manual to be sound advice for most any problem where you don't know what to do: Don\u2019t thrash! If you\u2019re not sure what to do next, clicking everywhere won\u2019t help. Think about what you know already, ask yourself what you need to know, collect your thoughts, and piece them together. Think of related items or places you\u2019ve seen, think of information you\u2019ve been given, pay close attention to everything you see, and don\u2019t forget anything. But most importantly - think of what you would do if you were really there. Remember, there is always [Google] if you need it...","title":"The Myst User's Manual"},{"location":"Science/Cognitive-Science/Problem-Solving/#working-through-the-stack","text":"There are varying levels of analysis when you encounter a problem (i.e., OSI levels, dependencies). When troubleshooting it makes sense to treat each of these levels separately, first determining at what level the problem is occurring. So for instance, in a setup like the following: haproxy nginx Apache Application server If a 503 server error is being propagated, you would first want to isolate which layer was causing the 503. You could accomplish this by perusing the logs at each level and comparing them.","title":"Working through the Stack"},{"location":"Science/Cognitive-Science/Problem-Solving/#operators","text":"Increasing log verbosity- this should be a first step in troubleshooting/debugging.","title":"Operators"},{"location":"Science/Cognitive-Science/Problem-Solving/#references","text":"John R. Anderson - Cognitive Psychology and its implications http://cognitivepsychology.wikidot.com/cognition:problem-solving","title":"References"},{"location":"Science/Cognitive-Science/Legacy/Neuroscience-Notes/","text":"Neuroscience Notes fMRI Methods Notes Pitch (like baseball, lurch forward) - sagittal motion; Roll (roy --> king) - coronal motion; Yaw (aw --> axial) -- horizontal motion Elaboration: pitch - 'yes' nod (rotation around x-axis); roll - 'no' nod(rotation around z-axis); yaw - 'maybe' nod (rotation around y-axis) Partial voluming - large voxel sizes bring together distinct elements, water down differences. Small voxels --> higher noise. Large voxels --> less granularity, ability to distinguish; Solution? Blur smaller voxels? Yes. PyMVPA Notes samples - rows in a matrix (e.g., runs, session, participants); 1st axis/index in a 2D array features - columns in a matrix (e.g., voxels); like regressors almost in GLM design matrix; 2nd axis/index in a 2D array sample attributes - categories under which a sample can be binned SPM Notes Website documenting SPM data structures - http://www.its.caltech.edu/~nsulliva/spmdatastructure.htm Andy's Brain Blog documentation of data structures - http://andysbrainblog.blogspot.com/2013/02/using-spmmat-to-stay-on-track-ii.html Information on preprocessing: http://www.ernohermans.com/wp-content/uploads/2011/11/spm8_startersguide.pdf http://www.fil.ion.ucl.ac.uk/~mgray/Presentations/Co-registration%20&%20Spaital%20Normalisation.ppt SPM.xY.P is a character matrix for first-level analyses, a cell array for second level-analyses.","title":"Neuroscience Notes"},{"location":"Science/Cognitive-Science/Legacy/Neuroscience-Notes/#neuroscience-notes","text":"","title":"Neuroscience Notes"},{"location":"Science/Cognitive-Science/Legacy/Neuroscience-Notes/#fmri-methods-notes","text":"Pitch (like baseball, lurch forward) - sagittal motion; Roll (roy --> king) - coronal motion; Yaw (aw --> axial) -- horizontal motion Elaboration: pitch - 'yes' nod (rotation around x-axis); roll - 'no' nod(rotation around z-axis); yaw - 'maybe' nod (rotation around y-axis) Partial voluming - large voxel sizes bring together distinct elements, water down differences. Small voxels --> higher noise. Large voxels --> less granularity, ability to distinguish; Solution? Blur smaller voxels? Yes.","title":"fMRI Methods Notes"},{"location":"Science/Cognitive-Science/Legacy/Neuroscience-Notes/#pymvpa-notes","text":"samples - rows in a matrix (e.g., runs, session, participants); 1st axis/index in a 2D array features - columns in a matrix (e.g., voxels); like regressors almost in GLM design matrix; 2nd axis/index in a 2D array sample attributes - categories under which a sample can be binned","title":"PyMVPA Notes"},{"location":"Science/Cognitive-Science/Legacy/Neuroscience-Notes/#spm-notes","text":"Website documenting SPM data structures - http://www.its.caltech.edu/~nsulliva/spmdatastructure.htm Andy's Brain Blog documentation of data structures - http://andysbrainblog.blogspot.com/2013/02/using-spmmat-to-stay-on-track-ii.html Information on preprocessing: http://www.ernohermans.com/wp-content/uploads/2011/11/spm8_startersguide.pdf http://www.fil.ion.ucl.ac.uk/~mgray/Presentations/Co-registration%20&%20Spaital%20Normalisation.ppt SPM.xY.P is a character matrix for first-level analyses, a cell array for second level-analyses.","title":"SPM Notes"},{"location":"Science/Scientific-Computing/ASICs/","text":"A List of ASICs Used in Scientific Computing I'm using this page to track instances where scientific computing could make use of hardware acceleration (or other advantages) for specific applications. Essentially, most of these technologies represent an instance where \"old is new again\" and the computing paradigms of the 40s-60s (where hardware was specialized and general purpose computing was not particularly commonplace) are back in vogue. Also see here . In general, due to the high expense and low flexibility involved in producing ASICs, any hardware acceleration undertaken in science should leverage existing acceleration that is widely available in the general market first. If that is insufficient, FPGAs can be used, but custom ASIC production is well beyond the scope of most labs (as well as definitely out of budget). Chances are that CPUs and GPUs (which have mostly incorporated the innovations of physics ASICs at this point) are more than sufficient for most common tasks. FPGAs might only come into play for higher precision floats (octs are necessary in astrophysics) or universal numbers. Also, see IT Technology and Markets, Status and Evolution (26 March 2018) for more accelerators / trends. QCDSP and QCDOC (photo here ; also see here DSPs more broadly (also see here Anton Intan (used primarily for data acquisition; less about acceleration and more about miniaturization) Falcon Computing (genomics) General reading on hardware acceleration Storage Acceleration ZFS Hardware Acceleration via QAT AI Acceleration Graphcore Dissecting theGraphcore IPU Architecture via Microbenchmarking Google's TPU An in-depth look at Google\u2019s first Tensor Processing Unit (TPU) Dave Patterson (Lecture): Evaluation of the Tensor Processing Unit Nervana On the use of lower precision numeric types: Making floating point math highly efficient for AI hardware Floating point precision in deep learning Lower Numerical Precision Deep Learning Inference and Training Why do we need floats for using neural networks? Database Acceleration CMU Database Acceleration Seminars https://news.ycombinator.com/item?id=18937101 ASICs Swarm64 Oracle's \"Software in Silicon\" Q100 and DB Mesh ( see here and here too) Via GPUS PGStrom Alenka BlazingSQL https://www.reddit.com/r/hardware/comments/9ld5df/in_a_parallel_universe_data_warehouses_run_on_gpus/ Network Acceleration SmartNICs and eBPF offloading (see here and here too).","title":"ASICs"},{"location":"Science/Scientific-Computing/ASICs/#a-list-of-asics-used-in-scientific-computing","text":"I'm using this page to track instances where scientific computing could make use of hardware acceleration (or other advantages) for specific applications. Essentially, most of these technologies represent an instance where \"old is new again\" and the computing paradigms of the 40s-60s (where hardware was specialized and general purpose computing was not particularly commonplace) are back in vogue. Also see here . In general, due to the high expense and low flexibility involved in producing ASICs, any hardware acceleration undertaken in science should leverage existing acceleration that is widely available in the general market first. If that is insufficient, FPGAs can be used, but custom ASIC production is well beyond the scope of most labs (as well as definitely out of budget). Chances are that CPUs and GPUs (which have mostly incorporated the innovations of physics ASICs at this point) are more than sufficient for most common tasks. FPGAs might only come into play for higher precision floats (octs are necessary in astrophysics) or universal numbers. Also, see IT Technology and Markets, Status and Evolution (26 March 2018) for more accelerators / trends. QCDSP and QCDOC (photo here ; also see here DSPs more broadly (also see here Anton Intan (used primarily for data acquisition; less about acceleration and more about miniaturization) Falcon Computing (genomics) General reading on hardware acceleration","title":"A List of ASICs Used in Scientific Computing"},{"location":"Science/Scientific-Computing/ASICs/#storage-acceleration","text":"ZFS Hardware Acceleration via QAT","title":"Storage Acceleration"},{"location":"Science/Scientific-Computing/ASICs/#ai-acceleration","text":"Graphcore Dissecting theGraphcore IPU Architecture via Microbenchmarking Google's TPU An in-depth look at Google\u2019s first Tensor Processing Unit (TPU) Dave Patterson (Lecture): Evaluation of the Tensor Processing Unit Nervana On the use of lower precision numeric types: Making floating point math highly efficient for AI hardware Floating point precision in deep learning Lower Numerical Precision Deep Learning Inference and Training Why do we need floats for using neural networks?","title":"AI Acceleration"},{"location":"Science/Scientific-Computing/ASICs/#database-acceleration","text":"CMU Database Acceleration Seminars https://news.ycombinator.com/item?id=18937101","title":"Database Acceleration"},{"location":"Science/Scientific-Computing/ASICs/#asics","text":"Swarm64 Oracle's \"Software in Silicon\" Q100 and DB Mesh ( see here and here too)","title":"ASICs"},{"location":"Science/Scientific-Computing/ASICs/#via-gpus","text":"PGStrom Alenka BlazingSQL https://www.reddit.com/r/hardware/comments/9ld5df/in_a_parallel_universe_data_warehouses_run_on_gpus/","title":"Via GPUS"},{"location":"Science/Scientific-Computing/ASICs/#network-acceleration","text":"SmartNICs and eBPF offloading (see here and here too).","title":"Network Acceleration"},{"location":"Science/Scientific-Computing/Architecture-Decision-Tree/","text":"A list of arguments in favor of using a particular architecture/library. Multicomputer/MPI If you're doing 3D simulations (currently still impractical using conventional multicore computers apparently). See here . Next question- why is MPI better suited for these kinds of applications over OpenMP / multithreading on a single host?","title":"Architecture Decision Tree"},{"location":"Science/Scientific-Computing/Architecture-Decision-Tree/#multicomputermpi","text":"If you're doing 3D simulations (currently still impractical using conventional multicore computers apparently). See here . Next question- why is MPI better suited for these kinds of applications over OpenMP / multithreading on a single host?","title":"Multicomputer/MPI"},{"location":"Science/Scientific-Computing/Columbia-Jeff-Dean-20190827/","text":"My Thoughts on Jeff Dean's 8/27/19 Deep Learning Presentation Someone else did a pretty okay summary of this talk already, so I'm just going to add a few conjectures of my own to this page. A lot of the presentation amounted to some form of \"Look at this cool thing we did\". The demonstrations were indeed cool, but also oddly specific. The thoughts below represent my distillation of what mattered the most in that talk to me. AutoML Much as programmers used to produce primarily artisanal assembly code back before compilers (and larger memory/disk) became a thing, the current crop of data scientists produce artisanal machine learning models. Just as a competent assembly programmer can produce hyper-optimized code by hand, an experienced data scientist can, with delicate care, produce a well-tuned model. However, the individuals with this level of expertise are rare. Given a set of constraints, Google's AutoML aims to evolve a machine learning model that is either \"good enough\" or in some cases better than a model that has been hand-tuned by a human. This makes it functionally analogous to a modern compiler, which aims to take high-level specifications and convert them to an optimized form of machine code that is satisfactory or in some cases optimal. Links Autocode Google AutoML AutoML (Wikipedia article about the field as a whole; not Google specific) Google's AutoML: Cutting Through the Hype AutoML from University of Freiburg Neural Architecture Search One challenge with creating machine learning models is that it's hard, long, and costly to get a model trained from scratch. Google (and possibly other folks) proffer the following solution- if there are a number of pre-trained models out there that are close enough to what you need, you can take one of those pre-trained models and use it as a starting point when training for a new model. This is conceptually similar (in my mind at least) to choosing a prior in Bayesian statistics. These pre-trained models can be put together into a searchable \"graph of expertise\". Links Neural Architecture Search (Wikipedia)","title":"Columbia Jeff Dean 20190827"},{"location":"Science/Scientific-Computing/Columbia-Jeff-Dean-20190827/#my-thoughts-on-jeff-deans-82719-deep-learning-presentation","text":"Someone else did a pretty okay summary of this talk already, so I'm just going to add a few conjectures of my own to this page. A lot of the presentation amounted to some form of \"Look at this cool thing we did\". The demonstrations were indeed cool, but also oddly specific. The thoughts below represent my distillation of what mattered the most in that talk to me.","title":"My Thoughts on Jeff Dean's 8/27/19 Deep Learning Presentation"},{"location":"Science/Scientific-Computing/Columbia-Jeff-Dean-20190827/#automl","text":"Much as programmers used to produce primarily artisanal assembly code back before compilers (and larger memory/disk) became a thing, the current crop of data scientists produce artisanal machine learning models. Just as a competent assembly programmer can produce hyper-optimized code by hand, an experienced data scientist can, with delicate care, produce a well-tuned model. However, the individuals with this level of expertise are rare. Given a set of constraints, Google's AutoML aims to evolve a machine learning model that is either \"good enough\" or in some cases better than a model that has been hand-tuned by a human. This makes it functionally analogous to a modern compiler, which aims to take high-level specifications and convert them to an optimized form of machine code that is satisfactory or in some cases optimal.","title":"AutoML"},{"location":"Science/Scientific-Computing/Columbia-Jeff-Dean-20190827/#links","text":"Autocode Google AutoML AutoML (Wikipedia article about the field as a whole; not Google specific) Google's AutoML: Cutting Through the Hype AutoML from University of Freiburg","title":"Links"},{"location":"Science/Scientific-Computing/Columbia-Jeff-Dean-20190827/#neural-architecture-search","text":"One challenge with creating machine learning models is that it's hard, long, and costly to get a model trained from scratch. Google (and possibly other folks) proffer the following solution- if there are a number of pre-trained models out there that are close enough to what you need, you can take one of those pre-trained models and use it as a starting point when training for a new model. This is conceptually similar (in my mind at least) to choosing a prior in Bayesian statistics. These pre-trained models can be put together into a searchable \"graph of expertise\".","title":"Neural Architecture Search"},{"location":"Science/Scientific-Computing/Columbia-Jeff-Dean-20190827/#links_1","text":"Neural Architecture Search (Wikipedia)","title":"Links"},{"location":"Science/Scientific-Computing/MATLAB/","text":"MATLAB I don't particularly like MATLAB. Licensing is always a pain and it seems like there's very little that it can do that SciPy/NumPy/Pandas can't do better at this point. It's great for checking your work if you're taking a linear algebra course though. This page is used to track like-minded opinions, as well as to track methods for weening researchers off of it and onto scientific Python, which currently seems to be the best supported alternative ( Julia seems to suffer from being overly ambitious in its scope, and doesn't seem to have gained widespread adoption). Articles Discussing Issues with MATLAB I Hate Matlab: How an IDE, a Language, and a Mentality Harm Abandon MATLAB ( dead mirror ) Reddit Comment on the MATLAB Compiler Porting MATLAB Code Over https://stackoverflow.com/questions/9845292/a-tool-to-convert-matlab-code-to-python","title":"MATLAB"},{"location":"Science/Scientific-Computing/MATLAB/#matlab","text":"I don't particularly like MATLAB. Licensing is always a pain and it seems like there's very little that it can do that SciPy/NumPy/Pandas can't do better at this point. It's great for checking your work if you're taking a linear algebra course though. This page is used to track like-minded opinions, as well as to track methods for weening researchers off of it and onto scientific Python, which currently seems to be the best supported alternative ( Julia seems to suffer from being overly ambitious in its scope, and doesn't seem to have gained widespread adoption).","title":"MATLAB"},{"location":"Science/Scientific-Computing/MATLAB/#articles-discussing-issues-with-matlab","text":"I Hate Matlab: How an IDE, a Language, and a Mentality Harm Abandon MATLAB ( dead mirror ) Reddit Comment on the MATLAB Compiler","title":"Articles Discussing Issues with MATLAB"},{"location":"Science/Scientific-Computing/MATLAB/#porting-matlab-code-over","text":"https://stackoverflow.com/questions/9845292/a-tool-to-convert-matlab-code-to-python","title":"Porting MATLAB Code Over"},{"location":"Science/Scientific-Computing/Machine-Learning/","text":"Notes on neural networks / machine learning. On the Use of Machine Learning / Neural Networks to Perform Scheduling Machine Learning for SystemsandSystems for Machine Learning Implementing a Process Scheduler Using Neural Network Technology A neural network job-shop scheduler reddit: What are the best papers at the intersection of operating systems and machine learning research? Links Neural Networks by 3Blue1Brown Intel's AI Courses A Full Hardware Guide to Deep Learning Fully Utilizing Your Deep Learning GPUs","title":"Machine Learning"},{"location":"Science/Scientific-Computing/Machine-Learning/#on-the-use-of-machine-learning-neural-networks-to-perform-scheduling","text":"Machine Learning for SystemsandSystems for Machine Learning Implementing a Process Scheduler Using Neural Network Technology A neural network job-shop scheduler reddit: What are the best papers at the intersection of operating systems and machine learning research?","title":"On the Use of Machine Learning / Neural Networks to Perform Scheduling"},{"location":"Science/Scientific-Computing/Machine-Learning/#links","text":"Neural Networks by 3Blue1Brown Intel's AI Courses A Full Hardware Guide to Deep Learning Fully Utilizing Your Deep Learning GPUs","title":"Links"},{"location":"Science/Scientific-Computing/Research-Security/","text":"Notes on Security Measures Needed for Scientific Research Level What needs to be secured Examples HIPAA-regulated Data PHI must be secured; patient confidentiality must be assured. Information about a diagnosis. Many many other pieces of data are considered PHI as well. IRB-regulated Data Subject confidentiality must be assured. Results of a survey administered by a grad student. Mechanical Turk data. Unregulated Data Significant harm to another will not occur if there is a breach. Astronomy data. Physics data. Anything that is not human subject research.","title":"Research Security"},{"location":"Science/Scientific-Computing/Research-Security/#notes-on-security-measures-needed-for-scientific-research","text":"Level What needs to be secured Examples HIPAA-regulated Data PHI must be secured; patient confidentiality must be assured. Information about a diagnosis. Many many other pieces of data are considered PHI as well. IRB-regulated Data Subject confidentiality must be assured. Results of a survey administered by a grad student. Mechanical Turk data. Unregulated Data Significant harm to another will not occur if there is a breach. Astronomy data. Physics data. Anything that is not human subject research.","title":"Notes on Security Measures Needed for Scientific Research"},{"location":"Science/Scientific-Computing/loni2cwl/","text":"This will be a table mapping LONI Pipeline Engine XML tags to Common Workflow Language syntax. LONI XML Tag Arguments/Attributes Description Equivalent Command Line Tool CWL Syntax pipeline Tag used to declare pipeline version A version number for the pipeline. cwlVersion moduleGroup A collection of modules; can act as a single node in a workflow. module A single executable in a pipeline. name Name of the executable. description A description of the executable. location Path to the executable. baseCommand package Package the executable belongs to. version Version of the package the executable belongs to. executableVersion Version of the executable that the module wraps. executableAuthors Authors of the executable that the XML module definition wraps. author Author name. Nested within ''executableAuthors'' and ''authors'' fullName Full name. email Author e-mail. website Author website. authors Authors of XML module definition for LONI Pipeline citations A grouping of multiple citation tags. citation Unnamed string containing citation text. Article citation. Nested within ''citations'' tag Tag used to identify the module using LONI Pipeline's search functionality. input/output An input or output parameter inputs/outputs name A name for the input parameter. description A description for the input parameter. enabled Whether or not a particular parameter is enabled for usage as part of the module. required Whether or not the input parameter is required for the module to execute. order The order in which a parameter should appear on the command line (0-indexed). switch The switch used on the command line (e.g., ''-input'') switchSpaced Should there be a space between a switch and its arguments? format Defines the nature of the data accepted/produced by the parameter. format type Type of data produced/accepted by parameter. Choose from the following options: File , Directory , String , Number , Enumerated . If you choose File , ''filetype'' must be defined within ''format''. cardinality How many arguments should be entered after the switch. -1 is infinite (unavailable for output) -2 is the number of arguments passed to the input (unavailable for input) base A base string to be transformed. filetypes A grouping of file types accepted by the input/output. filetype A file type accepted by the input/output. name The human readable name of the file type. extension The extension of the filetype (no \u2018.\u2019 required) description A short description of the file type need A nested element within the ''filetype'' tag that contains the extension of any file needed by this type. You can have multiple ''need'' tags within a ''filetype''. transform Allows you to define an output file name using string operations and a base string. order An index value describing at which point the transform should be executed.(?) op subtract , prepend , append , replace See: http://pipeline.bmap.ucla.edu/learn/xml-overview/ http://pipeline.bmap.ucla.edu/learn/glossary/","title":"Loni2cwl"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Cluster/","text":"Notes on the Architecture of Neuroinformatics Clusters Reference Implementations HCP/XNAT Architecture (Washington University at St. Louis) HCP/XNAT Architecture (University of Iowa) SciTran Overview LONI Oregon Neuroscience Grid Electrical Geodesics MRC CBU (Part 1) MRC CBU (Part 2) NIH DICOM Servers/SCPs Orthanc XNAT dcm4chee DCMTK Remote Access Options X2Go VNC Use VMs, store vmdk on NAS; run VM on workstation (i.e., WashU's setup). Misc LIMS Wiki List of spike sorting software","title":"Neuroinformatics Cluster"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Cluster/#notes-on-the-architecture-of-neuroinformatics-clusters","text":"","title":"Notes on the Architecture of Neuroinformatics Clusters"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Cluster/#reference-implementations","text":"HCP/XNAT Architecture (Washington University at St. Louis) HCP/XNAT Architecture (University of Iowa) SciTran Overview LONI Oregon Neuroscience Grid Electrical Geodesics MRC CBU (Part 1) MRC CBU (Part 2) NIH","title":"Reference Implementations"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Cluster/#dicom-serversscps","text":"Orthanc XNAT dcm4chee DCMTK","title":"DICOM Servers/SCPs"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Cluster/#remote-access-options","text":"X2Go VNC Use VMs, store vmdk on NAS; run VM on workstation (i.e., WashU's setup).","title":"Remote Access Options"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Cluster/#misc","text":"LIMS Wiki List of spike sorting software","title":"Misc"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/","text":"List of Neuroinformatics Tools w/ GPU-based Implementations General Page on Medical Imaging Using Nvidia GPUs ITK page on GPU Acceleration (possibly useful for making ANTS go faster) recon-all and mrisurf (Freesurfer) ( paper ). BROCCOLI GLIRT ( article ) Another GPU-based implementation of FLIRT GPU-Accelerated FLIRT and ANTS FSL's bedpost-x CEREBRuM Google FFN and here CLIJ (an ImageJ plugin that maps macro calls to OpenCL calls) http://on-demand.gputechconf.com/gtc/2017/presentation/S7342-robert-zigon-data-mining-in%20neuroimagic-%20genomics.pdf How exactly do GPUs work anyways? How a GPU Works by Kayvon Fatahalian ( course website GPGPU Lectures at Haifa LUG by Ofer Rosenberg Summary in the article about GPU-enabled FSL bedpost-x CUDA Streams Best Practices Programming GPUs with SYCL Debugging GPUS Debugging CUDA device-side assert in PyTorch fast.ai's Troubleshooting Tips Discussions AFNI https://afni.nimh.nih.gov/afni/community/board/read.php?1,143063,143063 https://afni.nimh.nih.gov/afni/community/board/read.php?1,71356,71365 https://afni.nimh.nih.gov/afni/community/board/read.php?1,67402,73081 Exciting News for AFNI https://www.openmp.org/updates/openmp-accelerator-support-gpus/ http://on-demand.gputechconf.com/gtc/2018/presentation/s8344-openmp-on-gpus-first-experiences-and-best-practices.pdf ANTS https://sourceforge.net/p/advants/discussion/840260/thread/260e1e38/ https://sourceforge.net/p/advants/discussion/840260/thread/4b134259/ FSL http://godzilla.kennedykrieger.org/penguin/fsl.shtml ImageJ https://forum.image.sc/t/does-imagej-utilize-the-gpu/10333 ITK https://discourse.itk.org/t/status-of-itk-gpu/277/8 https://itk.org/Wiki/ITK/Release_4/GPU_Acceleration Misc The case for consumer GPUs over enterprise GPUs: https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/ https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/ CryoEM https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9664-how-to-scale-from-workstation-through-cloud-to-hpc-in-cryo-em-processing.pdf http://on-demand.gputechconf.com/gtc/2017/presentation/s7232-lance-wilson-processing-the-next.pdf","title":"Neuroinformatics GPU"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#list-of-neuroinformatics-tools-w-gpu-based-implementations","text":"General Page on Medical Imaging Using Nvidia GPUs ITK page on GPU Acceleration (possibly useful for making ANTS go faster) recon-all and mrisurf (Freesurfer) ( paper ). BROCCOLI GLIRT ( article ) Another GPU-based implementation of FLIRT GPU-Accelerated FLIRT and ANTS FSL's bedpost-x CEREBRuM Google FFN and here CLIJ (an ImageJ plugin that maps macro calls to OpenCL calls) http://on-demand.gputechconf.com/gtc/2017/presentation/S7342-robert-zigon-data-mining-in%20neuroimagic-%20genomics.pdf","title":"List of Neuroinformatics Tools w/ GPU-based Implementations"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#how-exactly-do-gpus-work-anyways","text":"How a GPU Works by Kayvon Fatahalian ( course website GPGPU Lectures at Haifa LUG by Ofer Rosenberg Summary in the article about GPU-enabled FSL bedpost-x CUDA Streams Best Practices Programming GPUs with SYCL","title":"How exactly do GPUs work anyways?"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#debugging-gpus","text":"Debugging CUDA device-side assert in PyTorch fast.ai's Troubleshooting Tips","title":"Debugging GPUS"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#discussions","text":"","title":"Discussions"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#afni","text":"https://afni.nimh.nih.gov/afni/community/board/read.php?1,143063,143063 https://afni.nimh.nih.gov/afni/community/board/read.php?1,71356,71365 https://afni.nimh.nih.gov/afni/community/board/read.php?1,67402,73081","title":"AFNI"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#exciting-news-for-afni","text":"https://www.openmp.org/updates/openmp-accelerator-support-gpus/ http://on-demand.gputechconf.com/gtc/2018/presentation/s8344-openmp-on-gpus-first-experiences-and-best-practices.pdf","title":"Exciting News for AFNI"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#ants","text":"https://sourceforge.net/p/advants/discussion/840260/thread/260e1e38/ https://sourceforge.net/p/advants/discussion/840260/thread/4b134259/","title":"ANTS"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#fsl","text":"http://godzilla.kennedykrieger.org/penguin/fsl.shtml","title":"FSL"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#imagej","text":"https://forum.image.sc/t/does-imagej-utilize-the-gpu/10333","title":"ImageJ"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#itk","text":"https://discourse.itk.org/t/status-of-itk-gpu/277/8 https://itk.org/Wiki/ITK/Release_4/GPU_Acceleration","title":"ITK"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#misc","text":"The case for consumer GPUs over enterprise GPUs: https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/ https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/","title":"Misc"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-GPU/#cryoem","text":"https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9664-how-to-scale-from-workstation-through-cloud-to-hpc-in-cryo-em-processing.pdf http://on-demand.gputechconf.com/gtc/2017/presentation/s7232-lance-wilson-processing-the-next.pdf","title":"CryoEM"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/","text":"Neuroinformatics Pipelines A page to store links to documentation for common bits of software, and take notes on steps that overlap. I think that, by and large, the majority of neuroscience problems fall into three categories: image manipulation, quantifying behavioral observations, manipulating physiological observations, and stimulus presentation. Image Manipulation These utilities ultimately should amount to specialized versions of Photoshop/the GIMP that operate in 3D space rather than 2D. Some of them include some statistical processing tools as well. Essentially these then become hybridized versions of Photoshop and R/Stata/SPSS/SAS. RELION SPM12 AFNI ANTS BROCCOLI Brainstorm BrainVoyager ITK BEaST (also here ) Imaging Software Installed by the NIH MNI Tools Neurolucida As per earlier statements, compare these manuals to: The GIMP Adobe Photoshop Other Image Processing Libraries Open Source Ilastik OpenCV scikitiimage vips Tomopy netpbm (AFNI dependency) ImageMagick Proprietary Huygens Amira and Avizo Aphelion Imaris Additional Links An Inventory of Image Processing Algorithms Wikipedia's list of image processing algorithms Quantifying Behavioral Data DeepLabCut Sentiment Analysis TextBlob CoreNLP NLTK Eye-Tracking PyGaze WebGazer eyetrackingR saccades Manipulating Physiological Data Bioread (for BIOPAC) Stimulus Presentation Stimulus presentation packages, by their very nature, also quantify behavioral data into reaction times and specific responses. DirectRT ePrime PsychoPy jsPsych PsyScope","title":"Neuroinformatics Pipelines"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#neuroinformatics-pipelines","text":"A page to store links to documentation for common bits of software, and take notes on steps that overlap. I think that, by and large, the majority of neuroscience problems fall into three categories: image manipulation, quantifying behavioral observations, manipulating physiological observations, and stimulus presentation.","title":"Neuroinformatics Pipelines"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#image-manipulation","text":"These utilities ultimately should amount to specialized versions of Photoshop/the GIMP that operate in 3D space rather than 2D. Some of them include some statistical processing tools as well. Essentially these then become hybridized versions of Photoshop and R/Stata/SPSS/SAS. RELION SPM12 AFNI ANTS BROCCOLI Brainstorm BrainVoyager ITK BEaST (also here ) Imaging Software Installed by the NIH MNI Tools Neurolucida As per earlier statements, compare these manuals to: The GIMP Adobe Photoshop","title":"Image Manipulation"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#other-image-processing-libraries","text":"","title":"Other Image Processing Libraries"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#open-source","text":"Ilastik OpenCV scikitiimage vips Tomopy netpbm (AFNI dependency) ImageMagick","title":"Open Source"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#proprietary","text":"Huygens Amira and Avizo Aphelion Imaris","title":"Proprietary"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#additional-links","text":"An Inventory of Image Processing Algorithms Wikipedia's list of image processing algorithms","title":"Additional Links"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#quantifying-behavioral-data","text":"DeepLabCut","title":"Quantifying Behavioral Data"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#sentiment-analysis","text":"TextBlob CoreNLP NLTK","title":"Sentiment Analysis"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#eye-tracking","text":"PyGaze WebGazer eyetrackingR saccades","title":"Eye-Tracking"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#manipulating-physiological-data","text":"Bioread (for BIOPAC)","title":"Manipulating Physiological Data"},{"location":"Science/Scientific-Computing/Neuroinformatics/Neuroinformatics-Pipelines/#stimulus-presentation","text":"Stimulus presentation packages, by their very nature, also quantify behavioral data into reaction times and specific responses. DirectRT ePrime PsychoPy jsPsych PsyScope","title":"Stimulus Presentation"},{"location":"Software-Development/Data-Structures-In-The-Wild/","text":"Data Structures in the Wild Various sightings of data structure implementations being used in actual applications. Hash Table AWS S3 bucket - choosing random names for prefixes is better because of the primary clustering problem Stack pushd and popd","title":"Data Structures In The Wild"},{"location":"Software-Development/Data-Structures-In-The-Wild/#data-structures-in-the-wild","text":"Various sightings of data structure implementations being used in actual applications.","title":"Data Structures in the Wild"},{"location":"Software-Development/Data-Structures-In-The-Wild/#hash-table","text":"AWS S3 bucket - choosing random names for prefixes is better because of the primary clustering problem","title":"Hash Table"},{"location":"Software-Development/Data-Structures-In-The-Wild/#stack","text":"pushd and popd","title":"Stack"},{"location":"Software-Development/Demystifying-Old-Python-Scripts/","text":"Demystifying Old Python Scripts Sometimes, fair adventurer, you may come across a Python script with sparse documentation. In that case, you should avail yourself of the following tools: pyan - Creates a call dependency graph based off a static analysis of the script. Does not run code. pycallgraph - Actually needs to run the script (depends on the debugger). Creates a call graph using Gephi or Graphviz. Next, if there are no docstrings for arguments, make them. Remove unused functions / code. Familiarize yourself with APIs / modules used within the script. Refactor if necessary.","title":"Demystifying Old Python Scripts"},{"location":"Software-Development/Demystifying-Old-Python-Scripts/#demystifying-old-python-scripts","text":"Sometimes, fair adventurer, you may come across a Python script with sparse documentation. In that case, you should avail yourself of the following tools: pyan - Creates a call dependency graph based off a static analysis of the script. Does not run code. pycallgraph - Actually needs to run the script (depends on the debugger). Creates a call graph using Gephi or Graphviz. Next, if there are no docstrings for arguments, make them. Remove unused functions / code. Familiarize yourself with APIs / modules used within the script. Refactor if necessary.","title":"Demystifying Old Python Scripts"},{"location":"Software-Development/Open-Source/","text":"Open Source http://www.fordfoundation.org/library/reports-and-studies/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure/ http://www.dr-chuck.com/sakai-book/ http://www.dr-chuck.com/csev-blog/2014/09/how-to-achieve-vendor-lock-in-with-a-legit-open-source-license-affero-gpl/","title":"Open Source"},{"location":"Software-Development/Open-Source/#open-source","text":"http://www.fordfoundation.org/library/reports-and-studies/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure/ http://www.dr-chuck.com/sakai-book/ http://www.dr-chuck.com/csev-blog/2014/09/how-to-achieve-vendor-lock-in-with-a-legit-open-source-license-affero-gpl/","title":"Open Source"},{"location":"Software-Development/Packaging/","text":"Notes on Software Packaging I'm using this page to keep notes on the wonderful world of software packaging / distribution. Tools rpmbuild : Tool used to create RPMs, taking into account dependency resolution. alien : Allows you to convert RPMs to Debian packages. Doesn't take into account dependencies. CPack : Allows you to create multiple package formats from the same source code. Python's distutils can be used to create RPM packages. (see here too) stdeb can also be used with distutils to create Debian packages (see here too). PyInstaller . See issue with wx here Spec file Formats dsc RPM spec Further Reading Env and Stacks/Projects/UserLevelPackageManagement - Fedora Project Wiki","title":"Packaging"},{"location":"Software-Development/Packaging/#notes-on-software-packaging","text":"I'm using this page to keep notes on the wonderful world of software packaging / distribution.","title":"Notes on Software Packaging"},{"location":"Software-Development/Packaging/#tools","text":"rpmbuild : Tool used to create RPMs, taking into account dependency resolution. alien : Allows you to convert RPMs to Debian packages. Doesn't take into account dependencies. CPack : Allows you to create multiple package formats from the same source code. Python's distutils can be used to create RPM packages. (see here too) stdeb can also be used with distutils to create Debian packages (see here too). PyInstaller . See issue with wx here","title":"Tools"},{"location":"Software-Development/Packaging/#spec-file-formats","text":"dsc RPM spec","title":"Spec file Formats"},{"location":"Software-Development/Packaging/#further-reading","text":"Env and Stacks/Projects/UserLevelPackageManagement - Fedora Project Wiki","title":"Further Reading"},{"location":"Software-Development/Python-OS-Ops/","text":"Common Unix Command Line Operations and Their Python Equivalents Learning by analogy. Unix Command Line Operation Python Operation pwd https://docs.python.org/2/library/os.html#os.getcwd cd https://docs.python.org/2/library/os.html#os.chdir mkdir https://docs.python.org/2/library/os.html#os.mkdir mkdir -p https://docs.python.org/2/library/os.html#os.makedirs rm https://docs.python.org/2/library/os.html#os.remove rmdir https://docs.python.org/2/library/os.html#os.rmdir rm -r https://docs.python.org/2/library/shutil.html#shutil.rmtree mv https://docs.python.org/2/library/os.html#os.rename ln https://docs.python.org/2/library/os.html#os.link ln -s https://docs.python.org/2/library/os.html#os.symlink echo $VAR https://docs.python.org/2/library/os.html#os.getenv export VAR=foo https://docs.python.org/2/library/os.html#os.putenv ls https://docs.python.org/2/library/subprocess.html#subprocess.check_output ls -l https://docs.python.org/2/library/subprocess.html#subprocess.check_output chmod https://docs.python.org/2/library/os.html#os.chmod cp https://docs.python.org/2/library/shutil.html#shutil.copy cp -p https://docs.python.org/2/library/shutil.html#shutil.copy2 cp -r https://docs.python.org/2/library/shutil.html#shutil.copytree date datetime ps, top psutil Generally useful: shlex.split","title":"Python OS Ops"},{"location":"Software-Development/Python-OS-Ops/#common-unix-command-line-operations-and-their-python-equivalents","text":"Learning by analogy. Unix Command Line Operation Python Operation pwd https://docs.python.org/2/library/os.html#os.getcwd cd https://docs.python.org/2/library/os.html#os.chdir mkdir https://docs.python.org/2/library/os.html#os.mkdir mkdir -p https://docs.python.org/2/library/os.html#os.makedirs rm https://docs.python.org/2/library/os.html#os.remove rmdir https://docs.python.org/2/library/os.html#os.rmdir rm -r https://docs.python.org/2/library/shutil.html#shutil.rmtree mv https://docs.python.org/2/library/os.html#os.rename ln https://docs.python.org/2/library/os.html#os.link ln -s https://docs.python.org/2/library/os.html#os.symlink echo $VAR https://docs.python.org/2/library/os.html#os.getenv export VAR=foo https://docs.python.org/2/library/os.html#os.putenv ls https://docs.python.org/2/library/subprocess.html#subprocess.check_output ls -l https://docs.python.org/2/library/subprocess.html#subprocess.check_output chmod https://docs.python.org/2/library/os.html#os.chmod cp https://docs.python.org/2/library/shutil.html#shutil.copy cp -p https://docs.python.org/2/library/shutil.html#shutil.copy2 cp -r https://docs.python.org/2/library/shutil.html#shutil.copytree date datetime ps, top psutil Generally useful: shlex.split","title":"Common Unix Command Line Operations and Their Python Equivalents"},{"location":"Software-Development/Python/","text":"Python Various links to articles and notes regarding Python. Various Python-Related Readings The Global Interpreter Lock - Python's Hardest Problem Illustrating Python multithreading vs multiprocessing How Python Finds Packages Useful Packages Natural Sort","title":"Python"},{"location":"Software-Development/Python/#python","text":"Various links to articles and notes regarding Python.","title":"Python"},{"location":"Software-Development/Python/#various-python-related-readings","text":"The Global Interpreter Lock - Python's Hardest Problem Illustrating Python multithreading vs multiprocessing How Python Finds Packages","title":"Various Python-Related Readings"},{"location":"Software-Development/Python/#useful-packages","text":"Natural Sort","title":"Useful Packages"},{"location":"Systems-Administration/","text":"Systems Administration Notes This page stores both generic notes, as well as additional pages and categories related to systems administration. My general modus operandi here is to start taking notes here, and then to break things out into separate pages when they get too large. General Links Documentation Projects http://www.linfo.org/ http://www.tldp.org/ Blogs / Sysadmin Sites http://www.kegel.com/ http://www.rodsbooks.com/ http://grymoire.com/ https://daniel.haxx.se/ http://everythingsysadmin.com/ https://www.kennethreitz.org http://www.guppylake.com/~nsb/ https://brendangregg.com - Emphasis on monitoring https://pthree.org/ - Emphasis on storage / ZFS https://sysadmincasts.com Bootloaders UEFI Boot - How Does that Actually Work Then? Fast Boot as an issue with bootable thumb drives Scripting Languages Python Bash Explainshell Exec last command in bash !! . Variable expansion doesn't work with watch (8/10/19 - I'm not sure I believe this- I might just have been doing something with single quotes instead of double quotes) The -c flag for du caches file size estimates so that they can be retrieve more quickly on future invocations? ( More reading in addition to the man file) Type 'reset' when screen messes up your keyboard mapping. uniq -c : 'prefix lines by the number of occurrences' http://wiki.bash-hackers.org/howto/redirection_tutorial http://sebug.net/paper/os/linux/Linux%20Shell%20Scripting%20Tutorial%20v2.0.pdf http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-3.html http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-4.html Common Bash Pitfalls Writing Robust Shell Scripts CommandlineFu Good summary of redirection / control operators Where I learned that Here strings need to have quotes around the word to be interpreted as string literals Regular Expressions RegEx Golf Regular Expressions by Jan Goyvaerts Monitoring / Debugging Understanding Load Averages Strace: What a Process Does Practical strace Memory Linux Ate My RAM Apparently the Java heap makes use of the RAM allocated for buffer/cache (so the buffer/cache isn't freed up). Article on JVM Heap Size & Oracle Docs on JVM Heap Memory Subsystem Deep Dive Networking Succinct overview of iptables QUIC at 10,000 feet netstat overview Cheat Sheet for IP command Application-Layer Protocols HTTP Keep Alive Client https://www.w3.org/History/19921103-hypertext/hypertext/WWW/Protocols/HTTP.html https://www.ntu.edu.sg/home/ehchua/programming/webprogramming/HTTP_Basics.html https://daniel.haxx.se/docs/ftp-vs-http.html What inspired my interest in this topic TCPDump Tutorials http://www.alexonlinux.com/tcpdump-for-dummies http://bencane.com/2014/10/13/quick-and-practical-reference-for-tcpdump/ https://www.quora.com/What-is-the-difference-between-TCPs-FIN-and-RST-packets Security Strong Ciphers for Web Servers SSL Labs (assesses your site's security) Is TLS fast yet? TLS Overview (chapter of an O'Reilly book) CAA (combines SSL/TLS certificate file w/ a DNS record to increase security) GPG Quickstart Creating GPG Keys Using the CLI Backup Encryption Inventing the Sudo Command XKCD Password Generator Another XKCD Password Generator Dangerous Sudoers Entries Stop Disabling SELinux Explain Like I'm 5: Kerberos Storage Why NFS Sucks How to improve ZFS performance ZFS RAID Speed Capacity How I learned to stop worrying and love RAIDZ Lustre and Panasas Are Not So Different Backblaze Hard Drive Reliability Stats, Q1 2016 NDMP (Description and whitepaper) http://www.tldp.org/LDP/intro-linux/html/sect_03_01.html Does Writing to NFS Put Processes into Uninterruptible Sleep? Create LUKS Access xfs quota info from NFS client rpc.rquotad Using Linux quota command pointing to an /ifs nfs mounted filesystem. ACLs En Francais https://wiki.archlinux.org/index.php/Access_Control_Lists https://www.freebsd.org/doc/handbook/fs-acl.html RAID-5 A list of pages discussing why not to use RAID5: https://news.ycombinator.com/item?id=8306499 https://www.reddit.com/r/sysadmin/comments/ydi6i/dell_raid_5_is_no_longer_recommended_for_any/ https://www.reddit.com/r/sysadmin/comments/3yoc9z/raid_5raid_10_tradeoff/ (in which we learn that RAID5 works for SSDs) An article on the RAID \"write-hole\", which seems to be especially salient for RAID5: http://www.raid-recovery-guide.com/raid5-write-hole.aspx Tape tar tvf \\<device_name> - Read the file name from the tar header for the current file that the tape is pointed at. Database vs Filesystem https://stackoverflow.com/questions/38120895/database-vs-file-system-storage https://softwareengineering.stackexchange.com/questions/190482/why-use-a-database-instead-of-just-saving-your-data-to-disk https://dzone.com/articles/which-is-better-saving-files-in-database-or-in-fil AI / Neural Network / Deep Learning Workloads Why NFS Performance Won\u2019t Cut it For AI and Machine Learning Why Network File System (NFS) is not Suitable for AI Workloads? NFS vs Lustre Caching with CacheFS TFRecord Format Details Efficient PyTorch I/O library for Large Datasets, Many Files, Many GPUs (in which NVIDIA uses tars to work around issues with file metadata performance penalty) vmtouch (Simple package to pre-warm RAM with file pages) Identity Management / User Management https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/s1-users-tools.html Introduction to LDAP Applications Web Servers An analogy: web/app servers / load balancers belong to the same sub-class of problems that HPC schedulers treat, but are just more narrow in scope. A 301 redirect in nginx for HTTPS requires a cert because the packet needs to be decrypted for nginx to inspect the host field of the packet header. Canned nginx Configs (to use as templates) Databases http://philip.greenspun.com/sql/ What an in-memory database is and how it persists data efficiently What are pros and cons of PostgreSQL and MySQL? With respect to reliability, speed, scalability, and features. Virtualization Apparently KVM and Virtualbox are incompatible / can't be run simultaneously. See here for an idea on how to handle that (or just don't do that at all because it doesn't make too much sense to begin with- quoth the older and wiser me). Xen Networking Importing an OVA into KVM Containerization Kubernetes: An Overview Docker for Data Science Docker Security Cheat Sheet Cloud Computing If an AWS S3 upload is MultiPart , the ETag attribute of an S3 bucket object is not an MD5 hash. It is the hashes for each part uploaded concatenated, plus a dash and the number of parts uploaded (see here ). S3-compatible object stores https://minio.io/ https://cloudian.com/ https://wasabi.com/ http://pithos.io/ https://www.zenko.io/ https://leo-project.net/leofs/ https://github.com/eucalyptus/eucalyptus/wiki/Walrus-S3-API http://docs.ceph.com/docs/master/radosgw/s3/ Windows/Linux Compatibility Debian9ADSharedDisks_Sssd_PamMount Pam_mount RHEL Guide for multi-user SMB mount RHEL Windows Integration Guide Tools Atop Gas Hosts last (can show reboot times) lastlog (can show last login for a user- with decently informative timestamp) https://mxtoolbox.com/SuperTool.aspx https://peteris.rocks/blog/htop/ http://md5deep.sourceforge.net/ GNU Parallel","title":"Index"},{"location":"Systems-Administration/#systems-administration-notes","text":"This page stores both generic notes, as well as additional pages and categories related to systems administration. My general modus operandi here is to start taking notes here, and then to break things out into separate pages when they get too large.","title":"Systems Administration Notes"},{"location":"Systems-Administration/#general-links","text":"","title":"General Links"},{"location":"Systems-Administration/#documentation-projects","text":"http://www.linfo.org/ http://www.tldp.org/","title":"Documentation Projects"},{"location":"Systems-Administration/#blogs-sysadmin-sites","text":"http://www.kegel.com/ http://www.rodsbooks.com/ http://grymoire.com/ https://daniel.haxx.se/ http://everythingsysadmin.com/ https://www.kennethreitz.org http://www.guppylake.com/~nsb/ https://brendangregg.com - Emphasis on monitoring https://pthree.org/ - Emphasis on storage / ZFS https://sysadmincasts.com","title":"Blogs / Sysadmin Sites"},{"location":"Systems-Administration/#bootloaders","text":"UEFI Boot - How Does that Actually Work Then? Fast Boot as an issue with bootable thumb drives","title":"Bootloaders"},{"location":"Systems-Administration/#scripting-languages","text":"Python","title":"Scripting Languages"},{"location":"Systems-Administration/#bash","text":"Explainshell Exec last command in bash !! . Variable expansion doesn't work with watch (8/10/19 - I'm not sure I believe this- I might just have been doing something with single quotes instead of double quotes) The -c flag for du caches file size estimates so that they can be retrieve more quickly on future invocations? ( More reading in addition to the man file) Type 'reset' when screen messes up your keyboard mapping. uniq -c : 'prefix lines by the number of occurrences' http://wiki.bash-hackers.org/howto/redirection_tutorial http://sebug.net/paper/os/linux/Linux%20Shell%20Scripting%20Tutorial%20v2.0.pdf http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-3.html http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-4.html Common Bash Pitfalls Writing Robust Shell Scripts CommandlineFu Good summary of redirection / control operators Where I learned that Here strings need to have quotes around the word to be interpreted as string literals","title":"Bash"},{"location":"Systems-Administration/#regular-expressions","text":"RegEx Golf Regular Expressions by Jan Goyvaerts","title":"Regular Expressions"},{"location":"Systems-Administration/#monitoring-debugging","text":"Understanding Load Averages Strace: What a Process Does Practical strace","title":"Monitoring / Debugging"},{"location":"Systems-Administration/#memory","text":"Linux Ate My RAM Apparently the Java heap makes use of the RAM allocated for buffer/cache (so the buffer/cache isn't freed up). Article on JVM Heap Size & Oracle Docs on JVM Heap Memory Subsystem Deep Dive","title":"Memory"},{"location":"Systems-Administration/#networking","text":"Succinct overview of iptables QUIC at 10,000 feet netstat overview Cheat Sheet for IP command","title":"Networking"},{"location":"Systems-Administration/#application-layer-protocols","text":"","title":"Application-Layer Protocols"},{"location":"Systems-Administration/#http","text":"Keep Alive Client https://www.w3.org/History/19921103-hypertext/hypertext/WWW/Protocols/HTTP.html https://www.ntu.edu.sg/home/ehchua/programming/webprogramming/HTTP_Basics.html https://daniel.haxx.se/docs/ftp-vs-http.html What inspired my interest in this topic","title":"HTTP"},{"location":"Systems-Administration/#tcpdump-tutorials","text":"http://www.alexonlinux.com/tcpdump-for-dummies http://bencane.com/2014/10/13/quick-and-practical-reference-for-tcpdump/ https://www.quora.com/What-is-the-difference-between-TCPs-FIN-and-RST-packets","title":"TCPDump Tutorials"},{"location":"Systems-Administration/#security","text":"Strong Ciphers for Web Servers SSL Labs (assesses your site's security) Is TLS fast yet? TLS Overview (chapter of an O'Reilly book) CAA (combines SSL/TLS certificate file w/ a DNS record to increase security) GPG Quickstart Creating GPG Keys Using the CLI Backup Encryption Inventing the Sudo Command XKCD Password Generator Another XKCD Password Generator Dangerous Sudoers Entries Stop Disabling SELinux Explain Like I'm 5: Kerberos","title":"Security"},{"location":"Systems-Administration/#storage","text":"Why NFS Sucks How to improve ZFS performance ZFS RAID Speed Capacity How I learned to stop worrying and love RAIDZ Lustre and Panasas Are Not So Different Backblaze Hard Drive Reliability Stats, Q1 2016 NDMP (Description and whitepaper) http://www.tldp.org/LDP/intro-linux/html/sect_03_01.html Does Writing to NFS Put Processes into Uninterruptible Sleep? Create LUKS Access xfs quota info from NFS client rpc.rquotad Using Linux quota command pointing to an /ifs nfs mounted filesystem.","title":"Storage"},{"location":"Systems-Administration/#acls","text":"En Francais https://wiki.archlinux.org/index.php/Access_Control_Lists https://www.freebsd.org/doc/handbook/fs-acl.html","title":"ACLs"},{"location":"Systems-Administration/#raid-5","text":"A list of pages discussing why not to use RAID5: https://news.ycombinator.com/item?id=8306499 https://www.reddit.com/r/sysadmin/comments/ydi6i/dell_raid_5_is_no_longer_recommended_for_any/ https://www.reddit.com/r/sysadmin/comments/3yoc9z/raid_5raid_10_tradeoff/ (in which we learn that RAID5 works for SSDs) An article on the RAID \"write-hole\", which seems to be especially salient for RAID5: http://www.raid-recovery-guide.com/raid5-write-hole.aspx","title":"RAID-5"},{"location":"Systems-Administration/#tape","text":"tar tvf \\<device_name> - Read the file name from the tar header for the current file that the tape is pointed at.","title":"Tape"},{"location":"Systems-Administration/#database-vs-filesystem","text":"https://stackoverflow.com/questions/38120895/database-vs-file-system-storage https://softwareengineering.stackexchange.com/questions/190482/why-use-a-database-instead-of-just-saving-your-data-to-disk https://dzone.com/articles/which-is-better-saving-files-in-database-or-in-fil","title":"Database vs Filesystem"},{"location":"Systems-Administration/#ai-neural-network-deep-learning-workloads","text":"Why NFS Performance Won\u2019t Cut it For AI and Machine Learning Why Network File System (NFS) is not Suitable for AI Workloads? NFS vs Lustre Caching with CacheFS TFRecord Format Details Efficient PyTorch I/O library for Large Datasets, Many Files, Many GPUs (in which NVIDIA uses tars to work around issues with file metadata performance penalty) vmtouch (Simple package to pre-warm RAM with file pages)","title":"AI / Neural Network / Deep Learning Workloads"},{"location":"Systems-Administration/#identity-management-user-management","text":"https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/System_Administrators_Guide/s1-users-tools.html Introduction to LDAP","title":"Identity Management / User Management"},{"location":"Systems-Administration/#applications","text":"","title":"Applications"},{"location":"Systems-Administration/#web-servers","text":"An analogy: web/app servers / load balancers belong to the same sub-class of problems that HPC schedulers treat, but are just more narrow in scope. A 301 redirect in nginx for HTTPS requires a cert because the packet needs to be decrypted for nginx to inspect the host field of the packet header. Canned nginx Configs (to use as templates)","title":"Web Servers"},{"location":"Systems-Administration/#databases","text":"http://philip.greenspun.com/sql/ What an in-memory database is and how it persists data efficiently What are pros and cons of PostgreSQL and MySQL? With respect to reliability, speed, scalability, and features.","title":"Databases"},{"location":"Systems-Administration/#virtualization","text":"Apparently KVM and Virtualbox are incompatible / can't be run simultaneously. See here for an idea on how to handle that (or just don't do that at all because it doesn't make too much sense to begin with- quoth the older and wiser me). Xen Networking Importing an OVA into KVM","title":"Virtualization"},{"location":"Systems-Administration/#containerization","text":"Kubernetes: An Overview Docker for Data Science Docker Security Cheat Sheet","title":"Containerization"},{"location":"Systems-Administration/#cloud-computing","text":"If an AWS S3 upload is MultiPart , the ETag attribute of an S3 bucket object is not an MD5 hash. It is the hashes for each part uploaded concatenated, plus a dash and the number of parts uploaded (see here ).","title":"Cloud Computing"},{"location":"Systems-Administration/#s3-compatible-object-stores","text":"https://minio.io/ https://cloudian.com/ https://wasabi.com/ http://pithos.io/ https://www.zenko.io/ https://leo-project.net/leofs/ https://github.com/eucalyptus/eucalyptus/wiki/Walrus-S3-API http://docs.ceph.com/docs/master/radosgw/s3/","title":"S3-compatible object stores"},{"location":"Systems-Administration/#windowslinux-compatibility","text":"Debian9ADSharedDisks_Sssd_PamMount Pam_mount RHEL Guide for multi-user SMB mount RHEL Windows Integration Guide","title":"Windows/Linux Compatibility"},{"location":"Systems-Administration/#tools","text":"Atop Gas Hosts last (can show reboot times) lastlog (can show last login for a user- with decently informative timestamp) https://mxtoolbox.com/SuperTool.aspx https://peteris.rocks/blog/htop/ http://md5deep.sourceforge.net/ GNU Parallel","title":"Tools"},{"location":"Systems-Administration/Ansible-Notes/","text":"Ansible Notes Various thoughts/meditations on Ansible and Ansible playbooks. Data Structure Fidelity While configuration management tools are often compared to programming languages (e.g., \"Infrastructure as Code\"), I find that more often the syntax of such tools is more analogous to a database (e.g., similar to ITIL's \"CMDB\"). Phrased differently, while the distinction between data and program has been murky since the first stored program was introduced in the Manchester Baby , I think that Ansible veers more towards data than program (ditto for similar tools like Saltstack, Puppet, and Chef). What little actual programming there is involved in config management is more geared towards control flow than actual computations. Basically, in my view, Ansible is a shim that wraps around interfaces used to interact with configuration data using standard CRUD operations. Namely, Ansible roles contain data that needs to be written to: The Yum, RPM or dpkg databases. Data stored within flat files that live in /etc (such as fstab, passwd, sudoers, etc) Firewall interfaces like firewall-cmd Data/state maintained by systemd. In general, unless there is a compelling reason to develop an alternative schema for representing data, I think that data in Ansible roles and playbooks should resemble the data structures already used. This is because the act of re-mapping configuration data to a new schema from the one that the software developer has chosen ignores the developer's reasoning and implies that the information asymmetry between developer and end-user does not exist. We should accept the existence of asymmetry, and defer to the developers judgement save for exceptional circumstances. Conditionals Running the following repeatedly is the same as having a bunch of if/elif/else statements in a row (from ansible-guacamole ): - name: Setting a fact. set_fact: my_great_fact: foobar when: > (ansible_distribution == \"Debian\") or (ansible_distribution == \"Ubuntu\") - name: Setting a fact. set_fact: my_great_fact: fizzbuzz when: > (ansible_distribution == \"RedHat\") In contrast, this is the Ansible equivalent to a switch/case statement (from ansible-anaconda by Andrew Rothstein): - name: resolve platform specific vars include_vars: \"{{ item }}\" with_first_found: - files: - '{{ ansible_os_family }}.yml' - 'default.yml' paths: - '{{ role_path }}/vars' Ansible switch/case statements are preferable because fewer conditionals need to be evaluated and case statements are more maintainable / grokkable. In the case/statement, all that needs to happen is for ansible_os_family to be expanded once, while in the other example ansible_distribution is expanded three times and two when statements need to be evaluated.","title":"Ansible Notes"},{"location":"Systems-Administration/Ansible-Notes/#ansible-notes","text":"Various thoughts/meditations on Ansible and Ansible playbooks.","title":"Ansible Notes"},{"location":"Systems-Administration/Ansible-Notes/#data-structure-fidelity","text":"While configuration management tools are often compared to programming languages (e.g., \"Infrastructure as Code\"), I find that more often the syntax of such tools is more analogous to a database (e.g., similar to ITIL's \"CMDB\"). Phrased differently, while the distinction between data and program has been murky since the first stored program was introduced in the Manchester Baby , I think that Ansible veers more towards data than program (ditto for similar tools like Saltstack, Puppet, and Chef). What little actual programming there is involved in config management is more geared towards control flow than actual computations. Basically, in my view, Ansible is a shim that wraps around interfaces used to interact with configuration data using standard CRUD operations. Namely, Ansible roles contain data that needs to be written to: The Yum, RPM or dpkg databases. Data stored within flat files that live in /etc (such as fstab, passwd, sudoers, etc) Firewall interfaces like firewall-cmd Data/state maintained by systemd. In general, unless there is a compelling reason to develop an alternative schema for representing data, I think that data in Ansible roles and playbooks should resemble the data structures already used. This is because the act of re-mapping configuration data to a new schema from the one that the software developer has chosen ignores the developer's reasoning and implies that the information asymmetry between developer and end-user does not exist. We should accept the existence of asymmetry, and defer to the developers judgement save for exceptional circumstances.","title":"Data Structure Fidelity"},{"location":"Systems-Administration/Ansible-Notes/#conditionals","text":"Running the following repeatedly is the same as having a bunch of if/elif/else statements in a row (from ansible-guacamole ): - name: Setting a fact. set_fact: my_great_fact: foobar when: > (ansible_distribution == \"Debian\") or (ansible_distribution == \"Ubuntu\") - name: Setting a fact. set_fact: my_great_fact: fizzbuzz when: > (ansible_distribution == \"RedHat\") In contrast, this is the Ansible equivalent to a switch/case statement (from ansible-anaconda by Andrew Rothstein): - name: resolve platform specific vars include_vars: \"{{ item }}\" with_first_found: - files: - '{{ ansible_os_family }}.yml' - 'default.yml' paths: - '{{ role_path }}/vars' Ansible switch/case statements are preferable because fewer conditionals need to be evaluated and case statements are more maintainable / grokkable. In the case/statement, all that needs to happen is for ansible_os_family to be expanded once, while in the other example ansible_distribution is expanded three times and two when statements need to be evaluated.","title":"Conditionals"},{"location":"Systems-Administration/BOINCAMI/","text":"How to Create an AMI to Run BOINC Why Run BOINC In the Cloud Consumer PCs are not designed for power-hungry, memory-intensive scientific computing applications. Server hardware is. Even with the performance penalty for virtualization, it's much better to have numbers being crunched in a data center than on your laptop; you won't drive up your electric bill and incur needless wear and tear on your everyday electronics. Launch An Instance Using the Starcluster AMI Start an instance with one of the public Starcluster AMIs, since they already have many drivers and applications pre-installed for HPC (BLAS,CUDA,etc). Choose an HVM AMI, since these will include GPU support (should you ever want to use a GPU-based instance type). I chose Ubuntu 12.04 (starcluster-base-ubuntu-12.04-x86_64-hvm; ami-52a0c53b). Avoid Ubuntu 13.04, as support for that from Canonical is spotty. Install A Lightweight GUI and Remote Desktop Server There are some operations for BOINC that you can accomplish from the command line (boinccmd). Other operations (such as changing resources allocated to BOINC) are not possible or are difficult. A lot of BOINC's design seems to be centered upon the user interacting with the client via the graphical BOINC manager. To get around this, install a lightweight GUI like lxde and a remote desktop server like x2go. Create an Attachable/Detachable Volume Make a new volume in the AWS console. Attach to the instance. Partition, create a file system and mount at a suitable location (like '/boinc'). Install BOINC Use apt-get or yum to install BOINC client and BOINC manager. Move BOINC client's data from /var/lib/boinc-client to the new volume you created (mounted at '/boinc' or wherever you decided to mount it). Edit /etc/default/boinc-client and change BOINC_DIR from \"/var/lib/boinc-client\" to the new mount point ('/boinc'). Separating the data from the instance's root volume allows the tasks that were currently running to be preserved in the case of a poweroff (i.e., if you are using spot instances and the instance is abruptly terminated). Save the Instance as an AMI Remove the bash history, ssh directory in your home folder. Save as an EBS-backed AMI in the AWS console. When Running When you start a new instance with the AMI, don't forget to a) mount the attached volume and b) start the BOINC client service with sudo /etc/init.d/boinc-client start. With Spot Instances Make a snapshot of the BOINC data volume beforehand, since you don't know which AZ the instance will pop up in. Make new volume from snapshot in the instance's AZ accordingly.","title":"BOINCAMI"},{"location":"Systems-Administration/BOINCAMI/#how-to-create-an-ami-to-run-boinc","text":"","title":"How to Create an AMI to Run BOINC"},{"location":"Systems-Administration/BOINCAMI/#why-run-boinc-in-the-cloud","text":"Consumer PCs are not designed for power-hungry, memory-intensive scientific computing applications. Server hardware is. Even with the performance penalty for virtualization, it's much better to have numbers being crunched in a data center than on your laptop; you won't drive up your electric bill and incur needless wear and tear on your everyday electronics.","title":"Why Run BOINC In the Cloud"},{"location":"Systems-Administration/BOINCAMI/#launch-an-instance-using-the-starcluster-ami","text":"Start an instance with one of the public Starcluster AMIs, since they already have many drivers and applications pre-installed for HPC (BLAS,CUDA,etc). Choose an HVM AMI, since these will include GPU support (should you ever want to use a GPU-based instance type). I chose Ubuntu 12.04 (starcluster-base-ubuntu-12.04-x86_64-hvm; ami-52a0c53b). Avoid Ubuntu 13.04, as support for that from Canonical is spotty.","title":"Launch An Instance Using the Starcluster AMI"},{"location":"Systems-Administration/BOINCAMI/#install-a-lightweight-gui-and-remote-desktop-server","text":"There are some operations for BOINC that you can accomplish from the command line (boinccmd). Other operations (such as changing resources allocated to BOINC) are not possible or are difficult. A lot of BOINC's design seems to be centered upon the user interacting with the client via the graphical BOINC manager. To get around this, install a lightweight GUI like lxde and a remote desktop server like x2go.","title":"Install A Lightweight GUI and Remote Desktop Server"},{"location":"Systems-Administration/BOINCAMI/#create-an-attachabledetachable-volume","text":"Make a new volume in the AWS console. Attach to the instance. Partition, create a file system and mount at a suitable location (like '/boinc').","title":"Create an Attachable/Detachable Volume"},{"location":"Systems-Administration/BOINCAMI/#install-boinc","text":"Use apt-get or yum to install BOINC client and BOINC manager. Move BOINC client's data from /var/lib/boinc-client to the new volume you created (mounted at '/boinc' or wherever you decided to mount it). Edit /etc/default/boinc-client and change BOINC_DIR from \"/var/lib/boinc-client\" to the new mount point ('/boinc'). Separating the data from the instance's root volume allows the tasks that were currently running to be preserved in the case of a poweroff (i.e., if you are using spot instances and the instance is abruptly terminated).","title":"Install BOINC"},{"location":"Systems-Administration/BOINCAMI/#save-the-instance-as-an-ami","text":"Remove the bash history, ssh directory in your home folder. Save as an EBS-backed AMI in the AWS console.","title":"Save the Instance as an AMI"},{"location":"Systems-Administration/BOINCAMI/#when-running","text":"When you start a new instance with the AMI, don't forget to a) mount the attached volume and b) start the BOINC client service with sudo /etc/init.d/boinc-client start.","title":"When Running"},{"location":"Systems-Administration/BOINCAMI/#with-spot-instances","text":"Make a snapshot of the BOINC data volume beforehand, since you don't know which AZ the instance will pop up in. Make new volume from snapshot in the instance's AZ accordingly.","title":"With Spot Instances"},{"location":"Systems-Administration/Config-Management/","text":"Config Management Big Files, Git and Config Management A bunch o' links where people discuss Git LFS / Git annex support in config management suites: Saltstack https://github.com/saltstack/salt/issues/25672 Puppet https://github.com/puppetlabs/r10k/issues/695","title":"Config Management"},{"location":"Systems-Administration/Config-Management/#config-management","text":"","title":"Config Management"},{"location":"Systems-Administration/Config-Management/#big-files-git-and-config-management","text":"A bunch o' links where people discuss Git LFS / Git annex support in config management suites:","title":"Big Files, Git and Config Management"},{"location":"Systems-Administration/Config-Management/#saltstack","text":"https://github.com/saltstack/salt/issues/25672","title":"Saltstack"},{"location":"Systems-Administration/Config-Management/#puppet","text":"https://github.com/puppetlabs/r10k/issues/695","title":"Puppet"},{"location":"Systems-Administration/Containerization-Analogies/","text":"A collection of analogies for how Docker / containerization fits into the broader tech stack. Part of understanding these analogies is knowing that an operating system, at the end of the day, is really a program itself (albeit one that mediates access to the underlying hardware for other programs). Docker/Kata/CRI-O Container : Python Virtual Environment, where a container is an environment for operating system libraries and other components written in C. Essentially, a container is a virtual environment for C code / lower-level code. A Docker Image / Golden image : Application binary Dockerfile / Ansible / Config management : Application source code. A Dockerfile could be seen as more analogous to a Makefile though. Packer / Buildah / Image Builder : Make Dockerhub / container image registry : Yum repository","title":"Containerization Analogies"},{"location":"Systems-Administration/IT-Mistakes/","text":"Mistakes in IT Generating Unexpected Behavior When Invoking a Shell Command Typically a symptom of not reading the manual for a shell command first to totally grasp the flags and options. A great many mistakes of this variety (and of other classes) are issues of communication. You cannot embrace knowledge/information if you do not first encode it. This occurs in other contexts as well (such as not reading e-mails carefully enough). This can also be a symptom of inadequate testing. If possible, do a dry run of a command beforehand (i.e., rsync's '-n' flag) or use 'echo' before a script to make sure that the variable has the correct values. Also test results afterwards- make a checklist of what you expect an output to contain and what it should not contain. General advice: Avoid using cd with relative paths in scripts (i.e., be wary of 'cd ..'). If a glitch occurs in anything before the 'cd' command, or a previous cd command before the relative path behaves peculiarly, you may end up in an unexpected directory. Inadequate planning before putting together a script. Inadequate logging for a script or command (e.g., should have a text file with status of various steps piped to it; send time for file sync). Lack of consideration for how script may operate across environments (i.e., different versions of Linux, etc). You cannot change ownership of a file or directory from a normal user to root easily (unless you are root). If you are changing ownership to root (or any other user) as a normal user, it will not work . Simple Pragmatics Don't do uploads over Wifi when Ethernet is possible. Don't use Ethernet when Infiniband makes sense. Make sure that cords aren't plugged in in a way that is so mangled that unplugging one cord could accidentally power off another device. User Communication In some instances a user can be educated. Other times, it is better to apply a solution without educating the user, and then trying to educate the user later. Example: A user is agitated and under a lot of stress and needs the solution immediately. Inadequate communication with users (getting user needs) before beginning a purchase process. Performing a reboot without consulting users. Documentation Be mindful of the potential need to document changes in directory structure on a file system (redirecting the output of 'tree' or 'ls -R' to a text file might be a good idea here). Migrations/Upgrades Inadequate documentation of licenses before a migration to new systems. Inadequate planning before a migration. Not checking if and how licenses can be migrated between systems. Upgrading a system without a checklist. Upgrading a system without a rollback plan. Not having a time scale for a rollback plan. Data Transfer / Storage When transferring via Sneakernet (still occurs in rare situations): Not creating a manifest of files to be transferred before transferring them to a physical medium. Without such a manifest, there's no way of keeping track of whether or not all the files that were intended to be transferred actually were. Don't assume that because a bay can be taken out of a server, it is plug'n'play. You will be burned on this if you do not bear this in mind. Server hard drives are not to be treated like USB hard drives you may use with a desktop. dd if=[block level device or image] bs=[block size A] | pv | dd of=[block level device or image] bs=[block size B] conv=sync is a great way to corrupt an image. It's always a good idea to a) make sure you understand what a flag is doing ( conv=sync ) rather than trusting the documentation it came from blindly and b) make sure that the block size is the same on either side of a pipe using multiple dds.","title":"IT Mistakes"},{"location":"Systems-Administration/IT-Mistakes/#mistakes-in-it","text":"","title":"Mistakes in IT"},{"location":"Systems-Administration/IT-Mistakes/#generating-unexpected-behavior-when-invoking-a-shell-command","text":"Typically a symptom of not reading the manual for a shell command first to totally grasp the flags and options. A great many mistakes of this variety (and of other classes) are issues of communication. You cannot embrace knowledge/information if you do not first encode it. This occurs in other contexts as well (such as not reading e-mails carefully enough). This can also be a symptom of inadequate testing. If possible, do a dry run of a command beforehand (i.e., rsync's '-n' flag) or use 'echo' before a script to make sure that the variable has the correct values. Also test results afterwards- make a checklist of what you expect an output to contain and what it should not contain. General advice: Avoid using cd with relative paths in scripts (i.e., be wary of 'cd ..'). If a glitch occurs in anything before the 'cd' command, or a previous cd command before the relative path behaves peculiarly, you may end up in an unexpected directory. Inadequate planning before putting together a script. Inadequate logging for a script or command (e.g., should have a text file with status of various steps piped to it; send time for file sync). Lack of consideration for how script may operate across environments (i.e., different versions of Linux, etc). You cannot change ownership of a file or directory from a normal user to root easily (unless you are root). If you are changing ownership to root (or any other user) as a normal user, it will not work .","title":"Generating Unexpected Behavior When Invoking a Shell Command"},{"location":"Systems-Administration/IT-Mistakes/#simple-pragmatics","text":"Don't do uploads over Wifi when Ethernet is possible. Don't use Ethernet when Infiniband makes sense. Make sure that cords aren't plugged in in a way that is so mangled that unplugging one cord could accidentally power off another device.","title":"Simple Pragmatics"},{"location":"Systems-Administration/IT-Mistakes/#user-communication","text":"In some instances a user can be educated. Other times, it is better to apply a solution without educating the user, and then trying to educate the user later. Example: A user is agitated and under a lot of stress and needs the solution immediately. Inadequate communication with users (getting user needs) before beginning a purchase process. Performing a reboot without consulting users.","title":"User Communication"},{"location":"Systems-Administration/IT-Mistakes/#documentation","text":"Be mindful of the potential need to document changes in directory structure on a file system (redirecting the output of 'tree' or 'ls -R' to a text file might be a good idea here).","title":"Documentation"},{"location":"Systems-Administration/IT-Mistakes/#migrationsupgrades","text":"Inadequate documentation of licenses before a migration to new systems. Inadequate planning before a migration. Not checking if and how licenses can be migrated between systems. Upgrading a system without a checklist. Upgrading a system without a rollback plan. Not having a time scale for a rollback plan.","title":"Migrations/Upgrades"},{"location":"Systems-Administration/IT-Mistakes/#data-transfer-storage","text":"When transferring via Sneakernet (still occurs in rare situations): Not creating a manifest of files to be transferred before transferring them to a physical medium. Without such a manifest, there's no way of keeping track of whether or not all the files that were intended to be transferred actually were. Don't assume that because a bay can be taken out of a server, it is plug'n'play. You will be burned on this if you do not bear this in mind. Server hard drives are not to be treated like USB hard drives you may use with a desktop. dd if=[block level device or image] bs=[block size A] | pv | dd of=[block level device or image] bs=[block size B] conv=sync is a great way to corrupt an image. It's always a good idea to a) make sure you understand what a flag is doing ( conv=sync ) rather than trusting the documentation it came from blindly and b) make sure that the block size is the same on either side of a pipe using multiple dds.","title":"Data Transfer / Storage"},{"location":"Systems-Administration/Kubernetes-Notes/","text":"Kubernetes Notes I'm using this page as an unorganized dumping ground for various thoughts on K8s. K8s as an operating system Kubernetes is often thought of as a sort of second-level operating system, or an operating system that runs on top of another (lower-level) operating system (currently this is only GNU/Linux, although in principle a version of Kubernetes could be built for Solaris Zones or FreeBSD jails). By Andrew Tanenbaum's definition, a conventional first-level OS provides abstract interfaces to hardware for programmers (i.e., system calls) and a way of managing hardware resources (i.e., a process scheduler, I/O scheduler, drivers). Kubernetes, as a second-level OS, provides abstract interfaces not to low-level hardware resources, but to coarse-grained allocations of resources (e.g., CPU time, memory, whole GPUs / ASICs) and a sort of proxy-based switching setup (e.g., Services) using abstract objects. Just as a first-level operating system uses processes to enforce isolation between programs running in tandem, Kubernetes uses Pods/containers/cgroups to enforce isolation. Pods are like processes, containers are like threads. An operating system is like a government, and the layering of a second-level OS (Kubernetes) on top of a first-level OS (GNU/Linux) is like a kind of federalism. The Kubernetes control plane is almost like a microkernel in how it functions. Services within the kube-system namespace are like user space-based drivers / components in a microkernel system (e.g., the file system, drivers, etc). Microservices are processes that live within user space and make heavy use of interprocess communication (i.e., HTTP calls). kube-proxy is the Kubernetes control panel's implementation of IPC. Istio / service meshes are an alternative implementation of IPC where routing functionality, logging, and security are tightly-coupled with a service as a sidecar/ambassador container (i.e., a thread). A traditional distributed OS attempts to create a single virtual machine that spans many nodes using a microkernel. The cost of this lofty goal is that distributed operating systems are typically extremely complex / are a beast to maintain. By separating coarse-grained allocation of resources (handled by Kubernetes within user space) from fine-grained allocation of resources (handled by GNU/Linux on individual hosts) and making nodes more loosely-coupled instead of integrated into a single cohesive whole, the Kubernetes developers have decreased complexity while providing many of the advantages of the distributed operating system (in terms of providing resources at scale and optimizing resource utilization). A true distributed operating system should in theory be able to make scheduling / resource allocation decisions that are better than Kubernetes, but Kubernetes is adequate and easier (perfect is the enemy of good).","title":"Kubernetes Notes"},{"location":"Systems-Administration/Kubernetes-Notes/#kubernetes-notes","text":"I'm using this page as an unorganized dumping ground for various thoughts on K8s.","title":"Kubernetes Notes"},{"location":"Systems-Administration/Kubernetes-Notes/#k8s-as-an-operating-system","text":"Kubernetes is often thought of as a sort of second-level operating system, or an operating system that runs on top of another (lower-level) operating system (currently this is only GNU/Linux, although in principle a version of Kubernetes could be built for Solaris Zones or FreeBSD jails). By Andrew Tanenbaum's definition, a conventional first-level OS provides abstract interfaces to hardware for programmers (i.e., system calls) and a way of managing hardware resources (i.e., a process scheduler, I/O scheduler, drivers). Kubernetes, as a second-level OS, provides abstract interfaces not to low-level hardware resources, but to coarse-grained allocations of resources (e.g., CPU time, memory, whole GPUs / ASICs) and a sort of proxy-based switching setup (e.g., Services) using abstract objects. Just as a first-level operating system uses processes to enforce isolation between programs running in tandem, Kubernetes uses Pods/containers/cgroups to enforce isolation. Pods are like processes, containers are like threads. An operating system is like a government, and the layering of a second-level OS (Kubernetes) on top of a first-level OS (GNU/Linux) is like a kind of federalism. The Kubernetes control plane is almost like a microkernel in how it functions. Services within the kube-system namespace are like user space-based drivers / components in a microkernel system (e.g., the file system, drivers, etc). Microservices are processes that live within user space and make heavy use of interprocess communication (i.e., HTTP calls). kube-proxy is the Kubernetes control panel's implementation of IPC. Istio / service meshes are an alternative implementation of IPC where routing functionality, logging, and security are tightly-coupled with a service as a sidecar/ambassador container (i.e., a thread). A traditional distributed OS attempts to create a single virtual machine that spans many nodes using a microkernel. The cost of this lofty goal is that distributed operating systems are typically extremely complex / are a beast to maintain. By separating coarse-grained allocation of resources (handled by Kubernetes within user space) from fine-grained allocation of resources (handled by GNU/Linux on individual hosts) and making nodes more loosely-coupled instead of integrated into a single cohesive whole, the Kubernetes developers have decreased complexity while providing many of the advantages of the distributed operating system (in terms of providing resources at scale and optimizing resource utilization). A true distributed operating system should in theory be able to make scheduling / resource allocation decisions that are better than Kubernetes, but Kubernetes is adequate and easier (perfect is the enemy of good).","title":"K8s as an operating system"},{"location":"Systems-Administration/Misc-SysAdmin/","text":"Miscellaneous This page contains miscellaneous snippets that I pick up that I haven't been able to group into bigger concepts yet. In the sudoers file : #includedir is not a comment. It's a directive that points to supplemental config files. There are several other #include directives. This mimics C syntax. fail2ban : a great way to get rid of spammers / brute force attacks. Thanks Yarik! HP iLO power button options: https://www.experts-exchange.com/questions/27971206/HP-iLO.html git branch != git checkout for creating a new branch using the CLI. Fiber (specialized form of thread) cat /dev/null > /path/to/log allows you to wipe a log without restarting any associated process / daemon ZeroMQ references: http://nichol.as/zeromq-an-introduction http://intothesaltmine.readthedocs.io/en/latest/chapters/command-and-control/zeromq.html https://news.ycombinator.com/item?id=2428004 gpasswd - it exists ; it is a good way to remove users from groups etc modules are often more useful than grains in getting system info in salt / just because a grain doesn't exist, doesn't mean there isn't an easy way to get it that's not cmd.run Outside Links to be Sorted: https://teachyourselfcs.com https://www.aosabook.org","title":"Misc SysAdmin"},{"location":"Systems-Administration/Misc-SysAdmin/#miscellaneous","text":"This page contains miscellaneous snippets that I pick up that I haven't been able to group into bigger concepts yet. In the sudoers file : #includedir is not a comment. It's a directive that points to supplemental config files. There are several other #include directives. This mimics C syntax. fail2ban : a great way to get rid of spammers / brute force attacks. Thanks Yarik! HP iLO power button options: https://www.experts-exchange.com/questions/27971206/HP-iLO.html git branch != git checkout for creating a new branch using the CLI. Fiber (specialized form of thread) cat /dev/null > /path/to/log allows you to wipe a log without restarting any associated process / daemon ZeroMQ references: http://nichol.as/zeromq-an-introduction http://intothesaltmine.readthedocs.io/en/latest/chapters/command-and-control/zeromq.html https://news.ycombinator.com/item?id=2428004 gpasswd - it exists ; it is a good way to remove users from groups etc modules are often more useful than grains in getting system info in salt / just because a grain doesn't exist, doesn't mean there isn't an easy way to get it that's not cmd.run Outside Links to be Sorted: https://teachyourselfcs.com https://www.aosabook.org","title":"Miscellaneous"},{"location":"Systems-Administration/Scheduler-Comparisons/","text":"SLURM vs Kubernetes vs Nomad Features: SLURM is primarily focused on just scheduling, whereas Kubernetes has a scheduler (kube-scheduler), distributed configuration (etcd), and control loops / negative feedback loops (kube-controller-manager; a la Norbert Wiener / cybernetics) plus custom logic to deal with the fact that the unit of scheduling is a grouping of containers (a pod) instead of a group of processes (i.e., containers need to have networking, DNS, etc configured dynamically). There is, however, no reason why SLURM could not also have some of these features in theory (i.e., confd could be combined with SLURM and etcd to give SLURM a distributed configuration that is dynamically updated). Unit of Scheduling: SLURM schedules jobs (typically one or more processes), while Kubernetes schedules pods (groups of containers). Jobs can have resource isolation via cgroups, but don't have namespace isolation or any fancy logic to deal with filesystems (i.e., union file systems). Basically, jobs have 1/3 of what makes up a container. If namespace isolation and union filesystems aren't important, SLURM has a compelling advantage over Kubernetes due to having less complexity/overhead. Scheduling Algorithms: SLURM uses a priority queue or a backfill algorithm. Kubernetes filters available nodes, assigns them a priority score, and then pops off the node with the highest score (so also a priority queue most likely; see here ). Hashicorp Nomad uses a bin-packing algorithm (see here ). SLURM / Kubernetes Analogies Kubernetes SLURM Namespace Partition kubelet slurmd kube-scheduler / kube-controller-manager / slurm-apiserver slurmctld Container Runtime Vanilla cgroups etcd slurm.conf + NFS Prometheus / ELK slurmdbd (?) Labels / Selectors GRES / TRES / Features Marking a node as unschedulable / cordoning Draining a node / Making a resource reservation Further Reading Kubernetes Components Docker Overview A Summary of OverlayFS Scheduler Algorithm in Kubernetes More up-to-date overview of scheduling in Kubernetes Scheduling in Nomad Cgroups Guide (SLURM) Scheduling Configuration Guide (SLURM)","title":"Scheduler Comparisons"},{"location":"Systems-Administration/Scheduler-Comparisons/#slurm-vs-kubernetes-vs-nomad","text":"Features: SLURM is primarily focused on just scheduling, whereas Kubernetes has a scheduler (kube-scheduler), distributed configuration (etcd), and control loops / negative feedback loops (kube-controller-manager; a la Norbert Wiener / cybernetics) plus custom logic to deal with the fact that the unit of scheduling is a grouping of containers (a pod) instead of a group of processes (i.e., containers need to have networking, DNS, etc configured dynamically). There is, however, no reason why SLURM could not also have some of these features in theory (i.e., confd could be combined with SLURM and etcd to give SLURM a distributed configuration that is dynamically updated). Unit of Scheduling: SLURM schedules jobs (typically one or more processes), while Kubernetes schedules pods (groups of containers). Jobs can have resource isolation via cgroups, but don't have namespace isolation or any fancy logic to deal with filesystems (i.e., union file systems). Basically, jobs have 1/3 of what makes up a container. If namespace isolation and union filesystems aren't important, SLURM has a compelling advantage over Kubernetes due to having less complexity/overhead. Scheduling Algorithms: SLURM uses a priority queue or a backfill algorithm. Kubernetes filters available nodes, assigns them a priority score, and then pops off the node with the highest score (so also a priority queue most likely; see here ). Hashicorp Nomad uses a bin-packing algorithm (see here ).","title":"SLURM vs Kubernetes vs Nomad"},{"location":"Systems-Administration/Scheduler-Comparisons/#slurm-kubernetes-analogies","text":"Kubernetes SLURM Namespace Partition kubelet slurmd kube-scheduler / kube-controller-manager / slurm-apiserver slurmctld Container Runtime Vanilla cgroups etcd slurm.conf + NFS Prometheus / ELK slurmdbd (?) Labels / Selectors GRES / TRES / Features Marking a node as unschedulable / cordoning Draining a node / Making a resource reservation","title":"SLURM / Kubernetes Analogies"},{"location":"Systems-Administration/Scheduler-Comparisons/#further-reading","text":"Kubernetes Components Docker Overview A Summary of OverlayFS Scheduler Algorithm in Kubernetes More up-to-date overview of scheduling in Kubernetes Scheduling in Nomad Cgroups Guide (SLURM) Scheduling Configuration Guide (SLURM)","title":"Further Reading"},{"location":"Systems-Administration/Server-Vendors/","text":"Server Vendors A list of server vendors. Aspen Systems Silicon Mechanics Red Barn Dell A definition: VAR = Value-Added Reseller. A discussion","title":"Server Vendors"},{"location":"Systems-Administration/Server-Vendors/#server-vendors","text":"A list of server vendors. Aspen Systems Silicon Mechanics Red Barn Dell A definition: VAR = Value-Added Reseller. A discussion","title":"Server Vendors"},{"location":"Systems-Administration/Text-Editors/","text":"Create and edit text files Here are some links and resources to learn more about vim and emacs- the two most prominent text editors in the *nix world. vim Typing vimtutor at the command prompt gives an excellent walkthrough created by several individuals at the Colorado School of Mines. The Vim Tips Wiki Ben Kuperman's Vim Page Vim Quick Reference Card Vim Cheat Sheet Syntastic is a linter interface for vim for multiple languages. (Requires linters to be installed separately;i.e., pip install pylint ). Pathogen is a package/plugin manager, much like CRAN or CPAN. Odds and ends: Switch from less to vim- ':wv'. Check before replacing in vim :s/foo/bar/gc' Display line numbers in vim : http://vim.wikia.com/wiki/Display_line_numbers emacs GNU Emacs Tour Emacs Tutorial Emacs Mini Manual","title":"Text Editors"},{"location":"Systems-Administration/Text-Editors/#create-and-edit-text-files","text":"Here are some links and resources to learn more about vim and emacs- the two most prominent text editors in the *nix world.","title":"Create and edit text files"},{"location":"Systems-Administration/Text-Editors/#vim","text":"Typing vimtutor at the command prompt gives an excellent walkthrough created by several individuals at the Colorado School of Mines. The Vim Tips Wiki Ben Kuperman's Vim Page Vim Quick Reference Card Vim Cheat Sheet Syntastic is a linter interface for vim for multiple languages. (Requires linters to be installed separately;i.e., pip install pylint ). Pathogen is a package/plugin manager, much like CRAN or CPAN. Odds and ends: Switch from less to vim- ':wv'. Check before replacing in vim :s/foo/bar/gc' Display line numbers in vim : http://vim.wikia.com/wiki/Display_line_numbers","title":"vim"},{"location":"Systems-Administration/Text-Editors/#emacs","text":"GNU Emacs Tour Emacs Tutorial Emacs Mini Manual","title":"emacs"},{"location":"Systems-Administration/UPS/","text":"Useful links for UPS considerations: https://www.backupbatterypower.com/pages/ups-run-time-calculator http://www.apcmedia.com/salestools/SADE-5TNQYL/SADE-5TNQYL_R1_EN.pdf?sdirect=true","title":"UPS"},{"location":"Systems-Administration/udev/","text":"http://unix.stackexchange.com/questions/91085/udev-renaming-my-network-interface http://kmadac.github.io/posts/18-Regenerate-net-udev-rules-in-ubuntu-12.04/ http://www.linuxfromscratch.org/lfs/view/6.3/chapter07/network.html%2B/lib/udev/write_net_rules http://unix.stackexchange.com/questions/39370/how-to-reload-udev-rules-without-reboot","title":"Udev"},{"location":"Systems-Administration/Legacy/General-IT-Notes/","text":"General IT Notes General Problem-Solving Algorithm Look for a past solution that can be re-applied or modified. First look at solutions that I have already used. If there is not a simple or elegant solution, do independent research (books, mailing lists, StackExchange) to see if others have devised a solution in the past that can be applied. If step 1 fails, create some preliminary specifications for a solution. Look for tools to implement the specifications. Write pseudocode or make a flow chart to keep track of how components (functions, applications) might interact. Document as you go along. If the project is going to be long-term, set up a time table. Lessons learned from working with non-standalone PsychoPy Next time use a VM if you need to install deprecated or bleeding edge libraries on a system (i.e., 32-bit Python). Code reuse. User-friendly Subversion clients Mac - svnX, SCPlugin Windows - TortoiseSVN Technology to become acquainted with ACL permissions scp scp -r dir1 user@server:dir2 If dir2 does not exist, creates dir2 on server, populates with items within dir1. Bref: files nested w/in dir2 Else: If dir2 exists, copies dir1 into dir2 and nests it within dir2. Bref dir1 nested w/in dir2. chmod/permissions behavior If you remove 'execute' from a directory, you will not be able to cd into it. umask/permissions behavior octal codes for umask are the inverse of chmod codes grep: -F : returns lines with single occurrence of literal string (fgrep) -x : returns only if the entire line is an exact match for literal string -E : use extended regex (Perl style); no need for escapes to denote atoms (? review this) (egrep) returns lines with any number of consecutive repeats of search string . - wildcard character . or SOURCE : Runs a script's commands through the current shell line by line. The script's variables become available to the current shell environment. ls To list all directories (and only directories) within a directory use: ls -d */ NOT ls -d Zero Padding in Bash: printf \"%0*d\\n\" $padtowidth $i See here","title":"General IT Notes"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#general-it-notes","text":"","title":"General IT Notes"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#general-problem-solving-algorithm","text":"Look for a past solution that can be re-applied or modified. First look at solutions that I have already used. If there is not a simple or elegant solution, do independent research (books, mailing lists, StackExchange) to see if others have devised a solution in the past that can be applied. If step 1 fails, create some preliminary specifications for a solution. Look for tools to implement the specifications. Write pseudocode or make a flow chart to keep track of how components (functions, applications) might interact. Document as you go along. If the project is going to be long-term, set up a time table.","title":"General Problem-Solving Algorithm"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#lessons-learned-from-working-with-non-standalone-psychopy","text":"Next time use a VM if you need to install deprecated or bleeding edge libraries on a system (i.e., 32-bit Python). Code reuse.","title":"Lessons learned from working with non-standalone PsychoPy"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#user-friendly-subversion-clients","text":"Mac - svnX, SCPlugin Windows - TortoiseSVN","title":"User-friendly Subversion clients"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#technology-to-become-acquainted-with","text":"ACL permissions","title":"Technology to become acquainted with"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#scp","text":"scp -r dir1 user@server:dir2 If dir2 does not exist, creates dir2 on server, populates with items within dir1. Bref: files nested w/in dir2 Else: If dir2 exists, copies dir1 into dir2 and nests it within dir2. Bref dir1 nested w/in dir2.","title":"scp"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#chmodpermissions-behavior","text":"If you remove 'execute' from a directory, you will not be able to cd into it.","title":"chmod/permissions behavior"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#umaskpermissions-behavior","text":"octal codes for umask are the inverse of chmod codes","title":"umask/permissions behavior"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#grep","text":"-F : returns lines with single occurrence of literal string (fgrep) -x : returns only if the entire line is an exact match for literal string -E : use extended regex (Perl style); no need for escapes to denote atoms (? review this) (egrep) returns lines with any number of consecutive repeats of search string . - wildcard character","title":"grep:"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#or-source","text":"Runs a script's commands through the current shell line by line. The script's variables become available to the current shell environment.","title":". or SOURCE :"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#ls","text":"To list all directories (and only directories) within a directory use: ls -d */ NOT ls -d","title":"ls"},{"location":"Systems-Administration/Legacy/General-IT-Notes/#zero-padding-in-bash","text":"printf \"%0*d\\n\" $padtowidth $i See here","title":"Zero Padding in Bash:"},{"location":"Systems-Administration/Rants/Web-Browsers-Must-Die/","text":"Web Browsers Must Die I'm getting sick of the fact that web browsers seem to be usurping the functionality of a proper operating system, and that every Web 2.0 website seems to be some unholy bloated visual doggerel . I grew up on Web 1.0 in the 90s- I thrived in that environment. There was no clutter, no poppycock, no embedded bitcoin miners or ransomware payloads hidden in embedded ads. Just raw information, following Grice's maxims like a champ. Furthermore, call me old school (and I'm generally speaking a young-ish dude), but I'm annoyed by the tendency to shoehorn everything onto HTTP. Wasn't the point of having application layer protocols exist in the first place to tailor calls over the internet to specific application domains? This gripe isn't really based in anything technical (HTTP is actually probably better than most application-layer protocols that could be devised independently for many applications), but my gut instinct is that in many cases, HTTP is an unnecessary middle man when an API is built on top of it (disclaimer: I might be wrong here). Get off my lawn! I want to go back to simpler days, and so I'm trying to figure out what tools and utilities I can use to get me back to the olden times. I got in to this mode of thinking once a long time ago (circa 2013) but I seem to have circled back again. Here are some ideas: Wiki CLI because I want to feel like WAIS is a thing outside of LibraryComputing . RTV because I want to feel like the USENET still exists. Newsboat because when I want to read news, I want to read text and not see some shitty auto-play video that tanks all of my cell phone data (fuck you NY Times). cmdg for Gmail (since I think that Gmail labels have value over vanilla, standard e-mail). Maybe aerc would work too. mutt apparently has some support for Gmail labels: here and here Other concerns about mutt w/ gmail: here and here MPS Youtube to replace YouTube . I'm sick of being distracted by recommended videos trying to convince me of some nonsense or that contain some BuzzFeed -style listicle in video form. Finch for online chat. screen to give me the experience of cycling through a bunch of different tabs. For everything else, brow.sh or Lynx. An Ansible playbook to set all this up. A lightweight VM I can run on Bruno with all of this installed. OS X will need to stay because I like video games.","title":"Web Browsers Must Die"},{"location":"Systems-Administration/Rants/Web-Browsers-Must-Die/#web-browsers-must-die","text":"I'm getting sick of the fact that web browsers seem to be usurping the functionality of a proper operating system, and that every Web 2.0 website seems to be some unholy bloated visual doggerel . I grew up on Web 1.0 in the 90s- I thrived in that environment. There was no clutter, no poppycock, no embedded bitcoin miners or ransomware payloads hidden in embedded ads. Just raw information, following Grice's maxims like a champ. Furthermore, call me old school (and I'm generally speaking a young-ish dude), but I'm annoyed by the tendency to shoehorn everything onto HTTP. Wasn't the point of having application layer protocols exist in the first place to tailor calls over the internet to specific application domains? This gripe isn't really based in anything technical (HTTP is actually probably better than most application-layer protocols that could be devised independently for many applications), but my gut instinct is that in many cases, HTTP is an unnecessary middle man when an API is built on top of it (disclaimer: I might be wrong here). Get off my lawn! I want to go back to simpler days, and so I'm trying to figure out what tools and utilities I can use to get me back to the olden times. I got in to this mode of thinking once a long time ago (circa 2013) but I seem to have circled back again. Here are some ideas: Wiki CLI because I want to feel like WAIS is a thing outside of LibraryComputing . RTV because I want to feel like the USENET still exists. Newsboat because when I want to read news, I want to read text and not see some shitty auto-play video that tanks all of my cell phone data (fuck you NY Times). cmdg for Gmail (since I think that Gmail labels have value over vanilla, standard e-mail). Maybe aerc would work too. mutt apparently has some support for Gmail labels: here and here Other concerns about mutt w/ gmail: here and here MPS Youtube to replace YouTube . I'm sick of being distracted by recommended videos trying to convince me of some nonsense or that contain some BuzzFeed -style listicle in video form. Finch for online chat. screen to give me the experience of cycling through a bunch of different tabs. For everything else, brow.sh or Lynx. An Ansible playbook to set all this up. A lightweight VM I can run on Bruno with all of this installed. OS X will need to stay because I like video games.","title":"Web Browsers Must Die"}]}